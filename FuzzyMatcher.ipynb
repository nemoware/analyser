{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FuzzyMatcher.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/tensorflow-model/FuzzyMatcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "source": [
        "## MAIN, init, load code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"300px\", display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"tensorflow-model\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import HtmlRenderer\n",
        "  from renderer import to_multicolor_text\n",
        "  from renderer import known_subjects_dict\n",
        "\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  from ml_tools import ProbableValue\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "     \n",
        "     \n",
        "\n",
        "     \n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "\n",
        "def read_doc(fn):\n",
        "  import docx2txt, sys, os\n",
        "\n",
        "  text = ''\n",
        "  try:\n",
        "    text = docx2txt.process(fn)\n",
        "\n",
        "  except:\n",
        "    print(\"Unexpected error:\", sys.exc_info())\n",
        "    os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "    with open(fn + '.txt') as f:\n",
        "      text = f.read()\n",
        "\n",
        "  return text\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "  \n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX\n",
        "\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        " \n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL5p2qEg9aST",
        "colab_type": "text"
      },
      "source": [
        "### the model üíé for finding closest embedding to a pattern\n",
        "1. At first stage, we calculate attention vectors (AV) for every pattern\n",
        "2. Then, for each AV we're finding the closest point (`best point`) in text embedding space, \n",
        "3. Then we're calculating \"improved\" vectors -- we calculate distance from the `\"best point\"` (actually, from the best *window*)  to each point in text embedding space.  \n",
        "  \n",
        "WARNING: this should be used only if `best point` is close enough to the pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpGBXKR6KN0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tf_model import PatternSearchModel\n",
        "PM = PatternSearchModel(tf, hub)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeXJiOdC5YHB",
        "colab_type": "text"
      },
      "source": [
        "## Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDSb0Wrp5P8f",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        " \n",
        "want_upload = False #@param {type:\"boolean\"}\n",
        "search_for = \"\\u0432 \\u043B\\u0438\\u0446\\u0435 \\u0433\\u0435\\u043D\\u0435\\u0440\\u0430\\u043B\\u044C\\u043D\\u043E\\u0433\\u043E \\u0434\\u0438\\u0440\\u0435\\u043A\\u0442\\u043E\\u0440\\u0430\" #@param {type:\"string\"}\n",
        "from patterns import AbstractPatternFactory, FuzzyPattern\n",
        "\n",
        "from text_tools import tokenize_text\n",
        "import re\n",
        "from text_normalize import normalize_text, replacements_regex\n",
        "uploaded = \"\"\"\n",
        "–ë–µ—Ä–ª–∏–Ω—Å–∫–∞—è –¥–∞–¥–∞-—è—Ä–º–∞—Ä–∫–∞ –∏ –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –≤—ã—Å—Ç–∞–≤–∫–∞ —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞ –≤ –ø–∞—Ä–∏–∂¬≠—Å–∫–æ–π –≥–∞–ª–µ—Ä–µ–µ ¬´–ò–∑—è—â–Ω—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞¬ª –≤ 1938 –≥–æ–¥—É —Å—Ç–∞–ª–∏ –≤—ã—Å—à–∏–º–∏ —Ç–æ—á–∫–∞–º–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –¥–≤—É—Ö –¥–≤–∏–∂–µ–Ω–∏–π –∏ –ø–æ–¥–≤–µ–ª–∏ –∏–º –∏—Ç–æ–≥. –ù–∞ ¬´–°—é—Ä—Ä–µ–∞–ª–∏—Å¬≠—Ç–∏—á–µ—Å–∫–æ–π —É–ª–∏—Ü–µ¬ª, –∑–∞ –º–∞–Ω–µ–∫–µ–Ω–∞–º–∏, –≤—ã—Å—Ç—Ä–æ–∏–≤—à–∏–º–∏—Å—è –≤ –ø—Ä–æ—Ö–æ–¥–µ –≤ –≥–ª–∞–≤–Ω—ã–π –∑–∞–ª, —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª–∏—Å—å –ø–ª–∞–∫–∞—Ç—ã, –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è, –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, –æ—Ç—Å—ã–ª–∞—é—â–∏–µ –∫ —Ä–∞–Ω–Ω–∏–º —ç—Ç–∞–ø–∞–º —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞. –í –≥–ª–∞–≤–Ω–æ–º –∑–∞–ª–µ, –∑–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ—Ç–≤–µ—á–∞–ª –ú–∞—Ä—Å–µ–ª—å –î—é—à–∞–Ω‚Äâ\n",
        " \n",
        ", –∞ –∑–∞ –æ—Å–≤–µ—â–µ¬≠–Ω–∏–µ ‚Äî –ú–∞–Ω –†—ç–π‚Äâ\n",
        " \n",
        ", –∫–∞—Ä—Ç–∏–Ω—ã 1920-—Ö –≥–æ–¥–æ–≤ –≤–∏—Å–µ–ª–∏ —Ä—è–¥–æ–º —Å –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–ª–æ —Ä–∞–∑–≤–∏—Ç–∏\n",
        "–µ —Å—é—Ä—Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ ¬´–∏–Ω—Ç–µ—Ä–Ω–∞—Ü–∏¬≠–æ–Ω–∞¬≠–ª–∞¬ª. –ó–∞—Ä–æ–¥–∏–≤—à–∏—Å—å –∫–∞–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ —Ç–µ—á–µ–Ω–∏–µ, –∫ –∫–æ–Ω—Ü—É 1930-—Ö –≥–æ–¥–æ–≤ —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º —É–∂–µ –æ–∫–æ–ª–æ 15 –ª–µ—Ç –≥–æ—Å–ø–æ–¥—Å—Ç–≤–æ–≤–∞–ª –≤ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∞–≤–∞–Ω–≥–∞—Ä–¥–µ –ü–∞—Ä–∏–∂–∞. –ü—Ä–µ–∂–¥–µ —á–µ–º –ø–æ–π—Ç–∏ –Ω–∞ —Å–ø–∞–¥ —Å –Ω–∞—á–∞–ª–æ–º\n",
        "–í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –æ–Ω —Å—Ç–∞–ª —á–∞—Å—Ç—å—é —Å–≤–µ—Ç—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã –ü–∞—Ä–∏–∂–∞ –∏ –¥–∞–∂–µ –¥–æ –Ω–µ–∫–æ—Ç–æ–†–æ–π\n",
        "—Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–∏—Å—è–≥–Ω—É–ª –≤—ã—Å–æ–∫–æ–π –º–æ–¥–µ, –ø–æ–¥–æ–±–Ω–æ —Ç–æ–º—É –∫–∞–∫ —Ä—É—Å—Å–∫–∏–π –∞–≤–∞–Ω–≥–∞—Ä–¥ ‚Äî –ø—É—Å—Ç—å —Å–æ–≤—Å–µ–º –∏–Ω–∞—á–µ ‚Äî –ø—Ä–∏—Å—è–≥–Ω—É \n",
        "–≤ —Å–≤–æ–µ –≤—Ä–µ–º—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏. –ò–∑—è—â–µ—Å—Ç–≤–æ —Å—Ç–∏–ª—è, —Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—é—Ä—Ä–µ–ª–∏–∑–º—É, —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–∞–ª–æ —ç—Ç–æ–º—É \n",
        "\n",
        "¬≠ —Å–±–ª–∏–∂–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä–æ–µ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, —É–ø—Ä–æ—á–∏–ª–æ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –æ–±—â–µ—Å—Ç–≤–µ. \n",
        "\n",
        "–û–¥–Ω–∞–∫–æ –ø–æ–Ω–∞—á–∞–ª—É –¥–ª—è –ª–∏—Ç–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ —Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤-–±—É–Ω—Ç–∞—Ä–µ–π, –Ω–∏—á—É—Ç—å –Ω–µ —Å—Ç—Ä–µ–º–∏–≤—à–∏—Ö—Å—è –∫ —Å–æ—Ü–∏–∞–ª—å¬≠–Ω–æ–º—É —É—Å–ø–µ—Ö—É, \n",
        "–±—ã–ª–∞ –∫—É–¥–∞ –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–≤—è–∑—å —Å –¥–∞–¥–∞–∏–∑–º–æ–º\"\"\"\n",
        "\n",
        "#UNCOMMENT TO UPLOAD============================================================\n",
        "\n",
        " \n",
        "if want_upload:\n",
        "  uploaded = interactive_upload('Protocol')[0]\n",
        "# \n",
        "_regex_addon = [\n",
        "    (re.compile(r'[¬≠]'), '-'),\n",
        "]\n",
        "TOKENS=tokenize_text( normalize_text(uploaded, replacements_regex+_regex_addon))\n",
        "\n",
        "\n",
        "\n",
        "class PF(AbstractPatternFactory):\n",
        "  def __init__(self):\n",
        "    AbstractPatternFactory.__init__(self, None)\n",
        "    self._build_ner_patterns()\n",
        "\n",
        "  def _build_ner_patterns(self):\n",
        "    def cp(name, tuples):\n",
        "      return self.create_pattern(name, tuples)\n",
        "   \n",
        "    cp('_custom', ( '', search_for, ''))\n",
        "    \n",
        "\n",
        "# ---\n",
        "pf = PF()\n",
        "\n",
        "av = PM.find_patterns(text_tokens=TOKENS, patterns=pf.patterns)\n",
        "\n",
        "\n",
        " \n",
        "fig = plt.figure(figsize=(20, 6))\n",
        "ax = plt.axes()\n",
        "for name in av.vectors:\n",
        "  ax.plot(av.get_best(name, relu_th=0.8), label='v:'+name, alpha=0.5)\n",
        "ax.plot([0.5]*av.size, label='0.5', alpha=0.2)\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text(TOKENS, av.get_by_name('_custom'), _range=(0,1) )\n",
        "# TOKENS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dWk5fP3p2uZ",
        "colab_type": "text"
      },
      "source": [
        "# Fuzzy Matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVCTVXXP17rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fuzzy_matcher import AttentionVectors\n",
        "from ml_tools import sum_probabilities, subtract_probability, relu, momentum_p\n",
        "import numpy as np\n",
        "from ml_tools import FixedVector, FixedVectors,Tokens\n",
        "\n",
        "\n",
        "def group_indices(_indices, cut_threshold=2):\n",
        "  indices = sorted(_indices)\n",
        "\n",
        "  _start = indices[0]\n",
        "  _prev = indices[0]\n",
        "  slices = []\n",
        "  \n",
        "  _n=len(indices)\n",
        "\n",
        "  for i_i in range(_n):\n",
        "    i = indices[i_i]\n",
        "\n",
        "    if i - _prev > cut_threshold:\n",
        "      slices.append(slice(_start, _prev + 1))\n",
        "      _start = i\n",
        "      \n",
        "    if  i_i == _n - 1:\n",
        "      slices.append(slice(_start, i + 1))\n",
        "      \n",
        "    _prev = i\n",
        "  \n",
        "  return slices\n",
        "  \n",
        "  \n",
        "  \n",
        "# group_indices([2,8])\n",
        "\n",
        "\n",
        "class FuzzyMatcher:\n",
        "  def __init__(self, av: AttentionVectors):\n",
        "    self.av = av\n",
        "    self.constraints = []\n",
        "    self.incusions = []\n",
        "    \n",
        "    \n",
        "\n",
        "  def _add(self, name, max_tokens, multiplyer, to_left):\n",
        "    assert self.av.has(name), f'there is no vector named {name}'\n",
        "    self.constraints.append((name, max_tokens, multiplyer, to_left))\n",
        "\n",
        "  def _include(self, name, include):\n",
        "    assert self.av.has(name), f'there is no vector named {name}'\n",
        "    self.incusions.append((name, include))\n",
        "\n",
        "  def after(self, name, max_tokens: int, multiplyer=1) -> 'FuzzyMatcher':\n",
        "    self._add(name, max_tokens, multiplyer,  False)\n",
        "    return self\n",
        "\n",
        "  def before(self, name, max_tokens: int, multiplyer=1) -> 'FuzzyMatcher':\n",
        "    self._add(name, max_tokens, multiplyer,  True)\n",
        "    return self\n",
        "\n",
        "  def excluding(self, name) -> 'FuzzyMatcher':\n",
        "    self._include(name, False)\n",
        "    return self\n",
        "\n",
        "  def including(self, name) -> 'FuzzyMatcher':\n",
        "    self._include(name, True)\n",
        "    return self\n",
        "  \n",
        "  def _get_by_prefix(self, prefix ) -> FixedVectors:\n",
        "    for name in self.av.get_names_by_pattern(prefix):\n",
        "      yield self.av.get_by_name(name )\n",
        "\n",
        "  def _get(self, name, p_threshold=0.5):\n",
        "    _relu = 0.8\n",
        "    if name[-1] == '*':\n",
        "      vectors = self._get_by_prefix(name[:-1] )\n",
        "      v = sum_probabilities( [relu(v, p_threshold) for v in vectors])\n",
        "    else:\n",
        "      v = self.av.get_by_name (name)\n",
        "      v = relu(v, p_threshold) \n",
        "      \n",
        "    return v\n",
        "  \n",
        "  def extract_name(self, attention: FixedVector, tokens: Tokens, cut_threshold = 2) -> [slice]:\n",
        "    best_indices = []\n",
        "    #     best_indices = sorted(np.argsort(attention)[::-1][:20])\n",
        "\n",
        "    for i in np.argsort(attention)[::-1][:20]:\n",
        "      if attention[i] > 0.001:\n",
        "        best_indices.append(i)\n",
        "    best_indices = sorted(best_indices)\n",
        "\n",
        "    if len(best_indices) == 0:\n",
        "      return []\n",
        "\n",
        "    \n",
        "    slices = group_indices(best_indices, cut_threshold)\n",
        "\n",
        "    _dict = {}\n",
        "    for s in slices:\n",
        "      key = '__'.join(tokens[s])\n",
        "      _sum = np.max(attention[s])\n",
        "      _dict[key] = (_sum, s)\n",
        "\n",
        "    _arr = [x[1] for x in sorted(_dict.values(), key=lambda item: -item[0])]\n",
        "\n",
        "    return _arr\n",
        "\n",
        "  def compile(self, p_threshold=0.33, bias=-0.33) -> np.ndarray:\n",
        " \n",
        "    # regions\n",
        "    _momented = []\n",
        "    _momented.append(np.zeros(self.av.size))\n",
        "    for name, max_tokens, multiplyer, to_left in self.constraints:\n",
        "      v = self._get(name, p_threshold) + bias\n",
        "      \n",
        "      v = relu(v, p_threshold)  # TODO: remove? do not ReLu here?\n",
        "      \n",
        "      momented = momentum_p(v, half_decay=max_tokens, left=to_left)\n",
        "      momented -= v\n",
        "      \n",
        "      momented *= multiplyer\n",
        "#       momented = relu(momented, 0.5)\n",
        "      _momented.append(momented)\n",
        "\n",
        "    m_result = sum_probabilities(_momented)\n",
        "\n",
        "    # incusions\n",
        "    _included = []\n",
        "    _included.append(np.zeros(self.av.size))\n",
        "    for name, include in self.incusions:\n",
        "      v = self._get(name, p_threshold) + bias\n",
        "      if include:\n",
        "        _included.append(relu(v, p_threshold))\n",
        "    i_result = sum_probabilities(_included)\n",
        "\n",
        "    # exclusions\n",
        "    _excluded = []\n",
        "    _excluded.append(np.zeros(self.av.size))\n",
        "    for name, include in self.incusions:\n",
        "      v = self._get(name, p_threshold)+bias\n",
        "      if not include:\n",
        "        _excluded.append(relu(v, p_threshold))\n",
        "    e_result = sum_probabilities(_excluded)\n",
        "\n",
        "    m_result = sum_probabilities([m_result, i_result])\n",
        "    m_result = subtract_probability(m_result, e_result)\n",
        "\n",
        "    #     result = relu(result, 0.001)\n",
        "    return m_result\n",
        "  \n",
        "  \n",
        "  \n",
        "FuzzyMatcher2=FuzzyMatcher\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvhS_Tz61CAD",
        "colab_type": "text"
      },
      "source": [
        "### Make patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HsEwrDx0-d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "from patterns import AbstractPatternFactory, FuzzyPattern\n",
        "\n",
        "from text_tools import acronym\n",
        "\n",
        "ORG_TYPES = {\n",
        "  'org_unknown': 'undefined',\n",
        "  'org_ao':   '–ê–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ',\n",
        "  'org_zao':  '–ó–∞–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ',\n",
        "  'org_oao':  '–û—Ç–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ',\n",
        "  'org_ooo':  '–û–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é',\n",
        "  'org_nc':   '–ù–µ–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
        "  'org_fpsi': '–§–æ–Ω–¥ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤',\n",
        "  'org_any_1': '–£—á—Ä–µ–∂–¥–µ–Ω–∏–µ',\n",
        "  'org_any_2': '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
        "  'org_gau': '–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ',\n",
        "  'org_roo': '–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'\n",
        "   \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CPF(AbstractPatternFactory):\n",
        "  def __init__(self):\n",
        "    AbstractPatternFactory.__init__(self, None)\n",
        "    self._build_ner_patterns()\n",
        "\n",
        "  def _build_ner_patterns(self):\n",
        "    def cp(name, tuples):\n",
        "      assert tuples[1] != '', name\n",
        "      return self.create_pattern(name, tuples)\n",
        "\n",
        "    for o_type in ORG_TYPES.keys():\n",
        "      if o_type != 'org_unknown':\n",
        "        n = ORG_TYPES[o_type]\n",
        "        cp(o_type, ('', n, ' ¬´'))\n",
        "        \n",
        "        an = acronym(n)\n",
        "        if(len(an)>1):\n",
        "          cp(o_type+'_acronym', ('', an, ' ¬´'))\n",
        "\n",
        "    \n",
        "    cp('_named_1', ( ',', '–∏–º–µ–Ω—É–µ–º–æ–µ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º', '¬´'))\n",
        "    cp('_named_2', ( ',', '–∏–º–µ–Ω—É–µ–º–æ–µ –¥–∞–ª–µ–µ', '¬´'))\n",
        "    cp('_named_4', ( ',', '–∏–º–µ–Ω—É–µ–º–æ–µ –¥–∞–ª–µ–µ', '‚Äì'))\n",
        "    cp('_named_5', ( ',', '–∏–º–µ–Ω—É–µ–º–æ–µ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º', '‚Äì'))\n",
        "    \n",
        "    cp('_place_1', ( '', '–º–µ—Å—Ç–æ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è', ''))\n",
        "    cp('_place_2', ( '', '–≥–æ—Ä–æ–¥', '–ì–æ—Å–∫–≤–∞'))\n",
        "    \n",
        "   \n",
        "#     cp('_named_3', ( ',', '–¥–∞–ª–µ–µ', '‚Äì ¬´'))\n",
        "    \n",
        "    \n",
        "    cp('_made_a_deal', ( '', '–∑–∞–∫–ª—é—á–∏–ª–∏', '–Ω–∞—Å—Ç–æ—è—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –æ –Ω–∏–∂–µ—Å–ª–µ–¥—É—é—â–µ–º'))\n",
        "    \n",
        "    cp('_acting_on_1', ( ',', '–¥–µ–π—Å—Ç–≤—É—é—â–µ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏', '—É—Å—Ç–∞–≤–∞'))\n",
        "    cp('_acting_on_2', ( ',', '–¥–µ–π—Å—Ç–≤—É—é—â–µ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏', '–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏'))\n",
        " \n",
        "    \n",
        "    cp('_deal_side_1', ( ',', '—Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã', ''))\n",
        "    cp('_deal_side_2', ( ',', '—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã', ''))\n",
        "    \n",
        "\n",
        "    cp('_in_face_1', ( '', '–≤ –ª–∏—Ü–µ', ''))\n",
        "    cp('_in_face_2', ( '', '–≤ –ª–∏—Ü–µ', '–ì–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞'))\n",
        "    \n",
        "    \n",
        "    \n",
        "    px = ', –∏–º–µ–Ω—É–µ–º–æ–µ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º'\n",
        "    cp('_alias_out.1', ( px, '–ü–æ–∫—É–ø–∞—Ç–µ–ª—å', ''))\n",
        "    cp('_alias_out.2', ( px, '–ó–∞–µ–º—â–∏–∫', ''))\n",
        "    cp('_alias_out.3', ( px, '–ê—Ä–µ–Ω–¥–∞—Ç–æ—Ä', ''))\n",
        "    cp('_alias_out.4', ( px, '–ñ–µ—Ä—Ç–≤–æ–≤–∞—Ç–µ–ª—å', ''))\n",
        "    cp('_alias_out.5', ( px, '–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å', ''))\n",
        "    cp('_alias_out.5', ( px, '–ó–∞–∫–∞–∑—á–∏–∫', ''))\n",
        "    \n",
        "    cp('_alias_in.1', ( px, '–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å', ''))\n",
        "    cp('_alias_in.2', ( px, '–ü—Ä–æ–¥–∞–≤–µ—Ü', ''))    \n",
        "    \n",
        "\n",
        "# ---\n",
        "contract_pattern_factory = CPF()\n",
        " \n",
        "\n",
        "for o_type in ORG_TYPES.keys():\n",
        "  print(acronym(ORG_TYPES[o_type]),'\\t', ORG_TYPES[o_type])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_oAuIsFwFpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in contract_pattern_factory.patterns:\n",
        "  print(p.name, f'\\t\"{p.prefix_pattern_suffix_tuple[1]}\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOLBeEAFp65f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "av = PM.find_patterns(text_tokens=TOKENS, patterns=contract_pattern_factory.patterns)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_B3i8-7PfL",
        "colab_type": "text"
      },
      "source": [
        "# Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLhqg3zVxn2",
        "colab_type": "text"
      },
      "source": [
        "### load files from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADQZo1lu7RgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gspread\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "# google_spread = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "\n",
        "import glob\n",
        "def read_documents(filename_prefix):\n",
        "  texts = {}\n",
        "  for file in glob.glob(filename_prefix+\"*.doc\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.doc FILE!!', file)\n",
        "\n",
        "  for file in glob.glob(filename_prefix+\"*.docx\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.docx FILE!!', file)\n",
        "      \n",
        "  return texts\n",
        "\n",
        "contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "contracts = read_documents(contracts_filename_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2mkCARg1dC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BAD_DOCS=\"\"\"\n",
        "–î–ö–ü –ü–æ–ª–æ–≤–∏–Ω–Ω–æ–∏ÃÜ –ù–ë –∏ –ê–ó–°.docx\n",
        "2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx\n",
        "–ü—Ä–æ–µ–∫—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ –ü—É—Ä –∞–¥–º.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä –∞—Ä–µ–Ω–¥—ã.docx\n",
        "5. –¥–æ–≥–æ–≤–æ—Ä –ó–∞–ª–æ–≥–∞.docx\n",
        "4. –î–æ–≥–æ–≤–æ—Ä –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç_–ü–µ—Ç—Ä–æ–≤.docx\n",
        "1.1. –î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä 4.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä 5.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä1.docx\n",
        "–ü—Ä–æ–µ–∫—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ –ü—É—Ä –∞–¥–º - –±–ª–∞–≥–æ—Ç–≤.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä 6.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä 3.docx\n",
        "–î–û–ì–û–í–û–†_–î–µ—Ç—Å–∫–∏–∏ÃÜ –¥–æ–º.docx\n",
        "–ì–ü–ù 3 –î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è 21.02.2019.doc\n",
        "\"\"\"\n",
        "\n",
        "BAD_DOCS = [ '/content/gdrive/My Drive/GazpromOil/Contracts/'+x.strip() for x in BAD_DOCS.split('\\n') if x.strip()!='' ]\n",
        "for i in  range(len(BAD_DOCS)):\n",
        "  print(i, BAD_DOCS[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIVy3aa81uH7",
        "colab_type": "text"
      },
      "source": [
        "### Debug, see your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtLvU4tmA9Po",
        "colab_type": "text"
      },
      "source": [
        "#### Match aliases "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHXmBtUHA_gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt = contracts[BAD_DOCS[3]] \n",
        "\n",
        "\n",
        "from contract_parser import make_company_names_fm, match_contractor_aliases\n",
        "from text_tools import hot_punkt, hot_quotes\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMT7xx04BEp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhr2-tXBDNvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# def match_contractor_aliases(_av):\n",
        "#   alias_matcher = FuzzyMatcher2(_av)\n",
        "  \n",
        "# #   alias_matcher.including('_named_*')\n",
        "\n",
        "#   alias_matcher.after('_named_*', 10, 1)\n",
        "#   alias_matcher.before('_in_face*', 5, 0.8) \n",
        "#   alias_matcher.excluding('_named_*')\n",
        "#   alias_matcher.excluding('_punkt')\n",
        "#   alias_matcher.including('_alias_*')\n",
        "  \n",
        "  \n",
        "#   alias_matcher.after('_quotes_open', 5, 0.5) .before('_quotes_closing', 5, 0.5) \n",
        "#   alias_matcher .before('_deal_side_*', 5, 0.5)\n",
        "  \n",
        " \n",
        "#   alias_attention = alias_matcher.compile(p_threshold=0.4)\n",
        "# #   aliaces_indices = sorted(np.argsort(alias_attention)[::-1][:2])\n",
        "  \n",
        "#   alias_attention=relu(alias_attention, 0.9)\n",
        "#   return alias_attention\n",
        "\n",
        "\n",
        "# _av, token, alias_attentions = process_contract_aliases(txt)\n",
        "\n",
        "\n",
        "\n",
        "# alias_attention = match_contractor_aliases(_av)\n",
        "# _av.add('alias_attention', alias_attention)\n",
        "\n",
        "\n",
        "# GLOBALS__['renderer'].render_color_text(tokens, _av.get_by_name('alias_attention'),  _range=[0, 1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r1zmvnVDST8",
        "colab_type": "text"
      },
      "source": [
        "#### Find NEs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t80kJA8h1tpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "txt = contracts[BAD_DOCS[1]]\n",
        "\n",
        "def process_contract(txt):\n",
        "\n",
        "  tokens = tokenize_text(normalize_text(txt, replacements_regex))\n",
        "  tokens = tokens[:200]\n",
        "  \n",
        "#   print(tokens)\n",
        "  _av = PM.find_patterns(text_tokens=tokens, patterns=contract_pattern_factory.patterns)\n",
        "  _quotes_open, _quotes_closing = hot_quotes(tokens)\n",
        "  _av.add('_quotes_open', _quotes_open * 1)\n",
        "  _av.add('_quotes_closing', _quotes_closing * 1)\n",
        "  _av.add('_punkt', hot_punkt(tokens))\n",
        "  \n",
        "  \n",
        "  alias_attention = match_contractor_aliases(_av)\n",
        "  _av.add('alias_attention', relu(alias_attention, 0.8))\n",
        "  \n",
        "  \n",
        "  return _av, tokens\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "# txt = uploaded \n",
        "_av, tokens = process_contract(txt)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPHdf4f8mE4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "_fm, attention, alias_attention = make_company_names_fm(_av, relu_th=0.9)\n",
        "# GLOBALS__['renderer'].render_color_text(tokens, alias_attention,  _range=[0, 1])\n",
        "# print()\n",
        "\n",
        "_mx = np.max(attention) * 0.95\n",
        "print(_mx)\n",
        "attention = relu(attention, max(0.85, _mx))\n",
        "slices = _fm.extract_name(attention, tokens)\n",
        "for s in slices[:2]:\n",
        "  GLOBALS__['renderer'].render_color_text(tokens[s], attention[s], _range=(0,1))\n",
        "print(\"-\"*100)\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text(tokens, attention,  _range=[0, 1])\n",
        "GLOBALS__['renderer'].render_color_text(tokens, _av.get_by_name('alias_attention'),  _range=[0, 1])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em0u1r_EncbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgfEFnnBndjn",
        "colab_type": "text"
      },
      "source": [
        "## Batch: all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2TATqHI2B3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# from fuzzy_matcher import FuzzyMatcher\n",
        "from ml_tools import relu, cut_above\n",
        "from text_normalize import replacements_regex, normalize_text\n",
        "from text_tools import hot_quotes, my_punctuation, Tokens, tokenize_text, untokenize\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "def max_relu(x, a, k):\n",
        "  _mx = np.max(x) * k\n",
        "  print(_mx)\n",
        "  return relu(x, max(a, _mx))\n",
        "\n",
        "fc = 0\n",
        "export_string_all = ''\n",
        "for fn in contracts:#BAD_DOCS:# \n",
        "  fc += 1\n",
        "  print()\n",
        "  print(fc, '=' * 100)\n",
        "  print(fc, fn)\n",
        "  txt = contracts[fn]\n",
        "  tokens = tokenize_text(normalize_text(txt, replacements_regex))\n",
        "  tokens = tokens[:300]  # TODO: trimming just for the batch to speed things up\n",
        "\n",
        "  _av = PM.find_patterns(text_tokens=tokens, patterns=contract_pattern_factory.patterns)\n",
        "  _quotes_open, _quotes_closing = hot_quotes(tokens)\n",
        "  _av.add('_quotes_open', _quotes_open * 1)\n",
        "  _av.add('_quotes_closing', _quotes_closing * 1)\n",
        "  _av.add('_punkt', hot_punkt(tokens))\n",
        " \n",
        "\n",
        "  _fm, attention, alias_attention = make_company_names_fm(_av, relu_th=0.85)\n",
        "  \n",
        "#   attention = max_relu(attention, 0.85, 0.97)\n",
        "#   alias_attention = max_relu(alias_attention, 0.85, 0.9)\n",
        "  \n",
        "  \n",
        "  _as = sorted(_fm.extract_name(alias_attention, tokens)[0:2], key=lambda item: item.start)\n",
        "  print(\"Aliases:\")\n",
        "  for s in _as:\n",
        "    GLOBALS__['renderer'].render_color_text(tokens[s], alias_attention[s], _range=(0,1))\n",
        "    \n",
        "#   export_string += f'{fn.split(\"/\")[-1]}\\t{tokens[i1]}\\t{tokens[i2]}\\t{alias_attention[i1]}\\t{alias_attention[i2]}\\n'\n",
        "  \n",
        "  \n",
        "  print(\"NEs:\")\n",
        "  _nm = sorted(_fm.extract_name(attention, tokens)[0:2], key=lambda item: item.start)\n",
        "  for s in _nm:\n",
        "    GLOBALS__['renderer'].render_color_text(tokens[s], attention[s], _range=(0,1))\n",
        "\n",
        "  export_string=''\n",
        "  export_string += f'{fn.split(\"/\")[-1]}\\t'\n",
        " \n",
        "  if len(_as)==2:\n",
        "    export_string += f'{untokenize(tokens[_as[0]])}\\t{untokenize(tokens[_as[1]])}\\t'\n",
        "  else:\n",
        "    export_string += '\\t\\t'\n",
        "    \n",
        "  if len(_nm)==2  :\n",
        "    export_string += f'{untokenize(tokens[_nm[0]])}\\t{untokenize(tokens[_nm[1]])}\\t'\n",
        "  else:\n",
        "    export_string += '\\t\\t'\n",
        "    \n",
        "    \n",
        "  export_string +='\\n'\n",
        "  \n",
        "  print(export_string)\n",
        "  export_string_all += export_string\n",
        "#   # --------------------------------QUOTES attention-\n",
        "#   _quotes_open, _quotes_closing = hot_quotes(tokens)\n",
        "#   _av.add('_quotes_open', _quotes_open * 1)\n",
        "#   _av.add('_quotes_closing', _quotes_closing * 1)\n",
        "  \n",
        "#   _av.add('hot_punkt', hot_punkt(tokens))\n",
        "#   # ----------------------------END OF----QUOTES attention-\n",
        "\n",
        "  \n",
        "#   aliaces_indices, alias_attention = match_contractor_aliases(_av)\n",
        "#   _av.add('alias_attention', alias_attention)\n",
        "    \n",
        "#   _fm = FuzzyMatcher2(_av)\n",
        "  \n",
        "#   _fm.after('org_*',10).excluding('org_*')\n",
        "#   _fm.before('_quotes_closing', 10).after('_quotes_open', 10)\n",
        "  \n",
        " \n",
        "#   _fm.after('_deal_side_1', 30)  \n",
        "#   _fm.before('_named_*', 40) \n",
        "#   _fm.before('_in_face', 10)\\\n",
        "#     .before('_acting_on', 40) \\\n",
        "#     .before('_deal_side_*', 100)\\\n",
        "#     .before('_made_a_deal', 200)\\\n",
        "#     .before('_in_face', 10)\\\n",
        "  \n",
        "#   _fm.excluding('alias_attention').excluding('hot_punkt') \n",
        "  \n",
        " \n",
        "    \n",
        "# #   alias_attention = relu(alias_attention, min( )  )\n",
        "\n",
        "#   _sel_t = [tokens[aliaces_indices[0]], '---', tokens[aliaces_indices[1]]]\n",
        "#   _sel_v = np.array([alias_attention[aliaces_indices[0]], 0, alias_attention[aliaces_indices[1]]])\n",
        "\n",
        "# #   GLOBALS__['renderer'].render_color_text(_sel_t, _sel_v, _range=[0, 1])\n",
        "\n",
        "#   i1 = aliaces_indices[0]\n",
        "#   i2 = aliaces_indices[1]\n",
        "\n",
        "#   slices = _fm.extract_name(attention, tokens)\n",
        "#   export_string += f'{fn.split(\"/\")[-1]}\\t{tokens[i1]}\\t{tokens[i2]}\\t{alias_attention[i1]}\\t{alias_attention[i2]}\\n'\n",
        "\n",
        "#   alias_attention_sl = slice(i1 - 10, i2 + 10)\n",
        "# #   GLOBALS__['renderer'].render_color_text(tokens[alias_attention_sl], alias_attention[alias_attention_sl], _range=(0,1))\n",
        "\n",
        "# #   attention = _fm.compile(bias=-0.6)\n",
        "# #   attention = relu(attention, 0.9 * np.max(attention))\n",
        "  \n",
        "#   best_indices = np.argsort(alias_attention)[::-1][:10]\n",
        "#   best_index = np.min(best_indices)\n",
        "#   last_index = np.max(best_indices)\n",
        "    \n",
        "#   print('NEs:')\n",
        "  \n",
        "#   best_indices = np.argsort(attention)[::-1][:10]\n",
        "#   print(\"TOP TEN:\")\n",
        "#   for bi in best_indices:\n",
        "#     GLOBALS__['renderer'].render_color_text(tokens[bi:bi+1], attention[bi:bi+1], _range=(0,1))\n",
        "#   print(\"-------------------------:TOP TEN\")\n",
        "  \n",
        "#   slices = _fm.extract_name(attention, tokens)\n",
        "#   for s in slices:\n",
        "#     GLOBALS__['renderer'].render_color_text(tokens[s], attention[s], _range=(0,1))\n",
        "\n",
        "#   print('quote:')\n",
        "# #   last_index = min(last_index, best_index+100)\n",
        "# # #   print('BI:', best_indices)\n",
        "#   _slice = slice(best_index, last_index+2)\n",
        "#   GLOBALS__['renderer'].render_color_text(tokens[_slice], attention[_slice], _range=(0,1))\n",
        "\n",
        "\n",
        "print(export_string_all)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}