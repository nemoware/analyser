{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev Charters.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "9ilOKSGa0zPf",
        "eCveVZqvNhsr",
        "q1yEWke4nnZA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3f7JUyo5i1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Params\n",
        " \n",
        "\n",
        "document_parser_lib_version='1.1.14'#@param {type:\"string\"}\n",
        "charter_filename='/content/\\u0413\\u041F\\u041D \\u0423\\u0441\\u0442\\u0430\\u0432.docx'#@param {type:\"string\"}\n",
        "# _git_branch = 'protocols-4'\n",
        "_git_branch = 'charters-charity'#@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-DlKATL2JQ",
        "colab_type": "text"
      },
      "source": [
        "# INIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPEjJV3Elb8t",
        "colab_type": "text"
      },
      "source": [
        "## pull code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPFNIoGZL198",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "ÐÐ¸Ñ‡Ñ‚Ð¾ = None\n",
        "\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ðŸ¦Š GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('â¤ï¸importing Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mlpzzdNAWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyjarowinkler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMB4pkuu2wi",
        "colab_type": "text"
      },
      "source": [
        "### Init document-parser lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.13.1\n",
        "\n",
        "lib_version = document_parser_lib_version\n",
        "import os\n",
        "if not os.path.isfile(f'document-parser-{lib_version}-distribution.zip'):\n",
        "  !wget https://github.com/nemoware/document-parser/releases/download/$lib_version/document-parser-$lib_version-distribution.zip\n",
        "if not os.path.isdir(f'document-parser-{lib_version}'):\n",
        "  !unzip document-parser-$lib_version-distribution.zip\n",
        "\n",
        " \n",
        "os.environ ['documentparser']=f'/content/document-parser-{lib_version}'\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "wp = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNmYb03dFcJ9",
        "colab_type": "text"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fbSX2h2MedM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from analyser.contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from analyser.contract_patterns import ContractPatternFactory\n",
        "from analyser.legal_docs import LegalDocument\n",
        " \n",
        "from analyser.ml_tools import *\n",
        "\n",
        "# from headers_detector import doc_features, load_model, make_headline_attention_vector\n",
        "from analyser.hyperparams import HyperParameters\n",
        "from analyser.protocol_parser import protocol_votes_re\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ilOKSGa0zPf",
        "colab_type": "text"
      },
      "source": [
        "## render tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1jXK-Y60yHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----\n",
        "import matplotlib as mpl\n",
        "from analyser.documents import TextMap\n",
        "from colab_support.renderer import HtmlRenderer\n",
        "from analyser.legal_docs import DocumentJson\n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range, separator=separator)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range, separator=separator)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  for tag in cd.tags:\n",
        "    span = tag.span\n",
        "    _map = cd.tokenization_maps[tag.span_map]\n",
        "    print(tag)\n",
        "    \n",
        "def json2html(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['words'])\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        " \n",
        "  for hd in cd.headers:    \n",
        "    span = hd['span']      \n",
        "    markup_vector[span[0]:span[1]] += 2\n",
        "\n",
        "  for tag in cd.attributes:\n",
        "    if 'span' in  cd.attributes[tag]:\n",
        "      span = cd.attributes[tag]['span']\n",
        "      confidence = cd.attributes[tag]['confidence']\n",
        "      markup_vector[span[0]:span[1]] += confidence\n",
        "    else:\n",
        "      warnings.warn(f'{tag} has no span')\n",
        "\n",
        "  return renderer_.to_color_text(wordsmap.tokens, markup_vector)\n",
        "\n",
        "def print_headers(d: LegalDocument):\n",
        "  for p in d.paragraphs:\n",
        "    print('\\t --> ðŸ“‚', d.substr(p.header))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCveVZqvNhsr",
        "colab_type": "text"
      },
      "source": [
        "## ðŸ’… Init Embedder(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3k194xUFy20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from protocol_parser import  ProtocolPatternFactory\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        "# from contract_patterns import ContractPatternFactory\n",
        "elmo_embedder = ElmoEmbedder()\n",
        "elmo_embedder_default = ElmoEmbedder(layer_name=\"default\")\n",
        "\n",
        "# protocols_factory = ProtocolPatternFactory(elmo_embedder)\n",
        "# contracts_factory = ContractPatternFactory(elmo_embedder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to0MzdyWjEWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "from analyser.legal_docs import tokenize_doc_into_sentences_map, ContractValue\n",
        "from analyser.ml_tools import *\n",
        "from analyser.parsing import ParsingContext\n",
        "from analyser.patterns import *\n",
        " \n",
        "from analyser.text_tools import *\n",
        "\n",
        "# legal_docs.py\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTvjEfAxPzeI",
        "colab_type": "text"
      },
      "source": [
        "# Charter parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezyyvDYGdEs5",
        "colab_type": "text"
      },
      "source": [
        "## classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ie4ehdhE1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum, unique, EnumMeta\n",
        "from typing import List\n",
        "from analyser.structures import DisplayStringEnumMeta, CharterSubject\n",
        "from analyser.legal_docs import embedd_sentences, LegalDocumentExt\n",
        "from analyser.charter_parser import competence_headline_pattern_prefix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhNwA4xxP1kO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from analyser.embedding_tools import AbstractEmbedder\n",
        "from analyser.legal_docs import map_headlines_to_patterns\n",
        "from analyser.charter_parser import embedd_charter_subject_patterns, get_charter_subj_attentions, CharterParser, CharterDocument\n",
        "from analyser.contract_parser import find_value_sign_currency_attention\n",
        " \n",
        "\n",
        "def collect_sentences_having_constraint_values(subdoc: LegalDocumentExt, contract_values: [ContractValue]):\n",
        "  # collect sentences having constraint values\n",
        "  unique_sentence_spans = []\n",
        "  for contract_value in contract_values:\n",
        "    contract_value_sentence_span = subdoc.tokens_map.sentence_at_index(contract_value.parent.span[0],\n",
        "                                                                        return_delimiters=False)\n",
        "    if contract_value_sentence_span not in unique_sentence_spans:\n",
        "      unique_sentence_spans.append(contract_value_sentence_span)\n",
        "  # --\n",
        "  unique_sentence_spans = merge_colliding_spans(unique_sentence_spans, eps=1)\n",
        "  return unique_sentence_spans\n",
        "\n",
        "class CharterParser2(CharterParser):\n",
        "  def __init__(self, embedder: AbstractEmbedder = None, elmo_embedder_default: AbstractEmbedder = None):\n",
        "    super().__init__(embedder, elmo_embedder_default)\n",
        "  \n",
        "  \n",
        "\n",
        "  def find_attributes(self, charter: CharterDocument) -> CharterDocument:\n",
        "\n",
        "    if charter.sentences_embeddings is None:\n",
        "      # lazy embedding\n",
        "      self._ebmedd(charter)\n",
        "\n",
        "    # reset for preventing doubling tags\n",
        "    charter.margin_values = []\n",
        "    charter.constraint_tags = []\n",
        "    charter.charity_tags = []\n",
        "    charter.org_levels = []\n",
        "    charter.org_level_tags = []\n",
        "    # --------------\n",
        "    # (('Pattern name', 16), 0.8978644013404846),\n",
        "    patterns_by_headers = map_headlines_to_patterns(charter,\n",
        "                                                    self.patterns_named_embeddings,\n",
        "                                                    self.elmo_embedder_default)\n",
        "\n",
        "    _parent_org_level_tag_keys = []\n",
        "    for p_mapping in patterns_by_headers:\n",
        "      # kkk += 1\n",
        "\n",
        "      _paragraph_id = p_mapping[0][1]\n",
        "      _pattern_name = p_mapping[0][0]\n",
        "\n",
        "      paragraph_body = charter.paragraphs[_paragraph_id].body\n",
        "      confidence = p_mapping[1]\n",
        "      _org_level_name = _pattern_name.split('/')[-1]\n",
        "      org_level: OrgStructuralLevel = OrgStructuralLevel[_org_level_name]\n",
        "      subdoc = charter.subdoc_slice(paragraph_body.as_slice())\n",
        "\n",
        "      parent_org_level_tag = SemanticTag(f\"{org_level.name}\", org_level.name, paragraph_body.span)\n",
        "      parent_org_level_tag.confidence = confidence\n",
        "\n",
        "      constraint_tags, values, subject_attentions_map = self.attribute_charter_subjects(subdoc, self.subj_patterns_embeddings,\n",
        "                                                                parent_org_level_tag)\n",
        "      for value in values:\n",
        "        value += subdoc.start  # TODO: move into attribute_charter_subjects\n",
        "\n",
        "      for constraint_tag in constraint_tags:\n",
        "        constraint_tag.offset(subdoc.start)  # TODO: move into attribute_charter_subjects\n",
        "\n",
        "      charter.margin_values += values  # TODO: collect all, then assign to charter\n",
        "      charter.constraint_tags += constraint_tags\n",
        "\n",
        "      if values:\n",
        "        _key = parent_org_level_tag.get_key()\n",
        "        if _key in _parent_org_level_tag_keys:  # number keys to avoid duplicates\n",
        "          parent_org_level_tag.kind = _key + f\"-{len(_parent_org_level_tag_keys)}\"\n",
        "        charter.org_levels.append(parent_org_level_tag)  # TODO: collect all, then assign to charter\n",
        "        _parent_org_level_tag_keys.append(_key)\n",
        "\n",
        "      charity_subj_av_words = subject_attentions_map[CharterSubject.Charity]['words']\n",
        "      charity_tag = find_charity_paragraphs(parent_org_level_tag, subdoc, charity_subj_av_words )\n",
        "      print('-----charity_tag', charity_tag)\n",
        "      if charity_tag is not None:\n",
        "        charter.charity_tags.append(charity_tag)\n",
        "\n",
        "    return charter\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "  def attribute_charter_subjects(self, subdoc: LegalDocumentExt, emb_subj_patterns, parent_org_level_tag: SemanticTag):\n",
        "    \"\"\"\n",
        "    :param subdoc:\n",
        "    :param emb_subj_patterns:\n",
        "\n",
        "          emb_subj_patterns[subj] = {\n",
        "            'patterns':patterns,\n",
        "            'embedding':patterns_emb\n",
        "          }\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "    contract_values: [ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "    # -------------------\n",
        "\n",
        "    # collect sentences having constraint values\n",
        "    unique_sentence_spans = collect_sentences_having_constraint_values(subdoc, contract_values)\n",
        "\n",
        "    # attribute sentences to subject\n",
        "    constraint_tags = []\n",
        "\n",
        "    for sentence_number, contract_value_sentence_span in enumerate(unique_sentence_spans, start=1):\n",
        "\n",
        "      max_confidence = 0\n",
        "      best_subject: CharterSubject = CharterSubject.Other\n",
        "\n",
        "      for subj in subject_attentions_map.keys():\n",
        "        av = subject_attentions_map[subj]['words']\n",
        "\n",
        "        confidence_region = av[contract_value_sentence_span[0]:contract_value_sentence_span[1]]\n",
        "        confidence = estimate_confidence_by_mean_top_non_zeros(confidence_region)\n",
        "\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          best_subject = subj\n",
        "\n",
        "      #\n",
        "      constraint_tag = SemanticTag(SemanticTag.number_key(best_subject.name, sentence_number),\n",
        "                                   best_subject.name, contract_value_sentence_span,\n",
        "                                   parent=parent_org_level_tag)\n",
        "      constraint_tag.confidence = max_confidence\n",
        "      constraint_tags.append(constraint_tag)\n",
        "\n",
        "      # nest values\n",
        "      for contract_value in contract_values:\n",
        "        if constraint_tag.is_nested(contract_value.parent.span):\n",
        "          contract_value.parent.set_parent_tag(constraint_tag)\n",
        "\n",
        "      self._rename_margin_values_tags(contract_values)\n",
        "\n",
        "    return constraint_tags, contract_values, subject_attentions_map\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "###-------------------\n",
        "charter_parser = CharterParser2(elmo_embedder, elmo_embedder_default)\n",
        "results = wp.read_doc(charter_filename)\n",
        "# results = wp.read_doc('/content/Ð•Ð® Ð£ÑÑ‚Ð°Ð².docx')\n",
        "\n",
        "for d in results['documents'][:1]:  # XXX\n",
        "  if 'CHARTER' == d['documentType']:    \n",
        "    charter = join_paragraphs(d, 'no_id')    \n",
        "\n",
        "print_headers(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPX8aEVlLQee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from analyser.parsing import AuditContext\n",
        "## PHASE 1\n",
        "charter_parser.find_org_date_number(charter, AuditContext())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gY0ozfmu9CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## PHASE 2\n",
        "from analyser.contract_parser import _find _most_relevant_paragraph, find_value_sign_currency_attention\n",
        "def find_charity_paragraphs(parent_org_level_tag: SemanticTag, subdoc: LegalDocument,\n",
        "                            charity_subject_attention: FixedVector) -> SemanticTag:\n",
        "  paragraph_span, confidence, paragraph_attention_vector = _find_most_relevant_paragraph(subdoc,\n",
        "                                                                                         charity_subject_attention,\n",
        "                                                                                         min_len=20,\n",
        "                                                                                         return_delimiters=False)\n",
        "\n",
        "  if confidence > HyperParameters.charter_charity_attention_confidence:\n",
        "    subject_tag = SemanticTag(CharterSubject.Charity.name, CharterSubject.Charity.name, paragraph_span,\n",
        "                              parent=parent_org_level_tag)\n",
        "    subject_tag.offset(subdoc.start)\n",
        "    subject_tag.confidence = confidence\n",
        "    return subject_tag\n",
        "\n",
        "\n",
        "charter_parser.find_attributes(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1p08Mffvdvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if False:\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  plt.figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "  sns.set(font_scale=1)\n",
        "  ax = sns.heatmap(globals()['header_to_pattern_distances_'],  fmt=\"\", annot_kws={\"size\": 8}, vmin=0, vmax=1, yticklabels=charter.headers_as_sentences())\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkBu4zdkNl_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "renderer_.render_color_text(charter.tokens_cc, charter.get_tags_attention() , _range=(0,5) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgoO_XpMoNda",
        "colab_type": "text"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUIX6Mks9ife",
        "colab_type": "text"
      },
      "source": [
        "### constraint_tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEtzKsuk9lsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tag in charter.org_levels:\n",
        "  print (tag.get_parent())\n",
        "  print (tag)\n",
        "\n",
        "  sub = charter [tag.as_slice()]\n",
        "  renderer_.render_color_text(sub.tokens_cc, sub.get_tags_attention() , _range=(-5,5) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWMpB54Z9mpE",
        "colab_type": "text"
      },
      "source": [
        "### Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d66dqRR-oexX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tag in charter.get_tags():\n",
        "  print (tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1yEWke4nnZA",
        "colab_type": "text"
      },
      "source": [
        "## Find org level sections\n",
        " - find all headers spans\n",
        " - compare headers with patterns \n",
        " - find interresting spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piZZmrNTnxJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_slices = [p.header.as_slice() for p in charter.paragraphs]\n",
        "header_slices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUpmN9C0saqE",
        "colab_type": "text"
      },
      "source": [
        "remap to sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlDC8PFHseJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_slices_sent = charter.tokens_map.remap_slices(header_slices, charter.sentence_map)\n",
        "header_slices_sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRy1U1NiuaN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "section_ends = [ x[0] for x in header_slices_sent ] + [ len(charter.sentence_map) ] #+end of doc\n",
        "section_ends"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtzK_vohox1E",
        "colab_type": "text"
      },
      "source": [
        "Available sentence pattern (names)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgkFOANqotUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter.distances_per_sentence_pattern_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR3URDk8SAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fi \n",
        "# av_competence_headline_pattern = max_exclusive_pattern_by_prefix(charter.distances_per_sentence_pattern_dict, competence_headline_pattern_prefix)\n",
        "# av_competence_headline_pattern = relu(av_competence_headline_pattern, 0.66)\n",
        "\n",
        "# av_interresting_headers = headers_att * av_competence_headline_pattern\n",
        "# #words map\n",
        "# # sections_spans_words = charter.sentence_map.remap_slices(v, charter.tokens_map)\n",
        " \n",
        "# # v = charter.distances_per_sentence_pattern_dict['headline.head.directors.n']# - 0.1*charter.distances_per_sentence_pattern_dict['headline.head.directors.not'] \n",
        "\n",
        "# renderer_.render_color_text(charter.sentence_map.tokens, av_interresting_headers, _range=(0,1), separator='Â¶<br>')\n",
        "# # renderer_.render_color_text(charter.tokens_cc, sections_spans_words, _range=(0,1) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Q9zLUBPmU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter_parser.patterns_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1_1_XdNiWX",
        "colab_type": "text"
      },
      "source": [
        "## charter_subjects_attribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWtENbeDPe2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del get_charter_subj_attentions\n",
        "\n",
        "from analyser.legal_docs import remap_attention_vector\n",
        "from analyser.contract_parser import _find_most_relevant_paragraph, find_value_sign_currency_attention\n",
        "from analyser.legal_docs import remap_attention_vector, ContractValue, LegalDocumentExt\n",
        "from analyser.ml_tools import *\n",
        "\n",
        " \n",
        "from analyser.charter_parser import find_charity_paragraphs, get_charter_subj_attentions\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "###DEBUG attribute_charter_subjects -------------------\n",
        "\n",
        "\n",
        "charter.margin_values = []\n",
        "charter.constraint_tags=[]\n",
        "# --------------\n",
        "filtered = [p_mapping for p_mapping in patterns_by_headers if p_mapping]\n",
        "for p_mapping in filtered:\n",
        "  print('-'*40)\n",
        "  \n",
        "  paragraph = p_mapping[4]\n",
        "  org_level_name = p_mapping[1].split('/')[-1]\n",
        "  org_level = OrgStructuralLevel[org_level_name]\n",
        "  print(org_level_name)\n",
        "  subdoc = charter.subdoc_slice(paragraph.body.as_slice())\n",
        "  parent_org_level_tag = SemanticTag(org_level.name, org_level, paragraph.body.span)\n",
        "\n",
        "  constraint_tags, values = charter_parser.attribute_charter_subjects(subdoc, charter_parser.subj_patterns_embeddings,\n",
        "                                                            parent_org_level_tag)\n",
        "  \n",
        "  charter.margin_values += values\n",
        "  charter.constraint_tags += constraint_tags\n",
        "  for c in constraint_tags:\n",
        "    print(c)\n",
        "\n",
        "# for p_mapping in patterns_by_headers:\n",
        "#   print()\n",
        "#   print()\n",
        "#   if p_mapping:\n",
        "#     print('p_mapping', p_mapping)\n",
        "#     paragraph =  p_mapping[4]\n",
        "#     org_level_name =  p_mapping[1].split('/')[-1]\n",
        "#     org_level = OrgStructuralLevel[org_level_name]\n",
        "\n",
        "#     subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "#     parent_org_level_tag = SemanticTag(org_level.name, org_level, paragraph.body.span)\n",
        "\n",
        "#     constraint_tags, values = charter_parser.attribute_charter_subjects(subdoc, self.subj_patterns_embeddings, parent_org_level_tag)\n",
        "\n",
        "#     print(org_level)\n",
        "#     print('constraint_tags')\n",
        "#     for c in constraint_tags:\n",
        "#       print(c)\n",
        "#     print('values')\n",
        "\n",
        "for c in charter.margin_values:\n",
        "  print(c.value)\n",
        "  print ('quote:', charter.substr (c.parent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoPDev4ZRvk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter.margin_values=[]\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    # paragraph =  p_mapping[4]\n",
        "    # org_level =  p_mapping[1]\n",
        "\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level_name =  p_mapping[1].split('/')[-1] # TODO FIX it, looks weird\n",
        "    org_level = OrgStructuralLevel[org_level_name]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "    parent_org_level_tag = SemanticTag(org_level.name, org_level, paragraph.body.span)\n",
        "\n",
        "    consent_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "                                                                consent_patterns,\n",
        "                                                                consent_patterns_emb)\n",
        "    \n",
        "    # charity_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "    #                                                             consulting_patterns,\n",
        "    #                                                             consulting_patterns_emb)\n",
        "    \n",
        "    # print(order_patterns_distances.keys())\n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "\n",
        "    consent_v = relu(max_exclusive_pattern_by_prefix(consent_patterns_distances, 'consent/'),0.5)\n",
        "    consent_words = remap_attention_vector(consent_v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "\n",
        "    # charity_subj_av = relu(max_exclusive_pattern_by_prefix(charity_patterns_distances, 'consulting/'),0.6)\n",
        "    # charity_subj_av_words = remap_attention_vector(charity_subj_av, subdoc.sentence_map, subdoc.tokens_map)\n",
        "    # order_patterns_distances['order/consent']\n",
        "    # renderer_.render_color_text(subdoc.sentence_map.tokens, v, _range=(0,1), separator='ðŸ˜± <br>')\n",
        "    \n",
        "    # charity_patterns_distances_words = remap_attention_vector(v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "\n",
        "    \n",
        "    \n",
        "    # dd = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "    # charity_subj_av_words = dd[CharterSubject.Charity]['words']\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "\n",
        "    charity_subj_av_words = subject_attentions_map[CharterSubject.Charity]['words']\n",
        "    charity_tag = find_charity_paragraphs(parent_org_level_tag, subdoc, (charity_subj_av_words+consent_words)/2)\n",
        "    print(charity_tag)\n",
        "    if charity_tag is not None:\n",
        "      charter.charity_tags.append(charity_tag)\n",
        "\n",
        " \n",
        "\n",
        "    constraint_tags, values = attribute_charter_subjects(subdoc, emb_subj_patterns, parent_org_level_tag)\n",
        "    charter.margin_values += values\n",
        "\n",
        "    # renderer_.render_color_text(subdoc.tokens_cc, charity_subj_av_words + numbers_attention , _range=(0,2) )\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GfSFERvLrGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(charter.tokens_cc, charter.get_tags_attention() , _range=(0,2) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oZp0Fc8RhPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN4SKxM_QETF",
        "colab_type": "text"
      },
      "source": [
        "### Test attribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgpZqbgJF_0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    # paragraph =  p_mapping[4]\n",
        "    # org_level =  p_mapping[1]\n",
        "\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level_name =  p_mapping[1].split('/')[-1]\n",
        "    org_level=OrgStructuralLevel[org_level_name]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "    \n",
        "    consent_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "                                                                consent_patterns,\n",
        "                                                                consent_patterns_emb)\n",
        "    \n",
        "     \n",
        "    \n",
        "\n",
        "    consent_v = relu(max_exclusive_pattern_by_prefix(consent_patterns_distances, 'consent/'),0.5)\n",
        "    consent_words = remap_attention_vector(consent_v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "     \n",
        "     \n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "\n",
        "    #collect sentences with constraint values\n",
        "    sentence_spans=[]\n",
        "    for value in values:\n",
        "      sentence_span = subdoc.tokens_map.sentence_at_index(value.parent.span[0], return_delimiters=True)      \n",
        "      if sentence_span not in sentence_spans:\n",
        "        sentence_spans.append(sentence_span) \n",
        "    # sentence_spans = merge_colliding_spans(sentence_spans, eps=1)\n",
        "     \n",
        "\n",
        "\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "\n",
        "    # attribute sentences to subject\n",
        "    \n",
        "    for span in sentence_spans:\n",
        "      print ( )\n",
        "      print ('-'*30)\n",
        "      print ('VALUE', sentence_span, subdoc.tokens_map.text_range(span))\n",
        "      for subj in subject_attentions_map.keys():\n",
        "        av = subject_attentions_map[subj]['words']\n",
        "        \n",
        "        confidence_region = av[span[0]:span[1]]\n",
        "        confidence = estimate_confidence_by_mean_top_non_zeros(confidence_region)\n",
        "\n",
        "        renderer_.render_color_text(subdoc.tokens_map[span[0]:span[1]], confidence_region , _range=(0,1) )\n",
        "        print(subj,  confidence )\n",
        "      print ('~'*30)\n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    charity_subj_av_words = subject_attentions_map[CharterSubject.Consulting]['words']\n",
        "    charity_tag = find_charity_paragraphs(org_level, subdoc,(charity_subj_av_words+consent_words)/2)\n",
        "    print(charity_tag)\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDHXPCl9JL2N",
        "colab_type": "text"
      },
      "source": [
        "# Save json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tcAlZx6FGWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_str=charter.to_json()\n",
        "\n",
        "json_obj = DocumentJson.from_json(json_str)\n",
        "with open('charter.json', 'w') as file: \n",
        "  file.write(json_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}