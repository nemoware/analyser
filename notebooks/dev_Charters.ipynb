{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev Charters.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "0n-DlKATL2JQ",
        "WPEjJV3Elb8t",
        "eCveVZqvNhsr",
        "ooZtf4dE2yJh",
        "ZUnJClKU8Osh",
        "qfNelSq3BIrV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3f7JUyo5i1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Params\n",
        " \n",
        "\n",
        "document_parser_lib_version='1.1.11'#@param {type:\"string\"}\n",
        "charter_filename='/content/\\u0423\\u0441\\u0442\\u0430\\u0432 - \\u0413\\u041F\\u041D-\\u0422\\u0440\\u0430\\u043D\\u0441\\u043F\\u043E\\u0440\\u0442_\\u0413\\u041E\\u0421\\u0410-2018.docx'#@param {type:\"string\"}\n",
        "# _git_branch = 'protocols-4'\n",
        "_git_branch = 'charters'#@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-DlKATL2JQ",
        "colab_type": "text"
      },
      "source": [
        "# INIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPEjJV3Elb8t",
        "colab_type": "text"
      },
      "source": [
        "## pull code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPFNIoGZL198",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "–ù–∏—á—Ç–æ = None\n",
        "\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ü¶ä GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('‚ù§Ô∏èimporting Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "#----\n",
        "import matplotlib as mpl\n",
        "from documents import TextMap\n",
        "from colab_support.renderer import HtmlRenderer\n",
        "from legal_docs import DocumentJson\n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range, separator=separator)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range, separator=separator)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  for tag in cd.tags:\n",
        "    span = tag.span\n",
        "    _map = cd.tokenization_maps[tag.span_map]\n",
        "    print(tag)\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mlpzzdNAWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyjarowinkler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMB4pkuu2wi",
        "colab_type": "text"
      },
      "source": [
        "### Init document-parser lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lib_version = '1.1.10'\n",
        "import os\n",
        "if not os.path.isfile(f'document-parser-{lib_version}-distribution.zip'):\n",
        "  !wget https://github.com/nemoware/document-parser/releases/download/$lib_version/document-parser-$lib_version-distribution.zip\n",
        "if not os.path.isdir(f'document-parser-{lib_version}'):\n",
        "  !unzip document-parser-$lib_version-distribution.zip\n",
        "\n",
        " \n",
        "os.environ ['documentparser']=f'/content/document-parser-{lib_version}'\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "wp = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNmYb03dFcJ9",
        "colab_type": "text"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fbSX2h2MedM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from contract_patterns import ContractPatternFactory\n",
        "from legal_docs import LegalDocument\n",
        " \n",
        "from ml_tools import *\n",
        "\n",
        "# from headers_detector import doc_features, load_model, make_headline_attention_vector\n",
        "from hyperparams import HyperParameters\n",
        "from protocol_parser import protocol_votes_re\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ilOKSGa0zPf",
        "colab_type": "text"
      },
      "source": [
        "## render tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1jXK-Y60yHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def json2html(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['words'])\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        " \n",
        "  for hd in cd.headers:    \n",
        "    span = hd['span']      \n",
        "    markup_vector[span[0]:span[1]] += 2\n",
        "\n",
        "  for tag in cd.attributes:\n",
        "    if 'span' in  cd.attributes[tag]:\n",
        "      span = cd.attributes[tag]['span']\n",
        "      confidence = cd.attributes[tag]['confidence']\n",
        "      markup_vector[span[0]:span[1]] += confidence\n",
        "    else:\n",
        "      warnings.warn(f'{tag} has no span')\n",
        "\n",
        "  return renderer_.to_color_text(wordsmap.tokens, markup_vector)\n",
        "\n",
        "def print_headers(d: LegalDocument):\n",
        "  for p in d.paragraphs:\n",
        "    print('\\t --> üìÇ', d.substr(p.header))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCveVZqvNhsr",
        "colab_type": "text"
      },
      "source": [
        "## üíÖ Init Embedder(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3k194xUFy20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from protocol_parser import  ProtocolPatternFactory\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        "# from contract_patterns import ContractPatternFactory\n",
        "elmo_embedder = ElmoEmbedder()\n",
        "elmo_embedder_default = ElmoEmbedder(layer_name=\"default\")\n",
        "\n",
        "# protocols_factory = ProtocolPatternFactory(elmo_embedder)\n",
        "# contracts_factory = ContractPatternFactory(elmo_embedder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to0MzdyWjEWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "from legal_docs import tokenize_doc_into_sentences_map, ContractValue\n",
        "from ml_tools import *\n",
        "from parsing import ParsingContext\n",
        "from patterns import *\n",
        " \n",
        "from text_tools import *\n",
        "\n",
        "# legal_docs.py\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdn6goa45hoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooZtf4dE2yJh",
        "colab_type": "text"
      },
      "source": [
        "# C CHARTER (mock json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec2LyYZIMuQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_mock=False\n",
        "\n",
        "from structures import OrgStructuralLevel\n",
        "from contract_parser import find_value_sign_currency_attention \n",
        "\n",
        "if make_mock:\n",
        "\n",
        "  results = wp.read_doc('/content/–ú–ù–ì –£—Å—Ç–∞–≤.docx')\n",
        "  for d in results['documents'][:1]:  # XXX\n",
        "    if 'CHARTER' == d['documentType']:    \n",
        "      charter = join_paragraphs(d, 'no_id')\n",
        "\n",
        "  charter.tags=[]\n",
        "\n",
        "\n",
        "  values: [ContractValue] = find_value_sign_currency_attention(charter)\n",
        "  # print(values)\n",
        "\n",
        "  # values_list = []\n",
        "  # k=0;\n",
        "  # for v in values:\n",
        "  #   k+=1;\n",
        "  #   v.parent.kind=f'constraint_{k}'\n",
        "  #   as_list = v.as_list()\n",
        "  #   charter.tags+=as_list\n",
        "  #   values_list+=as_list\n",
        "\n",
        "  #   for xx in as_list:\n",
        "  #     print(xx.get_key())\n",
        "\n",
        "  tag_ceo = SemanticTag(f'competence-{OrgStructuralLevel.CEO.name}', f'{OrgStructuralLevel.CEO.name}', (7982, 8951))\n",
        "  tag_all = SemanticTag(f'competence-{OrgStructuralLevel.AllMembers.name}', f'{OrgStructuralLevel.AllMembers.name}', (2512, 3168))\n",
        "  \n",
        "  organs = [\n",
        "                tag_ceo,\n",
        "                tag_all\n",
        "                ]\n",
        "\n",
        "  competences = [\n",
        "                SemanticTag('finance', '', (8410, 8470), parent=tag_ceo),\n",
        "                SemanticTag('deals', '', (8474, 8555), parent=tag_ceo),\n",
        "                SemanticTag('court', '', (8715,8825), parent=tag_ceo)                             \n",
        "                ]\n",
        "  # charter.tags.append(block1)\n",
        "\n",
        "  charter.tags += organs\n",
        "  charter.tags += competences\n",
        "\n",
        "  k=0\n",
        "  for tag in competences:\n",
        "    \n",
        "    for contract_value in values:\n",
        "      k+=1\n",
        "      z ='min'\n",
        "      if contract_value.sign.value>0:\n",
        "        z ='max'\n",
        "      contract_value.parent.kind=f'constraint-{z}'\n",
        "      # for v in contract_value.as_list():\n",
        "      if tag.is_nested(contract_value.parent.span):\n",
        "        contract_value.parent.set_parent_tag ( tag)\n",
        "        print (contract_value.parent, ' IS NESTED IN', tag.span)\n",
        "        charter.tags+=contract_value.as_list()\n",
        "        print(contract_value.value.get_key())\n",
        "    \n",
        "      \n",
        "        \n",
        "  for tag in charter.tags:\n",
        "    print(tag.get_key())\n",
        "  \n",
        "\n",
        "\n",
        "  json_str=charter.to_json()\n",
        "\n",
        "  json_obj = DocumentJson.from_json(json_str)\n",
        "  with open('charter.json', 'w') as file: \n",
        "    file.write(json_str)\n",
        "\n",
        "\n",
        "\n",
        "  # 7982\n",
        "\n",
        "  # test_dists = relu (1-test_pattern._eval_distances(doc.embeddings), 0.5)\n",
        "  # v = relu(test_dists, 0.5)\n",
        "  # renderer_.render_color_text(charter.tokens, v)\n",
        "  # show_json(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVPQtYpZyIFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z4-TZd1uHGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(charter.tags)\n",
        "if make_mock:\n",
        "  html = json2html(json_obj)\n",
        "  display(HTML(html))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnNzYRfvP0cB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if make_mock:\n",
        "  jj = charter.to_json()\n",
        "  parsed = json.loads(jj)\n",
        "  del parsed['tokenization_maps']\n",
        "  print(json.dumps(parsed, indent=4, sort_keys=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeZlg7uV8OOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUnJClKU8Osh",
        "colab_type": "text"
      },
      "source": [
        "# CHARTER: parse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccivoWplG5os",
        "colab_type": "text"
      },
      "source": [
        "1. find competence-sections\n",
        "2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue92yTFQ8R0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_old_parser=False\n",
        "\n",
        "if use_old_parser:\n",
        "  from charter_parser import CharterDocumentParser\n",
        "  from charter_patterns import CharterPatternFactory\n",
        "\n",
        "\n",
        "  charter_pattern_factory = CharterPatternFactory(elmo_embedder)\n",
        "  CDP = CharterDocumentParser(charter_pattern_factory)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUIvIXQR8zPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_old_parser:\n",
        "  charter.embedd_tokens(elmo_embedder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWZ7rafX9Poy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_old_parser:\n",
        "  CDP.doc = charter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfI6smYH9fGp",
        "colab_type": "text"
      },
      "source": [
        "### competence_attention (not used??)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK09zp9e9Xo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_old_parser:\n",
        "  v = CDP._make_competence_attention_v()\n",
        "  renderer_.render_color_text(charter.tokens_cc, v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm22tl9CAsl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIbsP54RAsxg",
        "colab_type": "text"
      },
      "source": [
        "### Sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZUKgaRz_Pva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_old_parser:\n",
        "  sections = CDP.sections_finder.find_sections(charter, CDP.pattern_factory, CDP.pattern_factory.headlines, headline_patterns_prefix='headline.' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMRgtfOA_Zgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "if use_old_parser:\n",
        "  print('sections:'.upper())\n",
        "  for key in sections.keys():\n",
        "    section = sections[key]\n",
        "    print(section.subdoc.text)\n",
        "  print()\n",
        "  print('sections_filtered:'.upper())\n",
        "\n",
        "\n",
        "  sections_filtered = CDP._get_head_sections()\n",
        "  for key in sections_filtered.keys():\n",
        "    section = sections[key]\n",
        "    print(section.subdoc.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izscudlNOfac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tags_attention(self, tags):\n",
        "  _attention = np.zeros(self.__len__())\n",
        "\n",
        "  for t in tags:\n",
        "    _attention[t.as_slice()] += t.confidence\n",
        "  return _attention\n",
        "\n",
        "def as_tag(self):\n",
        "  st = SemanticTag(self.type, None, (self.subdoc.start, self.body.end))\n",
        "  st.confidence = self.confidence\n",
        "  return st\n",
        "\n",
        "if use_old_parser:\n",
        "  tags=[]\n",
        "  for key in sections_filtered.keys():\n",
        "    section = sections[key]\n",
        "    tags.append( as_tag(section) )\n",
        "\n",
        "  renderer_.render_color_text(charter.tokens, get_tags_attention(charter,tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GEGWZm5AnEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print_headers(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sd1Sp5iBIHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfNelSq3BIrV",
        "colab_type": "text"
      },
      "source": [
        "### Constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA2Ysm-sBKgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_old_parser:\n",
        "  for section_name in sections_filtered:\n",
        "    section = sections_filtered[section_name].body\n",
        "\n",
        "\n",
        "    all_margin_values: PatternSearchResults = section.find_sentences_by_pattern_prefix(section_name, CDP.pattern_factory,\n",
        "                                                                                        'sum__')\n",
        "    print(all_margin_values)\n",
        "    # for c in all_margin_values.constraints:\n",
        "    #   print(c)\n",
        "    \n",
        "    # print(section_name)\n",
        "    # value_constraints, charity_constraints, all_margin_values, charity_constraints = CDP._find_constraints_in_section(\n",
        "    #   org_level=section_name, section=section)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTvjEfAxPzeI",
        "colab_type": "text"
      },
      "source": [
        "# New"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnRQ5Cg-dCTA",
        "colab_type": "text"
      },
      "source": [
        "## Patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEbp4bLUGSLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "competence_headline_pattern_prefix = 'headline'\n",
        "comp_str_pat=[]\n",
        "comp_str_pat += [ [f'{competence_headline_pattern_prefix}/{ol.name}',  ol.display_string ] for ol in OrgStructuralLevel]\n",
        "comp_str_pat += [ [f'{competence_headline_pattern_prefix}/comp-q/{ol.name}', \"–∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ \" + ol.display_string +' –æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã'] for ol in OrgStructuralLevel]\n",
        "comp_str_pat += [ [f'{competence_headline_pattern_prefix}/comp/{ol.name}', \"–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ \" + ol.display_string] for ol in OrgStructuralLevel]\n",
        "comp_str_pat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc4MSHmVdEP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezyyvDYGdEs5",
        "colab_type": "text"
      },
      "source": [
        "## classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhNwA4xxP1kO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from legal_docs import embedd_sentences\n",
        "\n",
        "\n",
        "\n",
        "class CharterDocument5(LegalDocument):\n",
        "\n",
        "  def __init__(self, doc: LegalDocument):\n",
        "    super().__init__('')\n",
        "    if doc is not None:\n",
        "      self.__dict__ = doc.__dict__\n",
        "\n",
        "    self.sentence_map: TextMap = None\n",
        "    self.sentences_embeddings = None\n",
        "\n",
        "    self.distances_per_sentence_pattern_dict = {}\n",
        "\n",
        "    self.charity_tags=[]\n",
        "    # self.agents_tags: [SemanticTag] = []\n",
        "    # self.org_level: [SemanticTag] = []\n",
        "    # self.agenda_questions: [SemanticTag] = []\n",
        "    self.margin_values: [ContractValue] = []\n",
        "\n",
        "  def subdoc_slice(self, __s: slice, name='undef'):\n",
        "    sub = super().subdoc_slice(__s, name)\n",
        "    span = [max((0, __s.start)), max((0, __s.stop))]\n",
        "\n",
        "    sentences_span = self.tokens_map.remap_span(span, self.sentence_map)\n",
        "    _slice = slice(sentences_span[0], sentences_span[1])\n",
        "    sub.sentence_map = self.sentence_map.slice(_slice)\n",
        "  \n",
        "    if self.sentences_embeddings is not None:\n",
        "      sub.sentences_embeddings = self.sentences_embeddings[_slice]\n",
        "\n",
        "    return sub\n",
        "\n",
        "    \n",
        "  def get_tags(self) -> [SemanticTag]:\n",
        "    tags = []\n",
        "    tags += self.charity_tags\n",
        "    # tags += self.org_level\n",
        "    # tags += self.agenda_questions\n",
        "    for mv in self.margin_values:\n",
        "      tags += mv.as_list()\n",
        "\n",
        "    return tags\n",
        "\n",
        "   \n",
        "\n",
        "class CharterParser2(ParsingContext):\n",
        "\n",
        "  patterns_dict = [\n",
        "    # ['headline.head.directors.1', '0. –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –û–±—â–µ—Å—Ç–≤–∞'],    \n",
        "    # ['headline.head.directors.2', '–ö –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã'],    \n",
        "\n",
        "    # ['headline.head.all.n.a', '–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤ –û–±—â–µ—Å—Ç–≤–∞']\n",
        "    \n",
        "  ] + comp_str_pat\n",
        "\n",
        "  def __init__(self, embedder, elmo_embedder_default: ElmoEmbedder):\n",
        "    ParsingContext.__init__(self, embedder)\n",
        "    self.elmo_embedder_default = elmo_embedder_default\n",
        "\n",
        "    patterns_te = [p[1] for p in CharterParser2.patterns_dict]\n",
        "    self.patterns_embeddings = elmo_embedder_default.embedd_strings(patterns_te)\n",
        "  \n",
        "  def ebmedd(self, doc: CharterDocument5):\n",
        "    doc.sentence_map = tokenize_doc_into_sentences_map(doc, 200)\n",
        "\n",
        "    ### ‚öôÔ∏èüîÆ SENTENCES embedding\n",
        "    doc.sentences_embeddings = embedd_sentences(doc.sentence_map, self.elmo_embedder_default)\n",
        "\n",
        "    doc.distances_per_sentence_pattern_dict = calc_distances_per_pattern_dict(doc.sentences_embeddings,\n",
        "                                                                              self.patterns_dict,\n",
        "                                                                              self.patterns_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "results = wp.read_doc(charter_filename)\n",
        "# results = wp.read_doc('/content/–ï–Æ –£—Å—Ç–∞–≤.docx')\n",
        "\n",
        "for d in results['documents'][:1]:  # XXX\n",
        "  if 'CHARTER' == d['documentType']:    \n",
        "    charter = join_paragraphs(d, 'no_id')\n",
        "    charter = CharterDocument5(charter)\n",
        "\n",
        "print_headers(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16JvY5XcpdxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter_parser = CharterParser2(elmo_embedder, elmo_embedder_default)\n",
        "charter_parser.ebmedd(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1yEWke4nnZA",
        "colab_type": "text"
      },
      "source": [
        "## Find org level sections\n",
        " - find all headers spans\n",
        " - compare headers with patterns \n",
        " - find interresting spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piZZmrNTnxJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_slices = [p.header.as_slice() for p in charter.paragraphs]\n",
        "header_slices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUpmN9C0saqE",
        "colab_type": "text"
      },
      "source": [
        "remap to sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlDC8PFHseJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_slices_sent = charter.tokens_map.remap_slices(header_slices, charter.sentence_map)\n",
        "header_slices_sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRy1U1NiuaN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "section_ends = [ x[0] for x in header_slices_sent ] + [ len(charter.sentence_map) ] #+end of doc\n",
        "section_ends"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtzK_vohox1E",
        "colab_type": "text"
      },
      "source": [
        "Available sentence pattern (names)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgkFOANqotUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter.distances_per_sentence_pattern_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR3URDk8SAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fi \n",
        "# av_competence_headline_pattern = max_exclusive_pattern_by_prefix(charter.distances_per_sentence_pattern_dict, competence_headline_pattern_prefix)\n",
        "# av_competence_headline_pattern = relu(av_competence_headline_pattern, 0.66)\n",
        "\n",
        "# av_interresting_headers = headers_att * av_competence_headline_pattern\n",
        "# #words map\n",
        "# # sections_spans_words = charter.sentence_map.remap_slices(v, charter.tokens_map)\n",
        " \n",
        "# # v = charter.distances_per_sentence_pattern_dict['headline.head.directors.n']# - 0.1*charter.distances_per_sentence_pattern_dict['headline.head.directors.not'] \n",
        "\n",
        "# renderer_.render_color_text(charter.sentence_map.tokens, av_interresting_headers, _range=(0,1), separator='¬∂<br>')\n",
        "# # renderer_.render_color_text(charter.tokens_cc, sections_spans_words, _range=(0,1) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Q9zLUBPmU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter_parser.patterns_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVNAqHsQPnZe",
        "colab_type": "text"
      },
      "source": [
        "## Find relevant (org struct) paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNkbOL969lSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from doc_structure import get_tokenized_line_number\n",
        "from ml_tools import calc_distances_per_pattern_dict\n",
        "\n",
        "# charter.\n",
        "def headers_as_sentences(doc: LegalDocument, normal_case=True, strip_number=True):\n",
        "  _map = doc.tokens_map\n",
        "  if normal_case:\n",
        "    _map = doc.tokens_map_norm\n",
        "\n",
        "  numbered = [_map.slice(p.header.as_slice()) for p in doc.paragraphs]\n",
        "  stripped = []\n",
        "\n",
        "  for s in numbered:\n",
        "    if strip_number:\n",
        "      a = get_tokenized_line_number(s.tokens, 0)\n",
        "      _, span, _, _ = a\n",
        "      line = s.text_range([span[1], None]).strip()\n",
        "    else:\n",
        "      line = s.text\n",
        "\n",
        "    # print('üò±', line)\n",
        "    stripped.append(line)\n",
        "\n",
        "  return stripped\n",
        "\n",
        "\n",
        "def map_headlines_to_patterns(charter, charter_parser, pattern_prefix:str, pattern_suffixes:[str]):\n",
        "  \"\"\"\n",
        "  1. Collect headers as string, strip numbers, normalize case\n",
        "  2. Embedd heades with sentence embedder\n",
        "  3. Map headers to patterns\n",
        "  \"\"\"\n",
        "  headers = headers_as_sentences(charter)\n",
        "  headers_embedding = charter_parser.elmo_embedder_default.embedd_strings(headers)\n",
        "  \n",
        "  # #debug\n",
        "  # for e in headers_embedding:\n",
        "  #   print(e[0:3])\n",
        "  \n",
        "  header_to_pattern_distances = calc_distances_per_pattern_dict(headers_embedding,\n",
        "                                                                charter_parser.patterns_dict,\n",
        "                                                                charter_parser.patterns_embeddings)\n",
        "  \n",
        "  patterns_by_headers = [()]*len(headers)\n",
        "  for e in range(len(headers)):\n",
        "    # for each header \n",
        "    max_confidence = 0\n",
        "    for pattern_suffix in pattern_suffixes:\n",
        "      pattern_name = pattern_prefix+pattern_suffix\n",
        "      # find best pattern\n",
        "      confidence = header_to_pattern_distances[pattern_name][e]\n",
        "      if confidence > max_confidence and confidence > 0.7:\n",
        "        patterns_by_headers[e] = (pattern_name, pattern_suffix, confidence, headers[e], charter.paragraphs[e])\n",
        "        max_confidence = confidence\n",
        "  \n",
        "  return patterns_by_headers, header_to_pattern_distances\n",
        "\n",
        "\n",
        "\n",
        "#------------------------DEBUG CUT\n",
        "\n",
        "p_suffices = [ol.name for ol in OrgStructuralLevel] \n",
        "p_suffices += ['comp/'+ol.name for ol in OrgStructuralLevel] \n",
        " \n",
        "patterns_by_headers, header_to_pattern_distances = map_headlines_to_patterns(charter, charter_parser, competence_headline_pattern_prefix+'/', p_suffices )\n",
        "for k in patterns_by_headers :\n",
        "  if k:\n",
        "    print(k)\n",
        "\n",
        "\n",
        "print('\\n\\nheadline/comp/AllMembers')\n",
        "renderer_.render_color_text(headers_as_sentences(charter), header_to_pattern_distances['headline/comp/AllMembers'], _range=(0,1), separator='üò± <br>')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqd2E2E39P1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_values_attention(subdoc, values:List[ContractValue]):\n",
        "  numbers_attention = np.zeros(len(subdoc.tokens_map))\n",
        "    \n",
        "  for v in values:\n",
        "    numbers_attention  [ v.value.as_slice() ] += v.value.confidence\n",
        "    numbers_attention  [ v.currency.as_slice() ] += v.currency.confidence\n",
        "    numbers_attention  [ v.sign.as_slice() ] += v.sign.confidence\n",
        "    numbers_attention  [ v.parent.as_slice() ] += v.parent.confidence\n",
        "\n",
        "  return numbers_attention/2\n",
        "\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level =  p_mapping[1]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice(paragraph.body.as_slice())\n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "\n",
        "    #color \n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "    \n",
        "    # for v in values:\n",
        "    #   numbers_attention  [ v.value.as_slice() ] = 1\n",
        "    #   numbers_attention  [ v.currency.as_slice() ] = 1\n",
        "    #   numbers_attention  [ v.sign.as_slice() ] = 1\n",
        "\n",
        "    print('%'*100)\n",
        "    print(org_level)\n",
        "    # renderer_.render_color_text(subdoc.tokens_map.tokens, numbers_attention, _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eepowAGJZ4yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class CharterPatternFactory(AbstractPatternFactoryLowCase):\n",
        "\n",
        "#   def __init__(self, embedder):\n",
        "#     AbstractPatternFactoryLowCase.__init__(self, embedder)\n",
        "#     # _build_order_patterns(self) \n",
        "\n",
        "#     # self.embedd()\n",
        "\n",
        "# def _build_order_patterns():\n",
        "  \n",
        "#   def cp(r, name, tuples):\n",
        "#     r +=[  ['order/'+name, ' '.join(tuples)] ]\n",
        "\n",
        "#   ret = []\n",
        "#   # cp(ret, 'consent', ('–ü–æ—Ä—è–¥–æ–∫', '–æ–¥–æ–±—Ä–µ–Ω–∏—è —Å–¥–µ–ª–æ–∫', '–≤ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –∏–º–µ–µ—Ç—Å—è –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å'))\n",
        "#   cp(ret, 'solution', ('', '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π', '–æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫'))\n",
        "#   cp(ret, 'consent_1', ('', '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è', '–∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –∫–∞–∫–æ–π-–ª–∏–±–æ —Å–¥–µ–ª–∫–∏ –û–±—â–µ—Å—Ç–≤–∞'))\n",
        "\n",
        "#   prefix = '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏'\n",
        "\n",
        "#   cp(ret, 'deal.1', (prefix, 'c–¥–µ–ª–∫–∏', ', —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Ä–∞–≤–Ω–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç'))\n",
        "#   cp(ret, 'deal.2', (prefix, 'c–¥–µ–ª–∫–∏', ', —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–Ω–µ–µ'))\n",
        "\n",
        "#   return ret\n",
        "\n",
        "\n",
        "# order_patterns = _build_order_patterns() \n",
        "# order_patterns\n",
        " \n",
        " \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-3Khp4ybDUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum, unique, EnumMeta\n",
        "from typing import List\n",
        "from structures import DisplayStringEnumMeta\n",
        "\n",
        "@unique\n",
        "class CharterSubject(Enum, metaclass=DisplayStringEnumMeta):\n",
        "   \n",
        "  Deal = 0, '–°–¥–µ–ª–∫–∞'\n",
        "  Charity = 1, '–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å'\n",
        "  Other = 2, '–î—Ä—É–≥–æ–µ'\n",
        "  Lawsuit = 3, '–°—É–¥–µ–±–Ω—ã–µ –∏–∑–¥–µ—Ä–∂–∫–∏'\n",
        "  RealEstate = 4, '–ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å'\n",
        "  Insurance = 5, '–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ'\n",
        "  Consulting = 6, '–ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —É—Å–ª—É–≥–∏'\n",
        "\n",
        "\n",
        " \n",
        "strs_subjects_patterns = {\n",
        "    \n",
        "    # CharterSubject.Charity:[\n",
        "    #   '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –≤–Ω–µ—Å–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–æ–º –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏–π –Ω–∞ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏',  \n",
        "    #   '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–º–æ—â–∏',\n",
        "    #   '—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Å–¥–µ–ª–æ–∫ –¥–∞—Ä–µ–Ω–∏—è',\n",
        "    #   '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å–ø–æ–Ω—Å–æ—Ä—Å–∫–æ–≥–æ –∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞',\n",
        "    #   '–ø–µ—Ä–µ–¥–∞—á–∞ –≤ –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–µ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ',\n",
        "    #   '—Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–æ–∫ –º–µ–Ω—ã, –¥–∞—Ä–µ–Ω–∏—è, –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞—é—â–∏—Ö –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–µ –æ—Ç—á—É–∂–¥–µ–Ω–∏–µ '\n",
        "    # ],\n",
        "\n",
        "     \n",
        "    CharterSubject.Deal:[\n",
        "      '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫'\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Charity:[\n",
        "      '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏–π –Ω–∞ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏',  \n",
        "      '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–º–æ—â–∏',\n",
        "      '—Å–¥–µ–ª–æ–∫ –¥–∞—Ä–µ–Ω–∏—è',\n",
        "      '–¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å–ø–æ–Ω—Å–æ—Ä—Å–∫–æ–≥–æ –∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞',\n",
        "      '–ø–µ—Ä–µ–¥–∞—á–∞ –≤ –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–µ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ',\n",
        "      '–º–µ–Ω—ã, –¥–∞—Ä–µ–Ω–∏—è, –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–µ –æ—Ç—á—É–∂–¥–µ–Ω–∏–µ '\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Lawsuit:[\n",
        "      '–æ –Ω–∞—á–∞–ª–µ/—É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ª—é–±—ã—Ö —Å—É–¥–µ–±–Ω—ã—Ö —Å–ø–æ—Ä–æ–≤ –∏ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤',\n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏–∏ –û–±—â–µ—Å—Ç–≤–æ–º –º–∏—Ä–æ–≤–æ–≥–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è –ø–æ —Å—É–¥–µ–±–Ω–æ–º—É –¥–µ–ª—É —Å —Ü–µ–Ω–æ–π –∏—Å–∫–∞ '    \n",
        "    ],\n",
        "\n",
        "    CharterSubject.RealEstate:[\n",
        "      '—Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—Ç—á—É–∂–¥–∞–µ–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞',\n",
        "      '—Å–¥–µ–ª–∫–∏ —Å –∏–º—É—â–µ—Å—Ç–≤–æ–º –û–±—â–µ—Å—Ç–≤–∞'\n",
        "    ],\n",
        "\n",
        "    # CharterSubject.Insurance:[\n",
        "    #   '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è',\n",
        "    #   '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –ø—Ä–æ–¥–ª–µ–Ω–∏—è, –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –û–±—â–µ—Å—Ç–≤–æ–º –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è'\n",
        "    #   '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è'\n",
        "    # ],\n",
        "\n",
        "    CharterSubject.Insurance:[\n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è',\n",
        "      '–≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è'\n",
        "      '—Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–æ–∫ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è'\n",
        "    ],\n",
        "\n",
        "    \n",
        "\n",
        "    CharterSubject.Consulting:[\n",
        "      '–¥–æ–≥–æ–≤–æ—Ä–∞ –æ–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —É—Å–ª—É–≥',\n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞',\n",
        "      '—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞ –æ–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —É—Å–ª—É–≥ –∏–ª–∏ –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞',\n",
        "      '–æ–∫–∞–∑–∞–Ω–∏—è –æ–±—â–µ—Å—Ç–≤—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —É—Å–ª—É–≥ '\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Other:[\n",
        "      '—Ä–µ—à–µ–Ω–∏—è –æ –≤–∑—ã—Å–∫–∞–Ω–∏–∏ —Å –ì–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ —É–±—ã—Ç–∫–æ–≤',      \n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –æ–± –æ—Ç—Å—Ç—É–ø–Ω–æ–º , –Ω–æ–≤–∞—Ü–∏–∏ –∏/–∏–ª–∏ –ø—Ä–æ—â–µ–Ω–∏–∏ –¥–æ–ª–≥–∞ , –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –æ–± —É—Å—Ç—É–ø–∫–µ –ø—Ä–∞–≤–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥–µ –¥–æ–ª–≥–∞',\n",
        "      '–æ –≤—ã–¥–∞—á–µ –∏–ª–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –û–±—â–µ—Å—Ç–≤–æ–º –≤–µ–∫—Å–µ–ª–µ–π , –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ –ø–æ –Ω–∏–º –ø–µ—Ä–µ–¥–∞—Ç–æ—á–Ω—ã—Ö –Ω–∞–¥–ø–∏—Å–µ–π , –∞–≤–∞–ª–µ–π , –ø–ª–∞—Ç–µ–∂–µ–π',\n",
        "      '–Ω–µ—Ü–µ–ª–µ–≤–æ–µ —Ä–∞—Å—Ö–æ–¥–æ–≤–∞–Ω–∏–µ –û–±—â–µ—Å—Ç–≤–æ–º –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤'\n",
        "    ]\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    # CharterSubject.Consulting:[\n",
        "    #   '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞ –æ–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —É—Å–ª—É–≥',\n",
        "    #   '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞',\n",
        "    #   '—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ –æ–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —É—Å–ª—É–≥ –∏–ª–∏ –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞'\n",
        "    # ]\n",
        "}\n",
        " \n",
        "\n",
        "\n",
        "str_consent_patterns=[\n",
        "  '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫',\n",
        "  '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫',\n",
        "  '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –∫–∞–∫–æ–π-–ª–∏–±–æ —Å–¥–µ–ª–∫–∏ –û–±—â–µ—Å—Ç–≤–∞',\n",
        "  '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ c–¥–µ–ª–∫–∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Ä–∞–≤–Ω–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç'\n",
        "]\n",
        "\n",
        "def build_sentence_patterns(strings, prefix, embedder):\n",
        "  ret = []\n",
        "  for txt in strings:\n",
        "    ret.append ( [f'{prefix}/{len(ret)}',  txt ]  )\n",
        "\n",
        "  emb =  embedder.embedd_strings(strings)\n",
        "  return ret, emb\n",
        "\n",
        "emb_subj_patterns={}\n",
        "for subj in strs_subjects_patterns.keys():\n",
        "  strings = strs_subjects_patterns[subj]\n",
        "  patterns, patterns_emb = build_sentence_patterns(strings, f'subject/{subj.name}', elmo_embedder_default)\n",
        "\n",
        "  emb_subj_patterns[subj] = {\n",
        "    'patterns':patterns, \n",
        "    'embedding':patterns_emb    \n",
        "  }\n",
        "  \n",
        "\n",
        "consent_patterns, consent_patterns_emb = build_sentence_patterns(str_consent_patterns, 'consent', elmo_embedder_default)\n",
        "\n",
        "###-----debug-----\n",
        "emb_subj_patterns[CharterSubject.Insurance]['patterns']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngo4GUChdUF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# order_patterns_emb = elmo_embedder_default.embedd_strings([p[1] for p in order_patterns])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWtENbeDPe2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from contract_parser import _find_most_relevant_paragraph\n",
        "from legal_docs import remap_attention_vector\n",
        "\n",
        "\n",
        "def find_charity_paragraphs(org_level, subdoc, subject_attention ):\n",
        "  paragraph_span, confidence, paragraph_attention_vector = _find_most_relevant_paragraph(subdoc,\n",
        "                                                                                             subject_attention,\n",
        "                                                                                             min_len=20,\n",
        "                                                                                             return_delimiters=False)\n",
        "    \n",
        "  if confidence>0.6:\n",
        "    subject_tag = SemanticTag(f'{org_level}_{CharterSubject.Charity.name}', CharterSubject.Charity.name, paragraph_span)\n",
        "    subject_tag.offset(subdoc.start)\n",
        "\n",
        "\n",
        "    print(org_level)\n",
        "    print(confidence, subdoc.tokens_map.text_range(paragraph_span))\n",
        "    \n",
        "\n",
        "    return subject_tag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from contract_parser import _find_most_relevant_paragraph, find_value_sign_currency_attention\n",
        "from legal_docs import remap_attention_vector, ContractValue, LegalDocumentExt\n",
        "from ml_tools import *\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_charity_paragraphs(org_level:SemanticTag, subdoc, subject_attention):\n",
        "  paragraph_span, confidence, paragraph_attention_vector = _find_most_relevant_paragraph(subdoc,\n",
        "                                                                                         subject_attention,\n",
        "                                                                                         min_len=20,\n",
        "                                                                                         return_delimiters=False)\n",
        "\n",
        "  if confidence > 0.6:\n",
        "    subject_tag = SemanticTag(f'{CharterSubject.Charity.name}', CharterSubject.Charity.name, paragraph_span, parent=org_level)\n",
        "    subject_tag.offset(subdoc.start)\n",
        "\n",
        "    print(org_level)\n",
        "    print(confidence, subdoc.tokens_map.text_range(paragraph_span))\n",
        "\n",
        "    return subject_tag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_charter_subj_attentions(subdoc: LegalDocumentExt, emb_subj_patterns):\n",
        "  _distances_per_subj = {}\n",
        "  for subj in emb_subj_patterns.keys():\n",
        "    patterns = emb_subj_patterns[subj]['patterns']\n",
        "    embedding = emb_subj_patterns[subj]['embedding']\n",
        "    patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "                                                         patterns,\n",
        "                                                         embedding)\n",
        "\n",
        "    subj_av = relu(max_exclusive_pattern_by_prefix(patterns_distances, f'subject/{subj.name}/'), 0.6)\n",
        "    subj_av_words = remap_attention_vector(subj_av, subdoc.sentence_map, subdoc.tokens_map)\n",
        "\n",
        "    _distances_per_subj[subj] = {\n",
        "      'words': subj_av_words,\n",
        "      'sentences': subj_av,\n",
        "    }\n",
        "  return _distances_per_subj\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def charter_subjects_attribution(subdoc: LegalDocumentExt, emb_subj_patterns, parent_org_level_tag:SemanticTag):\n",
        "  \"\"\"\n",
        "  :param subdoc:\n",
        "  :param emb_subj_patterns:\n",
        "\n",
        "        emb_subj_patterns[subj] = {\n",
        "          'patterns':patterns,\n",
        "          'embedding':patterns_emb\n",
        "        }\n",
        "\n",
        "  :return:\n",
        "  \"\"\"\n",
        "\n",
        "  # ---\n",
        "  subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "  # ---\n",
        "\n",
        "  # collect sentences having constraint values\n",
        "  sentence_spans = []\n",
        "  for value in values:\n",
        "    sentence_span = subdoc.tokens_map.sentence_at_index(value.parent.span[0], return_delimiters=True)\n",
        "    if sentence_span not in sentence_spans:\n",
        "      sentence_spans.append(sentence_span)\n",
        "      sentence_spans = merge_colliding_spans(sentence_spans, eps=1)\n",
        "\n",
        "\n",
        "\n",
        "  # ---\n",
        "  # attribute sentences to subject\n",
        "\n",
        "  constraint_tags = []\n",
        "  for span in sentence_spans:\n",
        "    print()\n",
        "    print('- ' * 30)\n",
        "    print('VALUE', span, subdoc.tokens_map.text_range(span))\n",
        "\n",
        "    max_confidence = 0\n",
        "    best_subject = CharterSubject.Other\n",
        "\n",
        "    for subj in subject_attentions_map.keys():\n",
        "      av = subject_attentions_map[subj]['words']\n",
        "\n",
        "      confidence_region = av[span[0]:span[1]]\n",
        "      confidence = estimate_confidence_by_mean_top_non_zeros(confidence_region)\n",
        "\n",
        "      # renderer_.render_color_text(subdoc.tokens_map[span[0]:span[1]], confidence_region, _range=(0, 1))\n",
        "      print(subj, confidence)\n",
        "\n",
        "      if confidence> max_confidence:\n",
        "        max_confidence = confidence\n",
        "        best_subject = subj\n",
        "\n",
        "    constraint_tag = SemanticTag( f'{best_subject.name}', best_subject, span, parent=parent_org_level_tag )\n",
        "    constraint_tags.append(constraint_tag)\n",
        "    # nest values\n",
        "    for value in values:\n",
        "      if constraint_tag.is_nested(value.parent):\n",
        "        value.parent.set_parent_tag(constraint_tag)\n",
        "        \n",
        "    print('~ ' * 30)\n",
        "\n",
        "  return constraint_tags, values\n",
        "\n",
        "\n",
        "\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level =  p_mapping[1]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "    parent_org_level_tag = SemanticTag(org_level, org_level, paragraph.body.span)\n",
        "    \n",
        "    consent_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "                                                                consent_patterns,\n",
        "                                                                consent_patterns_emb)\n",
        "    \n",
        "    # charity_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "    #                                                             consulting_patterns,\n",
        "    #                                                             consulting_patterns_emb)\n",
        "    \n",
        "    # print(order_patterns_distances.keys())\n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "\n",
        "    consent_v = relu(max_exclusive_pattern_by_prefix(consent_patterns_distances, 'consent/'),0.5)\n",
        "    consent_words = remap_attention_vector(consent_v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "\n",
        "    # charity_subj_av = relu(max_exclusive_pattern_by_prefix(charity_patterns_distances, 'consulting/'),0.6)\n",
        "    # charity_subj_av_words = remap_attention_vector(charity_subj_av, subdoc.sentence_map, subdoc.tokens_map)\n",
        "    # order_patterns_distances['order/consent']\n",
        "    # renderer_.render_color_text(subdoc.sentence_map.tokens, v, _range=(0,1), separator='üò± <br>')\n",
        "    \n",
        "    # charity_patterns_distances_words = remap_attention_vector(v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "\n",
        "    \n",
        "    \n",
        "    # dd = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "    # charity_subj_av_words = dd[CharterSubject.Charity]['words']\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "\n",
        "    charity_subj_av_words = subject_attentions_map[CharterSubject.Charity]['words']\n",
        "    charity_tag = find_charity_paragraphs(parent_org_level_tag, subdoc, (charity_subj_av_words+consent_words)/2)\n",
        "    print(charity_tag)\n",
        "    if charity_tag is not None:\n",
        "      charter.charity_tags.append(charity_tag)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "    # renderer_.render_color_text(subdoc.tokens_cc, charity_subj_av_words + numbers_attention , _range=(0,2) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgpZqbgJF_0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level =  p_mapping[1]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "    \n",
        "    consent_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "                                                                consent_patterns,\n",
        "                                                                consent_patterns_emb)\n",
        "    \n",
        "     \n",
        "    \n",
        "\n",
        "    consent_v = relu(max_exclusive_pattern_by_prefix(consent_patterns_distances, 'consent/'),0.5)\n",
        "    consent_words = remap_attention_vector(consent_v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "     \n",
        "     \n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "\n",
        "    #collect sentences with constraint values\n",
        "    sentence_spans=[]\n",
        "    for value in values:\n",
        "      sentence_span = subdoc.tokens_map.sentence_at_index(value.parent.span[0], return_delimiters=True)      \n",
        "      if sentence_span not in sentence_spans:\n",
        "        sentence_spans.append(sentence_span) \n",
        "    # sentence_spans = merge_colliding_spans(sentence_spans, eps=1)\n",
        "     \n",
        "\n",
        "\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "\n",
        "    # attribute sentences to subject\n",
        "    \n",
        "    for span in sentence_spans:\n",
        "      print ( )\n",
        "      print ('-'*30)\n",
        "      print ('VALUE', sentence_span, subdoc.tokens_map.text_range(span))\n",
        "      for subj in subject_attentions_map.keys():\n",
        "        av = subject_attentions_map[subj]['words']\n",
        "        \n",
        "        confidence_region = av[span[0]:span[1]]\n",
        "        confidence = estimate_confidence_by_mean_top_non_zeros(confidence_region)\n",
        "\n",
        "        renderer_.render_color_text(subdoc.tokens_map[span[0]:span[1]], confidence_region , _range=(0,1) )\n",
        "        print(subj,  confidence )\n",
        "      print ('~'*30)\n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    charity_subj_av_words = subject_attentions_map[CharterSubject.Consulting]['words']\n",
        "    charity_tag = find_charity_paragraphs(org_level, subdoc, numbers_attention*(charity_subj_av_words+consent_words)/2)\n",
        "    print(charity_tag)\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZz5jLZc0bsw",
        "colab_type": "text"
      },
      "source": [
        "## Contraint values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2enqR9K2iPxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "values:List[ContractValue] = find_value_sign_currency_attention(charter, None)\n",
        "\n",
        "#color \n",
        "numbers_attention = np.zeros(len(charter.tokens_map))\n",
        "numbers_confidence = np.zeros(len(charter.tokens_map))\n",
        "for v in values:\n",
        "  numbers_confidence [ v.value.as_slice() ] +=v.value.confidence\n",
        "  numbers_attention  [ v.value.as_slice() ] = 1\n",
        "  numbers_attention  [ v.currency.as_slice() ] = 1\n",
        "  numbers_attention  [ v.sign.as_slice() ] = 1\n",
        "\n",
        "renderer_.render_color_text(charter.tokens_map.tokens, numbers_attention, _range=(0,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7icnLG6y7vEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for v in values:\n",
        "  v\n",
        "  print(v.value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tcAlZx6FGWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_str=charter.to_json()\n",
        "\n",
        "json_obj = DocumentJson.from_json(json_str)\n",
        "with open('charter.json', 'w') as file: \n",
        "  file.write(json_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}