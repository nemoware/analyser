{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev Charters.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "0n-DlKATL2JQ",
        "9ilOKSGa0zPf",
        "eCveVZqvNhsr",
        "q1yEWke4nnZA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3f7JUyo5i1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Params\n",
        " \n",
        "\n",
        "document_parser_lib_version='1.1.11'#@param {type:\"string\"}\n",
        "charter_filename='/content/\\u0413\\u041F\\u041D \\u0423\\u0441\\u0442\\u0430\\u0432.docx'#@param {type:\"string\"}\n",
        "# _git_branch = 'protocols-4'\n",
        "_git_branch = 'charters'#@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-DlKATL2JQ",
        "colab_type": "text"
      },
      "source": [
        "# INIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPEjJV3Elb8t",
        "colab_type": "text"
      },
      "source": [
        "## pull code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPFNIoGZL198",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "–ù–∏—á—Ç–æ = None\n",
        "\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ü¶ä GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('‚ù§Ô∏èimporting Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "#----\n",
        "import matplotlib as mpl\n",
        "from documents import TextMap\n",
        "from colab_support.renderer import HtmlRenderer\n",
        "from legal_docs import DocumentJson\n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range, separator=separator)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range, separator=separator)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  for tag in cd.tags:\n",
        "    span = tag.span\n",
        "    _map = cd.tokenization_maps[tag.span_map]\n",
        "    print(tag)\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mlpzzdNAWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyjarowinkler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMB4pkuu2wi",
        "colab_type": "text"
      },
      "source": [
        "### Init document-parser lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lib_version = document_parser_lib_version\n",
        "import os\n",
        "if not os.path.isfile(f'document-parser-{lib_version}-distribution.zip'):\n",
        "  !wget https://github.com/nemoware/document-parser/releases/download/$lib_version/document-parser-$lib_version-distribution.zip\n",
        "if not os.path.isdir(f'document-parser-{lib_version}'):\n",
        "  !unzip document-parser-$lib_version-distribution.zip\n",
        "\n",
        " \n",
        "os.environ ['documentparser']=f'/content/document-parser-{lib_version}'\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "wp = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNmYb03dFcJ9",
        "colab_type": "text"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fbSX2h2MedM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from contract_patterns import ContractPatternFactory\n",
        "from legal_docs import LegalDocument\n",
        " \n",
        "from ml_tools import *\n",
        "\n",
        "# from headers_detector import doc_features, load_model, make_headline_attention_vector\n",
        "from hyperparams import HyperParameters\n",
        "from protocol_parser import protocol_votes_re\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ilOKSGa0zPf",
        "colab_type": "text"
      },
      "source": [
        "## render tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1jXK-Y60yHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def json2html(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['words'])\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        " \n",
        "  for hd in cd.headers:    \n",
        "    span = hd['span']      \n",
        "    markup_vector[span[0]:span[1]] += 2\n",
        "\n",
        "  for tag in cd.attributes:\n",
        "    if 'span' in  cd.attributes[tag]:\n",
        "      span = cd.attributes[tag]['span']\n",
        "      confidence = cd.attributes[tag]['confidence']\n",
        "      markup_vector[span[0]:span[1]] += confidence\n",
        "    else:\n",
        "      warnings.warn(f'{tag} has no span')\n",
        "\n",
        "  return renderer_.to_color_text(wordsmap.tokens, markup_vector)\n",
        "\n",
        "def print_headers(d: LegalDocument):\n",
        "  for p in d.paragraphs:\n",
        "    print('\\t --> üìÇ', d.substr(p.header))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCveVZqvNhsr",
        "colab_type": "text"
      },
      "source": [
        "## üíÖ Init Embedder(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3k194xUFy20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from protocol_parser import  ProtocolPatternFactory\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        "# from contract_patterns import ContractPatternFactory\n",
        "elmo_embedder = ElmoEmbedder()\n",
        "elmo_embedder_default = ElmoEmbedder(layer_name=\"default\")\n",
        "\n",
        "# protocols_factory = ProtocolPatternFactory(elmo_embedder)\n",
        "# contracts_factory = ContractPatternFactory(elmo_embedder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to0MzdyWjEWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "from legal_docs import tokenize_doc_into_sentences_map, ContractValue\n",
        "from ml_tools import *\n",
        "from parsing import ParsingContext\n",
        "from patterns import *\n",
        " \n",
        "from text_tools import *\n",
        "\n",
        "# legal_docs.py\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdn6goa45hoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTvjEfAxPzeI",
        "colab_type": "text"
      },
      "source": [
        "# New"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnRQ5Cg-dCTA",
        "colab_type": "text"
      },
      "source": [
        "## Patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEbp4bLUGSLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pandas as pd\n",
        "# pattern_names=['a', 'b', 'c']\n",
        "# pattern__embeddings = [[1,0,0], [1,1,0], [1,0,1]]\n",
        "# sentences_embeddings = [[1, 0, 0],  [1, 0, 1]]\n",
        "\n",
        "# patterns_named_embeddings = pd.DataFrame (pattern__embeddings, columns=pattern_names)\n",
        " \n",
        "\n",
        "# # for i, col in patterns_named_embeddings.iteritems():\n",
        "# #   print(col.name, col.values)\n",
        "\n",
        "# patterns_named_embeddings.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lzTAUuXh1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# patterns_named_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7nPFWhw9EHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum, unique, EnumMeta\n",
        "from typing import List\n",
        "from structures import DisplayStringEnumMeta, CharterSubject\n",
        " \n",
        " \n",
        "\n",
        "str_consent_patterns=[\n",
        "  '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫',\n",
        "  '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫',\n",
        "  '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –∫–∞–∫–æ–π-–ª–∏–±–æ —Å–¥–µ–ª–∫–∏ –û–±—â–µ—Å—Ç–≤–∞',\n",
        "  '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏ c–¥–µ–ª–∫–∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Ä–∞–≤–Ω–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç'\n",
        "]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezyyvDYGdEs5",
        "colab_type": "text"
      },
      "source": [
        "## classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ie4ehdhE1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from legal_docs import embedd_sentences, LegalDocumentExt\n",
        "\n",
        "class CharterDocument(LegalDocumentExt):\n",
        "\n",
        "  def __init__(self, doc: LegalDocument):\n",
        "    super().__init__(doc)\n",
        "    if doc is not None:\n",
        "      self.__dict__ = doc.__dict__\n",
        "\n",
        "    self.sentence_map: TextMap = None\n",
        "    self.sentences_embeddings = None\n",
        "\n",
        "    self.distances_per_sentence_pattern_dict = {}\n",
        "\n",
        "    self.charity_tags = []\n",
        "    self.org_levels = []\n",
        "    self.constraint_tags = []\n",
        "    self.org_level_tags = []\n",
        "    \n",
        "\n",
        "    self.margin_values: [ContractValue] = []\n",
        "\n",
        "  def get_tags(self) -> [SemanticTag]:\n",
        "    tags = []\n",
        "    tags += self.charity_tags\n",
        "    tags += self.org_levels\n",
        "    tags += self.org_level_tags\n",
        "    tags += self.constraint_tags\n",
        "\n",
        "    for mv in self.margin_values:\n",
        "      tags += mv.as_list()\n",
        "\n",
        "    return tags\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhNwA4xxP1kO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from embedding_tools import AbstractEmbedder\n",
        "# from legal_docs import map_headlines_to_patterns\n",
        "from charter_parser import embedd_charter_subject_patterns, get_charter_subj_attentions\n",
        "from charter_parser import competence_headline_pattern_prefix\n",
        "from contract_parser import find_value_sign_currency_attention\n",
        "\n",
        "class CharterParser(ParsingContext):\n",
        "  strs_subjects_patterns = {\n",
        "\n",
        "    CharterSubject.Deal: [\n",
        "      '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫'\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Charity: [\n",
        "      '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏–π –Ω–∞ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏',\n",
        "      '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–º–æ—â–∏',\n",
        "      '—Å–¥–µ–ª–æ–∫ –¥–∞—Ä–µ–Ω–∏—è',\n",
        "      '–¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å–ø–æ–Ω—Å–æ—Ä—Å–∫–æ–≥–æ –∏ –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞',\n",
        "      '–ø–µ—Ä–µ–¥–∞—á–∞ –≤ –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–µ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ',\n",
        "      '–º–µ–Ω—ã, –¥–∞—Ä–µ–Ω–∏—è, –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–µ –æ—Ç—á—É–∂–¥–µ–Ω–∏–µ '\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Lawsuit: [\n",
        "      '–æ –Ω–∞—á–∞–ª–µ/—É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ª—é–±—ã—Ö —Å—É–¥–µ–±–Ω—ã—Ö —Å–ø–æ—Ä–æ–≤ –∏ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤',\n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏–∏ –û–±—â–µ—Å—Ç–≤–æ–º –º–∏—Ä–æ–≤–æ–≥–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è –ø–æ —Å—É–¥–µ–±–Ω–æ–º—É –¥–µ–ª—É —Å —Ü–µ–Ω–æ–π –∏—Å–∫–∞ '\n",
        "    ],\n",
        "\n",
        "    CharterSubject.RealEstate: [\n",
        "      '—Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—Ç—á—É–∂–¥–∞–µ–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞',\n",
        "      '—Å–¥–µ–ª–∫–∏ —Å –∏–º—É—â–µ—Å—Ç–≤–æ–º –û–±—â–µ—Å—Ç–≤–∞',\n",
        "      '—Å–¥–µ–ª–æ–∫ ( –≤ —Ç–æ–º —á–∏—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–¥–µ–ª–æ–∫ ) —Å –∏–º—É—â–µ—Å—Ç–≤–æ–º –û–±—â–µ—Å—Ç–≤–∞'\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Insurance: [\n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è',\n",
        "      '–≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è'\n",
        "      '—Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–æ–∫ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è'\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Consulting: [\n",
        "      '–¥–æ–≥–æ–≤–æ—Ä–∞ –æ–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —É—Å–ª—É–≥',\n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞',\n",
        "      '—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞ –æ–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —É—Å–ª—É–≥ –∏–ª–∏ –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞',\n",
        "      '–æ–∫–∞–∑–∞–Ω–∏—è –æ–±—â–µ—Å—Ç–≤—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —É—Å–ª—É–≥ '\n",
        "    ],\n",
        "\n",
        "    CharterSubject.Other: [\n",
        "      '—Ä–µ—à–µ–Ω–∏—è –æ –≤–∑—ã—Å–∫–∞–Ω–∏–∏ —Å –ì–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ —É–±—ã—Ç–∫–æ–≤',\n",
        "      '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –æ–± –æ—Ç—Å—Ç—É–ø–Ω–æ–º , –Ω–æ–≤–∞—Ü–∏–∏ –∏/–∏–ª–∏ –ø—Ä–æ—â–µ–Ω–∏–∏ –¥–æ–ª–≥–∞ , –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –æ–± —É—Å—Ç—É–ø–∫–µ –ø—Ä–∞–≤–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥–µ –¥–æ–ª–≥–∞',\n",
        "      '–æ –≤—ã–¥–∞—á–µ –∏–ª–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –û–±—â–µ—Å—Ç–≤–æ–º –≤–µ–∫—Å–µ–ª–µ–π , –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ –ø–æ –Ω–∏–º –ø–µ—Ä–µ–¥–∞—Ç–æ—á–Ω—ã—Ö –Ω–∞–¥–ø–∏—Å–µ–π , –∞–≤–∞–ª–µ–π , –ø–ª–∞—Ç–µ–∂–µ–π',\n",
        "      '–Ω–µ—Ü–µ–ª–µ–≤–æ–µ —Ä–∞—Å—Ö–æ–¥–æ–≤–∞–Ω–∏–µ –û–±—â–µ—Å—Ç–≤–æ–º –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤'\n",
        "    ]\n",
        "\n",
        "  }\n",
        "\n",
        "  def _make_patterns(self):\n",
        "    p=competence_headline_pattern_prefix #just shortcut\n",
        "    comp_str_pat = []\n",
        "    comp_str_pat += [[PATTERN_DELIMITER.join([p, ol.name]), ol.display_string.lower()] for ol in OrgStructuralLevel]\n",
        "    comp_str_pat += [[PATTERN_DELIMITER.join([p, 'comp', 'q', ol.name]), f'–∫ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ {ol.display_string} –æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã'.lower()] for ol in OrgStructuralLevel]\n",
        "    comp_str_pat += [[PATTERN_DELIMITER.join([p, 'comp', ol.name]), f\"–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ {ol.display_string}\".lower()] for ol in OrgStructuralLevel]\n",
        "\n",
        "    _key = PATTERN_DELIMITER.join([p, 'comp', 'qr', OrgStructuralLevel.ShareholdersGeneralMeeting.name])\n",
        "    comp_str_pat += [[_key,  '–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è –û–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤ –û–±—â–µ—Å—Ç–≤–∞'.lower() ]]\n",
        "\n",
        "    _key = PATTERN_DELIMITER.join([p, 'comp', 'qr', OrgStructuralLevel.BoardOfDirectors.name])\n",
        "    comp_str_pat += [[_key,  '–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –û–±—â–µ—Å—Ç–≤–∞'.lower() ]]\n",
        "    \n",
        "    \n",
        "    self.patterns_dict = comp_str_pat\n",
        "\n",
        "  def __init__(self, embedder: AbstractEmbedder, elmo_embedder_default: AbstractEmbedder):\n",
        "    ParsingContext.__init__(self, embedder)\n",
        "\n",
        "    self.patterns_dict = []\n",
        "\n",
        "    self.elmo_embedder_default: AbstractEmbedder = elmo_embedder_default\n",
        "\n",
        "    self._make_patterns()\n",
        "    patterns_texts = [p[1] for p in self.patterns_dict]\n",
        "\n",
        "    self.patterns_embeddings = elmo_embedder_default.embedd_strings(patterns_texts)\n",
        "    self.subj_patterns_embeddings = embedd_charter_subject_patterns(CharterParser.strs_subjects_patterns,\n",
        "                                                                    elmo_embedder_default)\n",
        "\n",
        "  def ebmedd(self, doc: CharterDocument):\n",
        "    doc.sentence_map = tokenize_doc_into_sentences_map(doc, 200)\n",
        "\n",
        "    ### ‚öôÔ∏èüîÆ SENTENCES embedding\n",
        "    doc.sentences_embeddings = embedd_sentences(doc.sentence_map, self.elmo_embedder_default)\n",
        "\n",
        "    doc.distances_per_sentence_pattern_dict = calc_distances_per_pattern_dict(doc.sentences_embeddings,\n",
        "                                                                              self.patterns_dict,\n",
        "                                                                              self.patterns_embeddings)\n",
        "\n",
        "  def analyse(self, charter: CharterDocument):\n",
        "\n",
        "    #patterns_by_headers : (pattern_name, pattern_suffix, confidence, headers[header_number], doc.paragraphs[header_number])\n",
        "    patterns_by_headers = self.map_charter_headlines_to_patterns(charter)\n",
        "\n",
        "    # patterns_by_headers =  attribute_patternmatch_to_index(header_to_pattern_distances, threshold=0.6):\n",
        "    # patterns_by_headers =  attribute_patternmatch_to_index(header_to_pattern_distances, threshold=0.6):\n",
        "    #TODO use \n",
        "    #patterns_by_headers =  attribute_patternmatch_to_index(header_to_pattern_distances, threshold=HyperParameters.header_topic_min_confidence):\n",
        "\n",
        "\n",
        "    charter.margin_values = []\n",
        "    charter.constraint_tags = []\n",
        "    charter.charity_tags = []\n",
        "    # --------------\n",
        "    # (('headline/comp/qr/ShareholdersGeneralMeeting', 16), 0.8978644013404846),\n",
        "    filtered = [p_mapping for p_mapping in patterns_by_headers if p_mapping]\n",
        "    \n",
        "    found_org_levels=[]  \n",
        "    kkk=0\n",
        "    for p_mapping in filtered:   \n",
        "      kkk+=1\n",
        "      print('filtered', p_mapping)\n",
        "\n",
        "      _paragraph_id = p_mapping[0][1] \n",
        "      print('_paragraph_id', _paragraph_id)\n",
        "      _pattern_name = p_mapping[0][0] \n",
        "      print('_pattern_name', _pattern_name)\n",
        "      paragraph = charter.paragraphs[_paragraph_id]\n",
        "      confidence = p_mapping[1]\n",
        "      _org_level_name = _pattern_name.split('/')[-1]      \n",
        "      org_level:OrgStructuralLevel = OrgStructuralLevel[_org_level_name]\n",
        "      subdoc = charter.subdoc_slice(paragraph.body.as_slice())\n",
        "\n",
        "      parent_org_level_tag = SemanticTag(f\"{org_level.name}-{kkk}\", org_level.name, paragraph.body.span)\n",
        "      parent_org_level_tag.confidence=confidence\n",
        "      \n",
        "\n",
        "      constraint_tags, values = self.attribute_charter_subjects(subdoc, self.subj_patterns_embeddings,\n",
        "                                                                parent_org_level_tag)\n",
        "      \n",
        "\n",
        "      for value in values:\n",
        "        value+=subdoc.start\n",
        "\n",
        "      for constraint_tag in constraint_tags:\n",
        "        constraint_tag.offset(subdoc.start)\n",
        "\n",
        "      charter.margin_values += values\n",
        "      charter.constraint_tags += constraint_tags\n",
        "\n",
        "\n",
        "      _parent_org_level_tag_keys=[]      \n",
        "      if charter.margin_values: \n",
        "        _key = parent_org_level_tag.get_key()\n",
        "        if _key in _parent_org_level_tag_keys:\n",
        "          parent_org_level_tag.kind= _key + f\"-{len(_parent_org_level_tag_keys)}\"\n",
        "        charter.org_levels.append(parent_org_level_tag)\n",
        "        _parent_org_level_tag_keys.append(_key)\n",
        "\n",
        "      \n",
        "      # charity_subj_av_words = subject_attentions_map[CharterSubject.Charity]['words']\n",
        "      # charity_tag = find_charity_paragraphs(parent_org_level_tag, subdoc, (charity_subj_av_words + consent_words) / 2)\n",
        "      # # print(charity_tag)\n",
        "      # if charity_tag is not None:\n",
        "      #   charter.charity_tags.append(charity_tag)\n",
        "\n",
        "  def _rename_margin_values_tags(self, values):\n",
        "   \n",
        "    for value in values:     \n",
        "      if value.sign.value<0:\n",
        "        sfx='-max'\n",
        "      elif value.sign.value>0:\n",
        "        sfx='-min'\n",
        "      else:\n",
        "        sfx = ''\n",
        "\n",
        "      value.parent.kind= f\"constraint{sfx}\"\n",
        "    \n",
        "    known_keys=[]\n",
        "    k=0 #constraints numbering\n",
        "    for value in values:\n",
        "      k+=1\n",
        "      if value.parent.get_key() in known_keys:\n",
        "         value.parent.kind= f\"{value.parent.kind}{TAG_KEY_DELIMITER}{k}\"\n",
        "\n",
        "      known_keys.append(value.parent.get_key())\n",
        "\n",
        "  def attribute_charter_subjects(self, subdoc: LegalDocumentExt, emb_subj_patterns, parent_org_level_tag: SemanticTag):\n",
        "    \"\"\"\n",
        "    :param subdoc:\n",
        "    :param emb_subj_patterns:\n",
        "\n",
        "          emb_subj_patterns[subj] = {\n",
        "            'patterns':patterns,\n",
        "            'embedding':patterns_emb\n",
        "          }\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "    values: List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "    # -------------------\n",
        "\n",
        "    # collect sentences having constraint values\n",
        "    sentence_spans = []   \n",
        "    for value in values:\n",
        "      sentence_span = subdoc.tokens_map.sentence_at_index(value.parent.span[0], return_delimiters=True)\n",
        "      if sentence_span not in sentence_spans:\n",
        "        sentence_spans.append(sentence_span)\n",
        "    sentence_spans = merge_colliding_spans(sentence_spans, eps=1)\n",
        "\n",
        "    # ---\n",
        "    # attribute sentences to subject\n",
        "    constraint_tags = []\n",
        "\n",
        "    i=0\n",
        "    for span in sentence_spans:\n",
        "      i+=1\n",
        "      max_confidence = 0\n",
        "      best_subject:CharterSubject = CharterSubject.Other\n",
        "\n",
        "      for subj in subject_attentions_map.keys():\n",
        "        av = subject_attentions_map[subj]['words']\n",
        "\n",
        "        confidence_region = av[span[0]:span[1]]\n",
        "        confidence = estimate_confidence_by_mean_top_non_zeros(confidence_region)\n",
        "\n",
        "        if confidence > max_confidence:\n",
        "          max_confidence = confidence\n",
        "          best_subject = subj\n",
        "\n",
        "      #\n",
        "      constraint_tag = SemanticTag(f'{best_subject.name}-{i}', best_subject.name, span, parent=parent_org_level_tag)\n",
        "      constraint_tags.append(constraint_tag)\n",
        "\n",
        "      # nest values\n",
        "      for value in values:\n",
        "        if constraint_tag.is_nested(value.parent.span):\n",
        "          value.parent.set_parent_tag(constraint_tag)\n",
        "        \n",
        "      self._rename_margin_values_tags(values)\n",
        "\n",
        "    return constraint_tags, values\n",
        "\n",
        "\n",
        "  def map_charter_headlines_to_patterns(self, charter: LegalDocument):\n",
        "    charter_parser = self\n",
        " \n",
        "\n",
        "    p_suffices = [PATTERN_DELIMITER.join(   c[0].split(PATTERN_DELIMITER)[1:]) for c in self.patterns_dict ]\n",
        "\n",
        "    # map = (pattern_name, pattern_suffix, confidence, headers[header_number], doc.paragraphs[header_number])\n",
        "    patterns_names = [c[0] for c in charter_parser.patterns_dict]\n",
        "    print('patterns_names', patterns_names)\n",
        "    print(self.patterns_embeddings.shape)\n",
        "    patterns_named_embeddings = pd.DataFrame(self.patterns_embeddings.T, columns=patterns_names)\n",
        "    map_ = map_headlines_to_patterns(charter,\n",
        "                                                patterns_named_embeddings,\n",
        "                                                charter_parser.elmo_embedder_default,\n",
        "                                                competence_headline_pattern_prefix,\n",
        "                                                p_suffices)\n",
        "\n",
        "    return map_\n",
        "\n",
        "\n",
        "\n",
        "results = wp.read_doc(charter_filename)\n",
        "# results = wp.read_doc('/content/–ï–Æ –£—Å—Ç–∞–≤.docx')\n",
        "\n",
        "for d in results['documents'][:1]:  # XXX\n",
        "  if 'CHARTER' == d['documentType']:    \n",
        "    charter = join_paragraphs(d, 'no_id')    \n",
        "\n",
        "print_headers(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16JvY5XcpdxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def map_headlines_to_patterns(doc: LegalDocument,\n",
        "                              patterns_named_embeddings,\n",
        "                              elmo_embedder_default: AbstractEmbedder,\n",
        "                              pattern_prefix: str,\n",
        "                              pattern_suffixes: [str]):\n",
        "  headers: [str] = doc.headers_as_sentences()\n",
        "\n",
        "  if not headers:\n",
        "    return [], []\n",
        "\n",
        "  headers_embedding = elmo_embedder_default.embedd_strings(headers)\n",
        "\n",
        "  header_to_pattern_distances = calc_distances_per_pattern (headers_embedding, patterns_named_embeddings)\n",
        "  return attribute_patternmatch_to_index(header_to_pattern_distances)\n",
        "   \n",
        "charter_parser = CharterParser(elmo_embedder, elmo_embedder_default)\n",
        "charter_parser.ebmedd(charter)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gY0ozfmu9CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _find_max_xy(vals):\n",
        "  max_x = 0\n",
        "  max_y = 0\n",
        "  maxval = 0\n",
        "  for x in range(vals.shape[0]):    \n",
        "    found = False\n",
        "    for y in range(vals.shape[1]):\n",
        "      val = vals[max_x][max_y]\n",
        "      if vals[x][y] > val:\n",
        "        max_y=y\n",
        "        max_x=x\n",
        "        maxval=val\n",
        "\n",
        "  return max_x, max_y, maxval\n",
        "\n",
        "\n",
        "def attribute_patternmatch_to_index(header_to_pattern_distances_:pd.DataFrame, threshold=0.6):\n",
        "  vals = header_to_pattern_distances_.values\n",
        "  headers_n =  vals.shape[0]\n",
        "  print('headers_n', headers_n)\n",
        "   \n",
        "  pairs=[]\n",
        "  for __header_index in range(headers_n):\n",
        "\n",
        "    header_index, pattern_index, maxval = _find_max_xy(vals)\n",
        "    pattern_name = header_to_pattern_distances_.columns[pattern_index]\n",
        "    max_pair = ((  pattern_name, header_index), maxval)          \n",
        "\n",
        "    if maxval > threshold:     \n",
        "      pairs.append(max_pair)\n",
        "\n",
        "    vals[:][pattern_index] =-1\n",
        "    vals[header_index][:] =-1\n",
        "\n",
        "      # # used_keys.append(mk)\n",
        "      # print('USED',mi)\n",
        "      # print('USED mk', max_pair[0][0])\n",
        "       \n",
        "       \n",
        "      \n",
        "\n",
        "  # print ('attribute_patternmatch_to_index', pairs)\n",
        "  # print(header_to_pattern_distances)\n",
        "  return pairs\n",
        "  \n",
        " \n",
        "  \n",
        "charter_parser.analyse(charter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6s3kEe6-qs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# globals()['header_to_pattern_distances'].T[3][:] = -1\n",
        "# globals()['header_to_pattern_distances_']\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkBu4zdkNl_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "renderer_.render_color_text(charter.tokens_cc, charter.get_tags_attention() , _range=(0,5) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgoO_XpMoNda",
        "colab_type": "text"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUIX6Mks9ife",
        "colab_type": "text"
      },
      "source": [
        "### constraint_tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEtzKsuk9lsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tag in charter.org_levels:\n",
        "  print (tag.get_parent())\n",
        "  print (tag)\n",
        "\n",
        "  sub = charter [tag.as_slice()]\n",
        "  renderer_.render_color_text(sub.tokens_cc, sub.get_tags_attention() , _range=(-5,5) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWMpB54Z9mpE",
        "colab_type": "text"
      },
      "source": [
        "### Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d66dqRR-oexX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tag in charter.get_tags():\n",
        "  print (tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45KP7yKoLVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from patterns import build_sentence_patterns\n",
        "# from charter_parser import embedd_charter_subject_patterns\n",
        "\n",
        "# emb_subj_patterns = embedd_charter_subject_patterns(strs_subjects_patterns, elmo_embedder_default)\n",
        "\n",
        "# consent_patterns = build_sentence_patterns(str_consent_patterns, 'consent', elmo_embedder_default)\n",
        "# consent_patterns_emb = elmo_embedder_default.embedd_strings(str_consent_patterns)\n",
        "\n",
        "# ###-----debug-----\n",
        "# emb_subj_patterns[CharterSubject.Insurance]['patterns']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1yEWke4nnZA",
        "colab_type": "text"
      },
      "source": [
        "## Find org level sections\n",
        " - find all headers spans\n",
        " - compare headers with patterns \n",
        " - find interresting spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piZZmrNTnxJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_slices = [p.header.as_slice() for p in charter.paragraphs]\n",
        "header_slices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUpmN9C0saqE",
        "colab_type": "text"
      },
      "source": [
        "remap to sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlDC8PFHseJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_slices_sent = charter.tokens_map.remap_slices(header_slices, charter.sentence_map)\n",
        "header_slices_sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRy1U1NiuaN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "section_ends = [ x[0] for x in header_slices_sent ] + [ len(charter.sentence_map) ] #+end of doc\n",
        "section_ends"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtzK_vohox1E",
        "colab_type": "text"
      },
      "source": [
        "Available sentence pattern (names)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgkFOANqotUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter.distances_per_sentence_pattern_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR3URDk8SAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fi \n",
        "# av_competence_headline_pattern = max_exclusive_pattern_by_prefix(charter.distances_per_sentence_pattern_dict, competence_headline_pattern_prefix)\n",
        "# av_competence_headline_pattern = relu(av_competence_headline_pattern, 0.66)\n",
        "\n",
        "# av_interresting_headers = headers_att * av_competence_headline_pattern\n",
        "# #words map\n",
        "# # sections_spans_words = charter.sentence_map.remap_slices(v, charter.tokens_map)\n",
        " \n",
        "# # v = charter.distances_per_sentence_pattern_dict['headline.head.directors.n']# - 0.1*charter.distances_per_sentence_pattern_dict['headline.head.directors.not'] \n",
        "\n",
        "# renderer_.render_color_text(charter.sentence_map.tokens, av_interresting_headers, _range=(0,1), separator='¬∂<br>')\n",
        "# # renderer_.render_color_text(charter.tokens_cc, sections_spans_words, _range=(0,1) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Q9zLUBPmU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter_parser.patterns_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVNAqHsQPnZe",
        "colab_type": "text"
      },
      "source": [
        "## Find relevant (org struct) paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNkbOL969lSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from doc_structure import get_tokenized_line_number\n",
        "from ml_tools import calc_distances_per_pattern_dict\n",
        "from legal_docs import headers_as_sentences, map_headlines_to_patterns\n",
        " \n",
        "# patterns_by_headers, header_to_pattern_distances = charter_parser.map_charter_headlines_to_patterns(charter )\n",
        "patterns_by_headers, header_to_pattern_distances  = charter_parser.map_charter_headlines_to_patterns(charter )\n",
        "for k in patterns_by_headers :\n",
        "  # if k:\n",
        "  print(k)\n",
        "\n",
        "\n",
        "print('\\n\\n headline/comp/BoardOfDirectors')\n",
        "renderer_.render_color_text(headers_as_sentences(charter), header_to_pattern_distances['headline/comp/BoardOfDirectors'], _range=(0,1), separator='üò± <br>')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVjHEoSxNnSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "# sns.set()\n",
        "\n",
        "# Build data\n",
        "np.random.seed(0) # seed the random number generator in order to make the run repeatable\n",
        "dat = pd.DataFrame(header_to_pattern_distances)\n",
        "df_labels = pd.DataFrame(header_to_pattern_distances)\n",
        " \n",
        "print(df_labels)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Op-Nm8eV1tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dat.idxmax(axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53XzExklNy-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dat.drop( columns=['headline/BoardOfDirectors'], axis=1)\n",
        "\n",
        "# dat.drop( [10])\n",
        "plt.figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "\n",
        "plt.suptitle(\"Seaborn heatmap example to display 2d percentage values\", color='m')\n",
        "sns.set(font_scale=0.8)\n",
        "\n",
        "ax = sns.heatmap(dat.T,  fmt=\"\", annot_kws={\"size\": 4}, vmin=0, vmax=1)\n",
        "# ax.set(xlabel='x label', ylabel='y label')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gEAF5DHNLWX",
        "colab_type": "text"
      },
      "source": [
        "## get_values_attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqd2E2E39P1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from contract_parser import _find_most_relevant_paragraph, find_value_sign_currency_attention\n",
        "\n",
        "def get_values_attention(subdoc, values:List[ContractValue]):\n",
        "  numbers_attention = np.zeros(len(subdoc.tokens_map))\n",
        "    \n",
        "  for v in values:\n",
        "    numbers_attention  [ v.value.as_slice() ] += v.value.confidence\n",
        "    numbers_attention  [ v.currency.as_slice() ] += v.currency.confidence\n",
        "    numbers_attention  [ v.sign.as_slice() ] += v.sign.confidence\n",
        "    numbers_attention  [ v.parent.as_slice() ] += v.parent.confidence\n",
        "\n",
        "  return numbers_attention/2\n",
        "\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level =  p_mapping[1]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice(paragraph.body.as_slice())\n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "\n",
        "    #color \n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "    \n",
        "    for v in values:\n",
        "      numbers_attention  [ v.value.as_slice() ] = 1\n",
        "      numbers_attention  [ v.currency.as_slice() ] = 1\n",
        "      numbers_attention  [ v.sign.as_slice() ] = 1\n",
        "\n",
        "    print('%'*100)\n",
        "    print(org_level)\n",
        "    renderer_.render_color_text(subdoc.tokens_map.tokens, numbers_attention, _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1_1_XdNiWX",
        "colab_type": "text"
      },
      "source": [
        "## charter_subjects_attribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWtENbeDPe2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del get_charter_subj_attentions\n",
        "\n",
        "from legal_docs import remap_attention_vector\n",
        "from contract_parser import _find_most_relevant_paragraph, find_value_sign_currency_attention\n",
        "from legal_docs import remap_attention_vector, ContractValue, LegalDocumentExt\n",
        "from ml_tools import *\n",
        "\n",
        " \n",
        "from charter_parser import find_charity_paragraphs, get_charter_subj_attentions\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "###DEBUG attribute_charter_subjects -------------------\n",
        "\n",
        "\n",
        "charter.margin_values = []\n",
        "charter.constraint_tags=[]\n",
        "# --------------\n",
        "filtered = [p_mapping for p_mapping in patterns_by_headers if p_mapping]\n",
        "for p_mapping in filtered:\n",
        "  print('-'*40)\n",
        "  \n",
        "  paragraph = p_mapping[4]\n",
        "  org_level_name = p_mapping[1].split('/')[-1]\n",
        "  org_level = OrgStructuralLevel[org_level_name]\n",
        "  print(org_level_name)\n",
        "  subdoc = charter.subdoc_slice(paragraph.body.as_slice())\n",
        "  parent_org_level_tag = SemanticTag(org_level.name, org_level, paragraph.body.span)\n",
        "\n",
        "  constraint_tags, values = charter_parser.attribute_charter_subjects(subdoc, charter_parser.subj_patterns_embeddings,\n",
        "                                                            parent_org_level_tag)\n",
        "  \n",
        "  charter.margin_values += values\n",
        "  charter.constraint_tags += constraint_tags\n",
        "  for c in constraint_tags:\n",
        "    print(c)\n",
        "\n",
        "# for p_mapping in patterns_by_headers:\n",
        "#   print()\n",
        "#   print()\n",
        "#   if p_mapping:\n",
        "#     print('p_mapping', p_mapping)\n",
        "#     paragraph =  p_mapping[4]\n",
        "#     org_level_name =  p_mapping[1].split('/')[-1]\n",
        "#     org_level = OrgStructuralLevel[org_level_name]\n",
        "\n",
        "#     subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "#     parent_org_level_tag = SemanticTag(org_level.name, org_level, paragraph.body.span)\n",
        "\n",
        "#     constraint_tags, values = charter_parser.attribute_charter_subjects(subdoc, self.subj_patterns_embeddings, parent_org_level_tag)\n",
        "\n",
        "#     print(org_level)\n",
        "#     print('constraint_tags')\n",
        "#     for c in constraint_tags:\n",
        "#       print(c)\n",
        "#     print('values')\n",
        "\n",
        "for c in charter.margin_values:\n",
        "  print(c.value)\n",
        "  print ('quote:', charter.substr (c.parent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoPDev4ZRvk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charter.margin_values=[]\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    # paragraph =  p_mapping[4]\n",
        "    # org_level =  p_mapping[1]\n",
        "\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level_name =  p_mapping[1].split('/')[-1] # TODO FIX it, looks weird\n",
        "    org_level = OrgStructuralLevel[org_level_name]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "    parent_org_level_tag = SemanticTag(org_level.name, org_level, paragraph.body.span)\n",
        "\n",
        "    consent_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "                                                                consent_patterns,\n",
        "                                                                consent_patterns_emb)\n",
        "    \n",
        "    # charity_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "    #                                                             consulting_patterns,\n",
        "    #                                                             consulting_patterns_emb)\n",
        "    \n",
        "    # print(order_patterns_distances.keys())\n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "\n",
        "    consent_v = relu(max_exclusive_pattern_by_prefix(consent_patterns_distances, 'consent/'),0.5)\n",
        "    consent_words = remap_attention_vector(consent_v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "\n",
        "    # charity_subj_av = relu(max_exclusive_pattern_by_prefix(charity_patterns_distances, 'consulting/'),0.6)\n",
        "    # charity_subj_av_words = remap_attention_vector(charity_subj_av, subdoc.sentence_map, subdoc.tokens_map)\n",
        "    # order_patterns_distances['order/consent']\n",
        "    # renderer_.render_color_text(subdoc.sentence_map.tokens, v, _range=(0,1), separator='üò± <br>')\n",
        "    \n",
        "    # charity_patterns_distances_words = remap_attention_vector(v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "\n",
        "    \n",
        "    \n",
        "    # dd = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "    # charity_subj_av_words = dd[CharterSubject.Charity]['words']\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "\n",
        "    charity_subj_av_words = subject_attentions_map[CharterSubject.Charity]['words']\n",
        "    charity_tag = find_charity_paragraphs(parent_org_level_tag, subdoc, (charity_subj_av_words+consent_words)/2)\n",
        "    print(charity_tag)\n",
        "    if charity_tag is not None:\n",
        "      charter.charity_tags.append(charity_tag)\n",
        "\n",
        " \n",
        "\n",
        "    constraint_tags, values = attribute_charter_subjects(subdoc, emb_subj_patterns, parent_org_level_tag)\n",
        "    charter.margin_values += values\n",
        "\n",
        "    # renderer_.render_color_text(subdoc.tokens_cc, charity_subj_av_words + numbers_attention , _range=(0,2) )\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GfSFERvLrGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(charter.tokens_cc, charter.get_tags_attention() , _range=(0,2) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oZp0Fc8RhPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN4SKxM_QETF",
        "colab_type": "text"
      },
      "source": [
        "### Test attribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgpZqbgJF_0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "for p_mapping in patterns_by_headers:\n",
        "  if p_mapping:\n",
        "    # paragraph =  p_mapping[4]\n",
        "    # org_level =  p_mapping[1]\n",
        "\n",
        "    paragraph =  p_mapping[4]\n",
        "    org_level_name =  p_mapping[1].split('/')[-1]\n",
        "    org_level=OrgStructuralLevel[org_level_name]\n",
        "\n",
        "    \n",
        "    subdoc = charter.subdoc_slice( paragraph.body.as_slice())\n",
        "    \n",
        "    consent_patterns_distances = calc_distances_per_pattern_dict(subdoc.sentences_embeddings,\n",
        "                                                                consent_patterns,\n",
        "                                                                consent_patterns_emb)\n",
        "    \n",
        "     \n",
        "    \n",
        "\n",
        "    consent_v = relu(max_exclusive_pattern_by_prefix(consent_patterns_distances, 'consent/'),0.5)\n",
        "    consent_words = remap_attention_vector(consent_v, subdoc.sentence_map, subdoc.tokens_map)\n",
        "     \n",
        "     \n",
        "    values:List[ContractValue] = find_value_sign_currency_attention(subdoc, None)\n",
        "\n",
        "    #collect sentences with constraint values\n",
        "    sentence_spans=[]\n",
        "    for value in values:\n",
        "      sentence_span = subdoc.tokens_map.sentence_at_index(value.parent.span[0], return_delimiters=True)      \n",
        "      if sentence_span not in sentence_spans:\n",
        "        sentence_spans.append(sentence_span) \n",
        "    # sentence_spans = merge_colliding_spans(sentence_spans, eps=1)\n",
        "     \n",
        "\n",
        "\n",
        "    subject_attentions_map = get_charter_subj_attentions(subdoc, emb_subj_patterns)\n",
        "\n",
        "    # attribute sentences to subject\n",
        "    \n",
        "    for span in sentence_spans:\n",
        "      print ( )\n",
        "      print ('-'*30)\n",
        "      print ('VALUE', sentence_span, subdoc.tokens_map.text_range(span))\n",
        "      for subj in subject_attentions_map.keys():\n",
        "        av = subject_attentions_map[subj]['words']\n",
        "        \n",
        "        confidence_region = av[span[0]:span[1]]\n",
        "        confidence = estimate_confidence_by_mean_top_non_zeros(confidence_region)\n",
        "\n",
        "        renderer_.render_color_text(subdoc.tokens_map[span[0]:span[1]], confidence_region , _range=(0,1) )\n",
        "        print(subj,  confidence )\n",
        "      print ('~'*30)\n",
        "    numbers_attention = get_values_attention(subdoc, values)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    charity_subj_av_words = subject_attentions_map[CharterSubject.Consulting]['words']\n",
        "    charity_tag = find_charity_paragraphs(org_level, subdoc,(charity_subj_av_words+consent_words)/2)\n",
        "    print(charity_tag)\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDHXPCl9JL2N",
        "colab_type": "text"
      },
      "source": [
        "# Save json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tcAlZx6FGWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_str=charter.to_json()\n",
        "\n",
        "json_obj = DocumentJson.from_json(json_str)\n",
        "with open('charter.json', 'w') as file: \n",
        "  file.write(json_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}