{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev: Parse ALL contracts and save all to JSONs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x_T-yqkhT2yo"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nemoware/analyser/blob/split-tf-phase/notebooks/dev_Parse_ALL_contracts_and_save_all_to_JSONs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raCAExPc0iE-",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Params\n",
        " \n",
        "\n",
        "document_parser_lib_version='1.1.13'#@param {type:\"string\"}\n",
        "# _git_branch = 'protocols-4'\n",
        "_git_branch = 'split-tf-phase'#@param {type:\"string\"}\n",
        "# _git_branch = 'semantic-tags'\n",
        "\n",
        "do_parse_contracts=False#@param {type:\"boolean\"}\n",
        "do_parse_protocols=False#@param {type:\"boolean\"}\n",
        "do_parse_charters=True#@param {type:\"boolean\"}\n",
        "\n",
        "do_parse_zip=False#@param {type:\"boolean\"}\n",
        "\n",
        "limit_number_of_files=2#@param {type:\"integer\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGbb8gDSTvuW",
        "colab_type": "text"
      },
      "source": [
        "### Mount gDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7_rZoyzMRzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_parse_contracts or do_parse_protocols or do_parse_charters:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbsxFAqC6pjQ",
        "colab_type": "text"
      },
      "source": [
        "# Import code from gitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F89NLdN6A8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "–ù–∏—á—Ç–æ = None\n",
        "\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ü¶ä GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('‚ù§Ô∏èimporting Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "#----\n",
        "import matplotlib as mpl\n",
        "from analyser.documents import TextMap\n",
        "from colab_support.renderer import HtmlRenderer\n",
        " \n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPLjOzkOATgk",
        "colab_type": "text"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCfuE2dw6wO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import unittest\n",
        "import warnings\n",
        "\n",
        "!pip install pyjarowinkler\n",
        "\n",
        "from analyser.contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from analyser.contract_patterns import ContractPatternFactory\n",
        "from analyser.documents import TextMap\n",
        "from analyser.legal_docs import LegalDocument, DocumentJson\n",
        "from analyser.ml_tools import SemanticTag, filter_values_by_key_prefix\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxRGxnjidln",
        "colab_type": "text"
      },
      "source": [
        "\n",
        " \n",
        "\n",
        "### Utilits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iceqc8qUjX8X",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ‚úÖUtil funcs, rendering\n",
        "\n",
        "\n",
        "def print_semantic_tag(tag: SemanticTag, map: TextMap):\n",
        "  print('-->', f'{tag.kind} \\t\\t [{tag.value}]\\t [{map.text_range(tag.span)}]')\n",
        "\n",
        "def print_headers(contract):\n",
        "  for p in contract.paragraphs:\n",
        "    print ('\\t --> üìÇ',contract.substr(p.header))\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        " \n",
        "  for tag in cd.tags:\n",
        "    if tag.kind !='headline':\n",
        "     print_semantic_tag(tag, wordsmap)\n",
        " \n",
        "\n",
        "\n",
        "def json2html(cd:DocumentJson, render_headlines=False):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['words'])\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        " \n",
        "  if render_headlines:\n",
        "    for hd in cd.headers:    \n",
        "      span = hd['span']      \n",
        "      markup_vector[span[0]:span[1]] += 0.2\n",
        "\n",
        "  for tag in cd.attributes:\n",
        "    if 'span' in  cd.attributes[tag]:\n",
        "      span = cd.attributes[tag]['span']\n",
        "      confidence = cd.attributes[tag]['confidence']\n",
        "      markup_vector[span[0]:span[1]] += confidence\n",
        "    else:\n",
        "      warnings.warn(f'{tag} has no span')\n",
        "\n",
        "  return renderer_.to_color_text(wordsmap.tokens, markup_vector)\n",
        "  \n",
        " \n",
        "def load_and_show_json(filename, render_headlines=False):\n",
        "  with open(filename, 'r') as json_file:\n",
        "    data_str = json_file.read()\n",
        "    # print(str(data_str))\n",
        "    json_obj:DocumentJson = DocumentJson.from_json( data_str)\n",
        "    html = json2html(json_obj, render_headlines=render_headlines)\n",
        "    display(HTML(html))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpHstfXIOVqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        "elmo_embedder = ElmoEmbedder()\n",
        "elmo_embedder_default = ElmoEmbedder(layer_name=\"default\")\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMB4pkuu2wi",
        "colab_type": "text"
      },
      "source": [
        "### Init document-parser lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lib_version = document_parser_lib_version\n",
        "import os\n",
        "if not os.path.isfile(f'document-parser-{lib_version}-distribution.zip'):\n",
        "  !wget https://github.com/nemoware/document-parser/releases/download/$lib_version/document-parser-$lib_version-distribution.zip\n",
        "if not os.path.isdir(f'document-parser-{lib_version}'):\n",
        "  !unzip document-parser-$lib_version-distribution.zip\n",
        "\n",
        " \n",
        "os.environ ['documentparser']=f'/content/document-parser-{lib_version}'\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHfhy1BhMNfm",
        "colab_type": "text"
      },
      "source": [
        "# Export all to JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClO8-VAuT63y",
        "colab_type": "text"
      },
      "source": [
        "#### Parse all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107CPsZglZ7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attempt = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dobawgyTEZ0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import analyser.hyperparams as hp\n",
        "\n",
        "codebasepath = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(hp.__file__)))\n",
        "expected = pd.read_csv ( os.path.join(codebasepath, 'vocab', 'contracts.validation.csv' ) )\n",
        "\n",
        "contracts_filename_prefix = '/content/gdrive/My Drive/GazpromOil/Contracts/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXnyIGsn68o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "attempt += 1\n",
        "dirname = '{0:%y.%m.%d}'.format(datetime.datetime.now())\n",
        "_out = f'/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/JSONs/{_git_branch}-{dirname}-{attempt}/'\n",
        "\n",
        "\n",
        "if not os.path.exists(_out):\n",
        "  os.makedirs(_out)\n",
        "\n",
        "print(_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIlMlf6BBG2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "from integration.word_document_parser import join_paragraphs, WordDocParser\n",
        " \n",
        "\n",
        "filenames_to_jsons = {}\n",
        "\n",
        "def for_each_document(filenames, accept, procedure, _out, save_json=True):\n",
        "  _common_path = os.path.commonpath (filenames)\n",
        "  \n",
        "  wp = WordDocParser()\n",
        "  cnt = 0\n",
        "  for fn in filenames:\n",
        "    filenames_to_jsons[fn] = []\n",
        "    cnt += 1\n",
        "    _relative = os.path.relpath(fn, _common_path )\n",
        "    short_fn = os.path.split (_relative)[-1]\n",
        "    subdir = os.path.dirname(_relative)\n",
        "    \n",
        "    # try:\n",
        "    # ------------------------\n",
        "    print('\\n\\n' )\n",
        "    print('=' * 100)\n",
        "    print(f'reading:\"{fn}\"')      \n",
        "    results = wp.read_doc(fn)\n",
        "    print(\"documents in file:\", len(results['documents']))\n",
        "    k = 0\n",
        "    for doc in results['documents']:  # XXX\n",
        "      k += 1\n",
        "      contract_id = _relative\n",
        "      if accept(doc):\n",
        "        \n",
        "        legal_doc = join_paragraphs(doc, contract_id)\n",
        "        # ------------------------\n",
        "        legal_doc = procedure(legal_doc)\n",
        "        assert legal_doc is not None\n",
        "\n",
        "        if save_json:\n",
        "          _dirr  = os.path.join(_out, subdir)\n",
        "          print('_dirr',_dirr)\n",
        "          _path = os.path.join(_dirr, f'{short_fn}--{k}.json')\n",
        "          print('_path',_path)\n",
        "          if not os.path.exists(_dirr):\n",
        "            os.makedirs(_dirr)\n",
        "\n",
        "          with open(_path, 'w') as file:\n",
        "            #\n",
        "            jjj = DocumentJson(legal_doc)\n",
        "            file.write(jjj.dumps())\n",
        "            filenames_to_jsons[fn].append(_path)\n",
        "            print(f'--> üìÇsaved file to {_path}')\n",
        "            print('^' * 100)\n",
        "\n",
        "    # except Exception as e:\n",
        "    #   print(e)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkHfn-lCE2V2",
        "colab_type": "text"
      },
      "source": [
        "# üß† Contracts\n",
        "Reading only short list of docs listed in the validation CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r0dVpb76jay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numbers\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from analyser.legal_docs import DocumentJson\n",
        "\n",
        "def print_tags(doc):\n",
        "  for t in doc.get_tags():\n",
        "    print(t.get_key(), t.value)\n",
        "\n",
        "\n",
        "def validate_contracts():\n",
        "  codebasepath = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(hp.__file__)))\n",
        "  expected = pd.read_csv(os.path.join(codebasepath, 'vocab', 'contracts.validation.csv'))\n",
        "  actual = pd.read_csv(os.path.join(_out, 'contracts.validation.csv'))\n",
        "\n",
        "  msgs = []\n",
        "  errors = 0\n",
        "\n",
        "  def _check(key, e, a, col) -> int:\n",
        "    _x1 = e[col]\n",
        "    _x2 = a[col]\n",
        "    if (isinstance(_x1, numbers.Number) and math.isnan(_x1)) and (isinstance(_x2, numbers.Number) and math.isnan(_x2)):\n",
        "      return 0\n",
        "    if _x1 != _x2:\n",
        "      msg = f\"{key}:\\t{col} \\n\\t expected: '{e[col]}' \\n\\t   ACTUAL: '{a[col]}'\"\n",
        "      msgs.append(msg)\n",
        "      return 1\n",
        "    return 0\n",
        "\n",
        "  for x in actual['_id']:\n",
        "    r1 = expected[expected['_id'] == x].iloc[0]\n",
        "    r2 = actual[actual['_id'] == x].iloc[0]\n",
        "\n",
        "    errors += _check(x, r1, r2, 'org_1_name')\n",
        "    errors += _check(x, r1, r2, 'org_2_name')\n",
        "    errors += _check(x, r1, r2, 'value')\n",
        "    errors += _check(x, r1, r2, 'subject')\n",
        "\n",
        "  print('=' * 100)\n",
        "  print('^' * 100)\n",
        "  print(f\"ERRRORS: \\t{errors} of {len(actual)} docs\")\n",
        "\n",
        "  with open(os.path.join(_out, 'errors.txt'), 'w') as the_file:\n",
        "    for m in msgs:\n",
        "      the_file.write(m + '\\n')\n",
        "\n",
        "    the_file.write('^' * 100)\n",
        "    the_file.write(f\"\\nERRRORS: \\t{errors} of {len(actual)} docs\")\n",
        "    print(f'errors saved to {the_file}')\n",
        "  return msgs\n",
        "\n",
        "\n",
        "def __attr_val(d, a):\n",
        "  if a in d:\n",
        "    return d[a]['value']\n",
        "\n",
        "\n",
        "def to_csv(filenames_to_jsons):\n",
        "  df = pd.DataFrame(columns=('_id', 'value', 'org_1_name', 'org_2_name', 'subject'))\n",
        "\n",
        "  i = 0\n",
        "  for fn in filenames_to_jsons.keys():\n",
        "    for json_fn in filenames_to_jsons[fn]:\n",
        "      # print(json_fn)\n",
        "      with open(json_fn, 'r') as json_file:\n",
        "        data_str = json_file.read()\n",
        "        json_obj: DocumentJson = DocumentJson.from_json(data_str)\n",
        "        # _id =os.path.relpath( json_obj._id, contracts_filename_prefix )\n",
        "        _id = json_obj._id\n",
        "        df.loc[i] = [_id,\n",
        "                     __attr_val(json_obj.attributes, 'sign_value_currency/value'),\n",
        "                     __attr_val(json_obj.attributes, 'org-1-name'),\n",
        "                     __attr_val(json_obj.attributes, 'org-2-name'),\n",
        "                     __attr_val(json_obj.attributes, 'subject')]\n",
        "        i += 1\n",
        "\n",
        "  df.to_csv(os.path.join(_out, 'contracts.validation.csv'), encoding='utf-8', index=False)\n",
        "  return df\n",
        "\n",
        "# validate_contracts()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUO6YXVOCihU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_contract(contract:LegalDocument) -> LegalDocument:  \n",
        "  print_headers(contract)\n",
        "\n",
        "  contractAnlysingContext.find_org_date_number(contract)\n",
        "  contractAnlysingContext.find_attributes(contract)  \n",
        "  \n",
        "  to_csv(filenames_to_jsons)  # TODO: do not read all here\n",
        "\n",
        "  print_tags(contract)\n",
        "  validate_contracts()\n",
        " \n",
        "  return contract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX2kddnPPtI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractAnlysingContext = ContractAnlysingContext(elmo_embedder)\n",
        "\n",
        "contract_filenames = [os.path.join(contracts_filename_prefix, fn) for fn in expected['_id']]\n",
        "contract_filenames=contract_filenames[:limit_number_of_files]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n56iHHhE6FX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# raise 'stop if not sure'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUASEb5sMxJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_parse_contracts:\n",
        "  for_each_document(contract_filenames, accept=lambda doc: 'CONTRACT' == doc['documentType'], procedure=process_contract,\n",
        "                    _out=_out )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJQac-Z5FqMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = to_csv(filenames_to_jsons)\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML(df.to_html(index=False)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-cATHm_btEt",
        "colab_type": "text"
      },
      "source": [
        "## üëå Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdL0Wu-SCZ5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_parse_contracts:\n",
        "  validate_contracts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aZ4HQRstCLH",
        "colab_type": "text"
      },
      "source": [
        "### Verify random json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIHEi_QqtGS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if do_parse_contracts:\n",
        "  # test_json_file = os.path.join(_out,filenames[3]).split('/')[-1]+'.json'\n",
        "  # test_json_file = '/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/JSONs/contracts-19.10.17-4/1-! –î–æ–≥–æ–≤–æ—Ä _1.docx.json'\n",
        "  # test_json_file = os.path.join(_out, '1-–î–î_6.docx.json')\n",
        "  # test_json_file = os.path.join(_out, '1-–î–æ–≥–æ–≤–æ—Ä 15.doc.json')\n",
        "\n",
        "# load_and_show_json('/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/JSONs/charters-19.11.19-1/7. –ü—Ä–æ—Ç–æ–∫–æ–ª –∑–∞–æ—á–Ω–æ–≥–æ –°–î –ì–ü–ù-–†–ü –ë–ì.docx--1.json')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAm0hQK3T1sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvsD3meg-Qrx",
        "colab_type": "text"
      },
      "source": [
        "# üß† Protocols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npTLGAaf-QCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' )\n",
        "\n",
        "from analyser.protocol_parser import   ProtocolDocument, ProtocolParser, find_protocol_org\n",
        "from analyser.hyperparams import HyperParameters\n",
        "\n",
        "\n",
        "def process_protocol(doc: LegalDocument) -> LegalDocument:\n",
        "   \n",
        "  protocol_analyser.find_org_date_number(doc)\n",
        "  protocol_analyser.find_attributes(doc)  \n",
        "  print('print_tags---------')\n",
        "  print_tags(doc)\n",
        "  # print_headers(doc)\n",
        "\n",
        "  # print('find_protocol_org---------')\n",
        "  # print(find_protocol_org(doc))\n",
        "  return doc\n",
        "\n",
        "protocols_filename_prefix = '/content/gdrive/My Drive/GazpromOil/Protocols'\n",
        "protocols_filenames =  WordDocParser().list_filenames(protocols_filename_prefix)\n",
        "protocols_filenames = protocols_filenames[:limit_number_of_files]\n",
        "\n",
        "if do_parse_protocols:\n",
        "  protocol_analyser = ProtocolParser(elmo_embedder, elmo_embedder_default)\n",
        "  \n",
        "  # protocols_filenames = ['/content/gdrive/My Drive/GazpromOil/Protocols/7. –ü—Ä–æ—Ç–æ–∫–æ–ª –∑–∞–æ—á–Ω–æ–≥–æ –°–î –ì–ü–ù-–†–ü –ë–ì.docx']\n",
        "  for_each_document(protocols_filenames, accept=lambda doc: 'PROTOCOL' == doc['documentType'], procedure=process_protocol,\n",
        "                    _out=_out, save_json=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_T-yqkhT2yo",
        "colab_type": "text"
      },
      "source": [
        "# Process contracts zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS03zEe2UVOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Params\n",
        " \n",
        "\n",
        "zip_file_name='contract-protocol-pairs'#@param {type:\"string\"}\n",
        " \n",
        "out_dir=f'{zip_file_name}-out'\n",
        "if do_parse_zip:\n",
        "  \n",
        "  zipfn=f'/content/{zip_file_name}.zip'\n",
        "  # zipfn = '/content/contract-protocol-pairs.zip'\n",
        "  !unzip $zipfn -d $zip_file_name\n",
        "  !mkdir $out_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jDICSdRVmOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        " \n",
        "# zippp = 'contract-protocol-pairs'\n",
        "filenames = [file for file in glob.glob( f\"/content/{zip_file_name}/**/*.doc*\", recursive=True)] \n",
        "filenames[22:24]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNG1ya6hZXOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from analyser.protocol_parser import   ProtocolDocument, ProtocolParser\n",
        "from analyser.hyperparams import HyperParameters\n",
        "\n",
        "if do_parse_zip:\n",
        "\n",
        "  protocol_analyser = ProtocolParser(elmo_embedder, elmo_embedder_default)\n",
        "\n",
        "  ### TAKES TIME!!!\n",
        "  for_each_document(filenames, accept=lambda doc: 'PROTOCOL' == doc['documentType'], procedure=process_protocol,\n",
        "                    _out=out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08AEXStSU1N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_parse_zip:\n",
        "  load_and_show_json('contract-protocol-pairs-out/–ö—Ä—É–ø–Ω–∞—è —Å–¥–µ–ª–∫–∞(7)/1_–ü—Ä–æ—Ç–æ–∫–æ–ª_–¢—é–º–µ–Ω—å.doc--1.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaJ5OMrjU-Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_zip_contract(contract:LegalDocument) -> LegalDocument:  \n",
        "  contractAnlysingContext.analyze_contract_doc(contract)  \n",
        "  print_headers(contract)  \n",
        "  return contract\n",
        "\n",
        "### TAKES TIME!!!\n",
        "if do_parse_zip:\n",
        "  for_each_document( filenames, accept=lambda doc: 'CONTRACT' == doc['documentType'], procedure=process_zip_contract,\n",
        "                    _out=out_dir )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmAz_lJ1VMqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_parse_zip:\n",
        "  load_and_show_json('contract-protocol-pairs-out/–†–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä_–æ–±–µ–∑–ª–∏—á–µ–Ω (1)/–î–æ–≥–æ–≤–æ—Ä_–ì–ü–ù-–¢–µ—Ä–º–∏–Ω–∞–ª.docx--1.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMUz42spZ29X",
        "colab_type": "text"
      },
      "source": [
        "zip it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX1TpxHQZ2GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def zipfolder(foldername, target_dir):            \n",
        "  zipobj = zipfile.ZipFile(foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "  rootlen = len(target_dir) + 1\n",
        "  for base, dirs, files in os.walk(target_dir):\n",
        "    for file in files:\n",
        "      fn = os.path.join(base, file)\n",
        "      zipobj.write(fn, fn[rootlen:])\n",
        "\n",
        "\n",
        "if do_parse_zip:\n",
        "  zipfolder(f'{zip_file_name}-out-branch-{_git_branch}', out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WtVB6k47lC5J"
      },
      "source": [
        "# üß† Charters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YGEtLEHD5t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charters_filename_prefix = '/content/gdrive/My Drive/GazpromOil/Charters'\n",
        "WordDocParser().list_filenames(charters_filename_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmaq0CCBlU20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from analyser.charter_parser import CharterParser\n",
        "charter_parser = CharterParser(elmo_embedder, elmo_embedder_default)\n",
        "\n",
        "def process_charter(charter: LegalDocument) -> LegalDocument:\n",
        "  # protocol_analyser.analyse(doc)\n",
        "  print_headers(charter)\n",
        "  # charter_parser.ebmedd(charter)\n",
        "  # charter_parser.analyse(charter)\n",
        "\n",
        "  charter_parser.find_org_date_number(charter)\n",
        "  charter_parser.find_attributes(charter)  \n",
        "  print('print_tags---------')\n",
        "  print_tags(charter)\n",
        "\n",
        "  \n",
        "  return charter\n",
        "\n",
        "charters_filename_prefix = '/content/gdrive/My Drive/GazpromOil/Charters'\n",
        "charters_filenames =  WordDocParser().list_filenames(charters_filename_prefix)\n",
        "charters_filenames = charters_filenames[:limit_number_of_files]\n",
        "\n",
        "if do_parse_charters:\n",
        "  for_each_document(charters_filenames, accept=lambda doc: 'CHARTER' == doc['documentType'], procedure=process_charter,\n",
        "                  _out=_out, save_json=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47_chU_ao-3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  load_and_show_json('/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/JSONs/split-tf-phase-19.12.03-1/–ì–ü–ù –£—Å—Ç–∞–≤.docx--1.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlPN0TNVt7x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raise('not sure, not run')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ1hjJYlsX8Q",
        "colab_type": "text"
      },
      "source": [
        "# Picke"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WRYiXauA8pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _save_p(FN, doc):\n",
        "  with open(f'{FN}.pickle', 'wb') as handle:\n",
        "    pickle.dump(doc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujN7eDfzsZk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "\n",
        "def save_contract(d: LegalDocument) -> LegalDocument:  \n",
        "  # print(d.__dict__)\n",
        "  # print_headers(d)\n",
        "  # charter_parser.ebmedd(charter)\n",
        "  # charter_parser.analyse(charter)\n",
        "  contractAnlysingContext.analyze_contract_doc(d)  \n",
        "  \n",
        "  # d.embedd_tokens(elmo_embedder)\n",
        "  fn = d._id\n",
        "  print(fn)\n",
        "  _save_p(fn, d)\n",
        "  \n",
        "  return d\n",
        "\n",
        "# charters_filename_prefix = '/content/gdrive/My Drive/GazpromOil/Charters'\n",
        "# charters_filenames =  WordDocParser().list_filenames(charters_filename_prefix)\n",
        "p_filenames = [\n",
        "               '/content/gdrive/My Drive/GazpromOil/Contracts/–î–æ–≥–æ–≤–æ—Ä _2_.docx',\n",
        "               '/content/gdrive/My Drive/GazpromOil/Contracts/–î–æ–≥–æ–≤–æ—Ä 8.docx',\n",
        "               '/content/gdrive/My Drive/GazpromOil/Contracts/–î–æ–≥–æ–≤–æ—Ä 2.docx',\n",
        "               '/content/gdrive/My Drive/GazpromOil/Contracts/2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx'\n",
        "]\n",
        "\n",
        "\n",
        "# if do_parse_contracts:\n",
        "for_each_document(p_filenames, accept=lambda doc: 'CONTRACT' == doc['documentType'], procedure=save_contract,\n",
        "                _out=_out, save_json=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY9cJRXlDxb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from charter_parser import CharterParser\n",
        "charter_parser = CharterParser(elmo_embedder, elmo_embedder_default)\n",
        "\n",
        "def save_charter(d: LegalDocument) -> LegalDocument:  \n",
        "  \n",
        "  charter_parser.ebmedd(d)\n",
        "\n",
        "  fn = d._id\n",
        "  print('PICKLING ',fn)\n",
        "  _save_p(fn, d)\n",
        "  \n",
        "  return d\n",
        "  \n",
        "c_filenames = [\n",
        "               '/content/gdrive/My Drive/GazpromOil/Charters/–ì–ü–ù –£—Å—Ç–∞–≤.docx',\n",
        "               '/content/gdrive/My Drive/GazpromOil/Charters/–ï–Æ –£—Å—Ç–∞–≤.docx'\n",
        "]\n",
        "\n",
        "if do_parse_charters:\n",
        "  for_each_document(c_filenames, accept=lambda doc: 'CHARTER' == doc['documentType'], procedure=save_charter,\n",
        "                  _out=_out, save_json=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH6hD7FzE905",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wp = WordDocParser()\n",
        "FN='–ü—Ä–æ—Ç–æ–∫–æ–ª_–°–î_ 3.docx'\n",
        "FILENAME=f'/content/{FN}'\n",
        "res = wp.read_doc(FILENAME)\n",
        "doc = join_paragraphs(res['documents'][0], FILENAME)\n",
        "doc.embedd_tokens(elmo_embedder)\n",
        "print('embedded ok', doc.embeddings.shape)\n",
        "\n",
        "with open(f'{FN}.pickle', 'wb') as handle:\n",
        "  pickle.dump(doc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl9pVOyzCCi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ee=contractAnlysingContext.pattern_factory.embedder\n",
        "contractAnlysingContext.pattern_factory.embedder=None\n",
        "_save_p('contract_pattern_factory', contractAnlysingContext.pattern_factory)\n",
        "contractAnlysingContext.pattern_factory.embedder=_ee"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-19diMBJejNZ",
        "colab_type": "text"
      },
      "source": [
        "#Parse 2K sectioned contracts (Achtung, dis takes time)\n",
        "- upload zipped results of document-parser first "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxlAyKnbh4O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "!unzip /content/jsons_phase0.zip\n",
        "import glob\n",
        "filenames = [file for file in glob.glob( \"/content/jsons_phase0/**/*.json\", recursive=True)] \n",
        "\n",
        "!mkdir out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPkY14L71ieD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import csv, json\n",
        "from bson import json_util\n",
        "\n",
        "\n",
        "from contract_parser import ContractAnlysingContext\n",
        "from integration.word_document_parser import join_paragraphs\n",
        "\n",
        "\n",
        "def export_csv(rows, headline=['1', '2', '3', '4', '5', '6', '7', '8', '9']):\n",
        "  with open(f'contracts-stats.csv', mode='w') as csv_file:\n",
        "    _writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    _writer.writerow(headline)\n",
        "    for l in rows:\n",
        "      _writer.writerow(l)\n",
        "\n",
        "\n",
        "def _parse_contract(res, doc_id, row, ctx: ContractAnlysingContext):\n",
        "  contract = join_paragraphs(res, doc_id)\n",
        "  ctx.analyze_contract_doc(contract)\n",
        "\n",
        "  row[4:8] = [contract.tag_value('org.1.name'),\n",
        "              contract.tag_value('org.1.alias'),\n",
        "              contract.tag_value('org.2.name'),\n",
        "              contract.tag_value('org.2.alias')]\n",
        "  return contract\n",
        "\n",
        " \n",
        "rows=[]\n",
        "cnt=0\n",
        "\n",
        "for fn in filenames[200:]:\n",
        "  with open(fn, 'r') as file:\n",
        "    json_string = file.read()\n",
        "    res = json.loads(json_string, object_hook=json_util.object_hook)\n",
        "\n",
        "  if 'CONTRACT' == res[\"documentType\"]:\n",
        "    row = [cnt, '', None, None, None, None, None, None, fn, None]\n",
        "    row[2:4] = [res[\"documentType\"], res[\"documentDate\"]]\n",
        "    contract = _parse_contract(res, fn,  row, contractAnlysingContext)\n",
        "         \n",
        "    short_fn = fn.split('/')[-1]\n",
        "    with open(f'/content/out/{short_fn}.json', 'w') as file:\n",
        "      jjj = DocumentJson(contract)\n",
        "      file.write(jjj.dumps())\n",
        "\n",
        "\n",
        "    rows.append(row)\n",
        "    if cnt % 5 == 0: #save every 5 rows\n",
        "      export_csv(rows)\n",
        "\n",
        "  cnt+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoWC-mK1zwlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qln4vihrY9gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def zipfolder(foldername, target_dir):            \n",
        "  zipobj = zipfile.ZipFile(foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "  rootlen = len(target_dir) + 1\n",
        "  for base, dirs, files in os.walk(target_dir):\n",
        "    for file in files:\n",
        "      fn = os.path.join(base, file)\n",
        "      zipobj.write(fn, fn[rootlen:])\n",
        "\n",
        "\n",
        "\n",
        "zipfolder('out','/content/out')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}