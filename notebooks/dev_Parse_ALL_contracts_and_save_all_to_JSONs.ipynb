{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev: Parse ALL contracts and save all to JSONs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "al6xZ5y6FBU1",
        "T9CRJ19ZAPyG",
        "B8DgCjtPAqKT"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbsxFAqC6pjQ",
        "colab_type": "text"
      },
      "source": [
        "# Import code from gitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F89NLdN6A8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "–ù–∏—á—Ç–æ = None\n",
        "_git_branch = 'dev'\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ü¶ä GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('‚ù§Ô∏èimporting Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "#----\n",
        "import matplotlib as mpl\n",
        "from documents import TextMap\n",
        "from renderer import HtmlRenderer\n",
        "from legal_docs import DocumentJson\n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  for tag in cd.tags:\n",
        "    span = tag.span\n",
        "    _map = cd.tokenization_maps[tag.span_map]\n",
        "    print(tag)\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPLjOzkOATgk",
        "colab_type": "text"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCfuE2dw6wO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import unittest\n",
        "import warnings\n",
        "\n",
        "!pip install pyjarowinkler\n",
        "\n",
        "from contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from contract_patterns import ContractPatternFactory\n",
        "from documents import TextMap\n",
        "from legal_docs import LegalDocument\n",
        "from legal_docs import DocumentJson\n",
        "\n",
        "from ml_tools import SemanticTag, filter_values_by_key_prefix\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxRGxnjidln",
        "colab_type": "text"
      },
      "source": [
        "\n",
        " \n",
        "\n",
        "### Utilits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iceqc8qUjX8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_semantic_tag(tag: SemanticTag, map: TextMap):\n",
        "  print('-->', f'{tag.kind} \\t\\t [{tag.value}]\\t [{map.text_range(tag.span)}]')\n",
        "\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        " \n",
        "  for tag in cd.tags:\n",
        "    if tag.kind !='headline':\n",
        "     print_semantic_tag(tag, wordsmap)\n",
        " \n",
        "\n",
        "\n",
        "def json2html(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['words'])\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        " \n",
        "  for tag in cd.attributes:\n",
        "    if 'span' in  cd.attributes[tag]:\n",
        "      span = cd.attributes[tag]['span']\n",
        "      confidence = cd.attributes[tag]['confidence']\n",
        "      markup_vector[span[0]:span[1]] += confidence\n",
        "    else:\n",
        "      warnings.warn(f'{tag} has no span')\n",
        "\n",
        "  return renderer_.to_color_text(wordsmap.tokens, markup_vector, _range=(0, 1))\n",
        "  \n",
        " \n",
        "def load_and_show_json(filename):\n",
        "  with open(filename, 'r') as json_file:\n",
        "    data_str = json_file.read()\n",
        "    # print(str(data_str))\n",
        "    json_obj:DocumentJson = DocumentJson.from_json( data_str)\n",
        "    html = json2html(json_obj)\n",
        "    display(HTML(html))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHfhy1BhMNfm",
        "colab_type": "text"
      },
      "source": [
        "## Export all contracts to JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGbb8gDSTvuW",
        "colab_type": "text"
      },
      "source": [
        "### Mount gDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7_rZoyzMRzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpHstfXIOVqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from embedding_tools import ElmoEmbedder\n",
        "elmo_embedder = ElmoEmbedder(module_url = 'https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz')\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from contract_parser import ContractAnlysingContext\n",
        "contractAnlysingContext = ContractAnlysingContext(elmo_embedder, renderer_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMB4pkuu2wi",
        "colab_type": "text"
      },
      "source": [
        "### Init document-parser lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lib_version='1.1.3'\n",
        "\n",
        "!wget https://github.com/nemoware/document-parser/releases/download/$lib_version/document-parser-$lib_version-distribution.zip\n",
        "!unzip document-parser-$lib_version-distribution.zip\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ ['documentparser']=f'/content/document-parser-{lib_version}'\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "wp = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClO8-VAuT63y",
        "colab_type": "text"
      },
      "source": [
        "##### Parse all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107CPsZglZ7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attempt = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXnyIGsn68o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "attempt += 1\n",
        "dirname = '{0:%y.%m.%d}'.format(datetime.datetime.now())\n",
        "_out = f'/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/JSONs/contracts-{dirname}-{attempt}/'\n",
        "!mkdir '$_out'\n",
        "print(_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUASEb5sMxJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from legal_docs import DocumentJson\n",
        "\n",
        "contracts_filename_prefix = '/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "\n",
        "filenames = wp.list_filenames(contracts_filename_prefix)\n",
        "filenames = ['/content/gdrive/My Drive/GazpromOil/Contracts/3 —ç—Ç–∞–ø –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º/! –î–æ–≥–æ–≤–æ—Ä _1.docx']\n",
        "print(filenames)\n",
        "\n",
        "cnt = 0\n",
        "for fn in filenames:\n",
        "  cnt += 1\n",
        "  print(f'reading:\"{fn}\"')\n",
        "  short_fn = fn.split('/')[-1]\n",
        "\n",
        "  try:\n",
        "    # ------------------------\n",
        "    results = wp.read_doc(fn)\n",
        "    k=0\n",
        "    for doc in results['documents'][:1]:\n",
        "      k+=1\n",
        "      contract = join_paragraphs(doc, fn)\n",
        "      # ------------------------\n",
        "\n",
        "      contractAnlysingContext.analyze_contract_doc(contract)\n",
        "      contract = contractAnlysingContext.contract\n",
        "      \n",
        "      _path =  os.path.join(_out, f'{k}-{short_fn}.json')\n",
        "      with open(_path, 'w') as file:\n",
        "        # \n",
        "        jjj = DocumentJson(contract)\n",
        "        file.write(jjj.dumps())\n",
        "        print(f'saved file to {_path}')\n",
        "  except Exception as e:\n",
        "    print(e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aZ4HQRstCLH",
        "colab_type": "text"
      },
      "source": [
        "### Verify random json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIHEi_QqtGS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_json_file = os.path.join(_out,filenames[3]).split('/')[-1]+'.json'\n",
        "test_json_file = '/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/JSONs/contracts-19.10.17-4/1-! –î–æ–≥–æ–≤–æ—Ä _1.docx.json'\n",
        "load_and_show_json(test_json_file)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-19diMBJejNZ",
        "colab_type": "text"
      },
      "source": [
        "#Parse 2K sectioned contracts (Achtung, dis takes time)\n",
        "- upload zipped results of document-parser first "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxlAyKnbh4O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzNVR7Kee5mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/jsons_phase0.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1d3bop-fyqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "filenames = [file for file in glob.glob( \"/content/jsons_phase0/**/*.json\", recursive=True)] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUwOn_EstJeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPkY14L71ieD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import csv, json\n",
        "from bson import json_util\n",
        "\n",
        "\n",
        "from contract_parser import ContractAnlysingContext\n",
        "from integration.word_document_parser import join_paragraphs\n",
        "\n",
        "\n",
        "def export_csv(rows, headline=['1', '2', '3', '4', '5', '6', '7', '8', '9']):\n",
        "  with open(f'contracts-stats.csv', mode='w') as csv_file:\n",
        "    _writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    _writer.writerow(headline)\n",
        "    for l in rows:\n",
        "      _writer.writerow(l)\n",
        "\n",
        "\n",
        "def _parse_contract(res, doc_id, row, ctx: ContractAnlysingContext):\n",
        "  contract = join_paragraphs(res, doc_id)\n",
        "  ctx.analyze_contract_doc(contract)\n",
        "\n",
        "  row[4:8] = [contract.tag_value('org.1.name'),\n",
        "              contract.tag_value('org.1.alias'),\n",
        "              contract.tag_value('org.2.name'),\n",
        "              contract.tag_value('org.2.alias')]\n",
        "  return contract\n",
        "\n",
        " \n",
        "rows=[]\n",
        "cnt=0\n",
        "\n",
        "for fn in filenames[200:]:\n",
        "  with open(fn, 'r') as file:\n",
        "    json_string = file.read()\n",
        "    res = json.loads(json_string, object_hook=json_util.object_hook)\n",
        "\n",
        "  if 'CONTRACT' == res[\"documentType\"]:\n",
        "    row = [cnt, '', None, None, None, None, None, None, fn, None]\n",
        "    row[2:4] = [res[\"documentType\"], res[\"documentDate\"]]\n",
        "    contract = _parse_contract(res, fn,  row, contractAnlysingContext)\n",
        "         \n",
        "    short_fn = fn.split('/')[-1]\n",
        "    with open(f'/content/out/{short_fn}.json', 'w') as file:\n",
        "      jjj = DocumentJson(contract)\n",
        "      file.write(jjj.dumps())\n",
        "\n",
        "\n",
        "    rows.append(row)\n",
        "    if cnt % 5 == 0: #save every 5 rows\n",
        "      export_csv(rows)\n",
        "\n",
        "  cnt+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoWC-mK1zwlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qln4vihrY9gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def zipfolder(foldername, target_dir):            \n",
        "  zipobj = zipfile.ZipFile(foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "  rootlen = len(target_dir) + 1\n",
        "  for base, dirs, files in os.walk(target_dir):\n",
        "    for file in files:\n",
        "      fn = os.path.join(base, file)\n",
        "      zipobj.write(fn, fn[rootlen:])\n",
        "\n",
        "\n",
        "\n",
        "zipfolder('out','/content/out')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}