{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search patterns right in TF (solo session).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "nwpPPXqRQs6-"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/tensorflow-model/notebooks/Search_patterns_right_in_TF_(solo_session).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL5p2qEg9aST",
        "colab_type": "text"
      },
      "source": [
        "# A model üíé for finding closest embedding to a pattern\n",
        "1. At first stage, we calculate attention vectors (AV) for every pattern\n",
        "2. Then, for each AV we're finding the closest point (`best point`) in text embedding space, \n",
        "3. Then we're calculating \"improved\" vectors -- we calculate distance from the `\"best point\"` (actually, from the best *window*)  to each point in text embedding space.  \n",
        "  \n",
        "WARNING: this should be used only if `best point` is close enough to the pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "source": [
        "## MAIN, init, load code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"300px\", display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"tensorflow-model\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import HtmlRenderer\n",
        "  from renderer import to_multicolor_text\n",
        "  from renderer import known_subjects_dict\n",
        "\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  from ml_tools import ProbableValue\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "     \n",
        "     \n",
        "\n",
        "     \n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  # AZ:----------PROTOCOLS RENDERER-------------------------\n",
        "\n",
        "   \n",
        "\n",
        " \n",
        "\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "  \n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXXX\n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX\n",
        "\n",
        "\"\"\"# step 0. –ò–Ω–∏—Ç\"\"\"\n",
        "\n",
        "## do preparation here\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        " \n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SVjJmsiFMWI",
        "colab_type": "text"
      },
      "source": [
        "#  Finalize TF graph for embedding üíé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWnYlGa8IVns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from fuzzy_matcher import AttentionVectors, prepare_patters_for_embedding\n",
        "from patterns import FuzzyPattern\n",
        "from text_tools import Tokens, hot_punkt\n",
        "\n",
        "\n",
        "def make_punkt_mask(tokens: Tokens) -> np.ndarray:\n",
        "  return 1.0 - 0.999 * hot_punkt(tokens)\n",
        "\n",
        "\n",
        "class PatternSearchModel:\n",
        "\n",
        "\n",
        "  def __init__(self, tf, hub,\n",
        "               module_url='https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz'):\n",
        "    self.tf = tf\n",
        "    self.hub = hub\n",
        "    self.module_url = module_url\n",
        "\n",
        "    # cosine_similarities, cosine_similarities_improved\n",
        "    self.embedding_session = None\n",
        "    self.make_embedding_session_and_graph()\n",
        "\n",
        "  def _center_weighted(self, x, weights):\n",
        "    # todo reduce sum or reduce mean? Does not matter because we normalize it\n",
        "    tf = self.tf\n",
        "\n",
        "    _weighted = tf.multiply(x, tf.expand_dims(weights, -1, name=\"expanded\"), name='weighted')\n",
        "    _weighted_sum = self.tf.reduce_sum(_weighted, axis=0, name='weighted_sum')\n",
        "\n",
        "    _total_weight = self.tf.reduce_sum(weights, axis=0, name='total_weight')\n",
        "    return _weighted_sum / _total_weight\n",
        "\n",
        "  def _center(self, x):\n",
        "    return self.tf.reduce_mean(x, axis=0)\n",
        "\n",
        "  def _pad_tensor(self, tensor, padding, el, name):\n",
        "    tf = self.tf\n",
        "    _mask_padding = tf.tile(el, padding, name='pad_' + name)\n",
        "    return tf.concat([tensor, _mask_padding], axis=0, name='pad_tensor_' + name)\n",
        "\n",
        "  def _build_graph(self) -> None:\n",
        "    tf = self.tf  # hack for PyCharm because i don't want to download TF, it is provided by CoLab from UI\n",
        "\n",
        "    # BUILD IT -----------------------------------------------------------------\n",
        "    elmo = self.hub.Module(self.module_url, trainable=False)\n",
        "\n",
        "    # inputs:--------------------------------------------------------------------\n",
        "    self.text_input = tf.placeholder(dtype=tf.string, name=\"text_input\")\n",
        "    self.text_lengths = tf.placeholder(dtype=tf.int32, name='text_lengths')\n",
        "\n",
        "    self.mask = tf.placeholder(dtype=tf.float32, name='p_weights')\n",
        "\n",
        "    self.pattern_input = tf.placeholder(dtype=tf.string, name='pattern_input')\n",
        "    self.pattern_lengths = tf.placeholder(dtype=tf.int32, name='pattern_lengths')\n",
        "    self.pattern_slices = tf.placeholder(dtype=tf.int32, name='pattern_slices', shape=[None, 2])\n",
        "    # ------------------------------------------------------------------- /inputs\n",
        "\n",
        "    patterns_max_len = tf.math.reduce_max(self.pattern_lengths, keepdims=True, name='patterns_max_len')\n",
        "\n",
        "    number_of_patterns = tf.shape(self.pattern_input)[0]\n",
        "\n",
        "    self.mask_ext = self._pad_tensor(self.mask, patterns_max_len, tf.constant([0.001]), name='weights_padded')\n",
        "    text_input_ext = [\n",
        "      self._pad_tensor(self.text_input[0], patterns_max_len, tf.constant(['\\n']), name='text_input_ext')]\n",
        "\n",
        "    # 1. text embedding---------------\n",
        "    # TODO: try to deal with segmented text (this is about trailing index - [0] )\n",
        "    _text_embedding = self._embed(elmo,\n",
        "                                  text_input_ext,\n",
        "                                  [self.text_lengths[0] + patterns_max_len[0]])[0]\n",
        "\n",
        "    # 2. patterns embedding\n",
        "    _patterns_embeddings = self._embed(elmo, self.pattern_input, self.pattern_lengths)\n",
        "\n",
        "    # for looping\n",
        "    text_range = tf.range(self.text_lengths[0], dtype=tf.int32, name='text_input_range')\n",
        "    patterns_range = tf.range(number_of_patterns, dtype=tf.int32, name='patterns_range')\n",
        "\n",
        "    self.cosine_similarities = tf.map_fn(\n",
        "      lambda i: self.for_every_pattern((self.pattern_lengths[i], self.pattern_slices[i], _patterns_embeddings[i]),\n",
        "                                       _text_embedding, self.mask_ext,\n",
        "                                       text_range), patterns_range, dtype=tf.float32, name='cosine_similarities')\n",
        "\n",
        "    def improve_dist(attention_vector, pattern_len):\n",
        "      \"\"\"\n",
        "        finding closest point (aka 'best point')\n",
        "      \"\"\"\n",
        "      max_i = tf.math.argmax(attention_vector, output_type=tf.dtypes.int32)\n",
        "      best_embedding_range = _text_embedding[max_i:max_i + pattern_len]  # metapattern\n",
        "\n",
        "      return self._convolve(text_range, _text_embedding, weights=self.mask_ext, pattern_emb_sliced=best_embedding_range,\n",
        "                            name='improving')\n",
        "\n",
        "    def find_best_embeddings():\n",
        "      return tf.map_fn(\n",
        "        lambda pattern_i: improve_dist(self.cosine_similarities[pattern_i], self.pattern_lengths[pattern_i]),\n",
        "        patterns_range, dtype=tf.float32, name=\"find_best_embeddings\")\n",
        "\n",
        "    self.cosine_similarities_improved = find_best_embeddings()\n",
        "\n",
        "    unpadded_text_embedding_ = self._embed(elmo,\n",
        "                                           self.text_input,\n",
        "                                           [self.text_lengths[0]])[0]\n",
        "    text_center = self._center_weighted(unpadded_text_embedding_, self.mask)\n",
        "    self.distances_to_center = tf.map_fn(\n",
        "      lambda i: self.get_matrix_vector_similarity(unpadded_text_embedding_[i:i + 1], self.mask[i:i + 1],\n",
        "                                                  text_center),\n",
        "      text_range, dtype=tf.float32)\n",
        "    word_text_center_i = tf.math.argmax(self.distances_to_center, output_type=tf.dtypes.int32)\n",
        "    word_text_center = unpadded_text_embedding_[word_text_center_i]\n",
        "    self.distances_to_local_center = tf.map_fn(\n",
        "      lambda i: self.get_matrix_vector_similarity(unpadded_text_embedding_[i:i + 1], self.mask[i:i + 1],\n",
        "                                                  word_text_center), text_range, dtype=tf.float32)\n",
        "\n",
        "  @staticmethod\n",
        "  def _embed(elmo, text_input_p, lengths_p):\n",
        "    # 1. text embedding\n",
        "    return elmo(\n",
        "      inputs={\n",
        "        \"tokens\": text_input_p,\n",
        "        \"sequence_len\": lengths_p\n",
        "      },\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)[\"elmo\"]\n",
        "\n",
        "  def _normalize(self, x):\n",
        "    #       _norm = tf.norm(x, keep_dims=True)\n",
        "    #       return x/ (_norm + 1e-8)\n",
        "    return self.tf.nn.l2_normalize(x, 0)  # TODO: try different norm\n",
        "\n",
        "  def get_vector_similarity(self, a, b):\n",
        "    a_norm = self._normalize(a)  # normalizing is kinda required if we want cosine return [0..1] range\n",
        "    b_norm = self._normalize(b)  # DO WE? TODO: try different norm\n",
        "    return 1.0 - self.tf.losses.cosine_distance(a_norm, b_norm, axis=0)  # TODO: how on Earth Cosine could be > 1????\n",
        "\n",
        "  def get_matrix_vector_similarity(self, matrix, column_weights, vector):\n",
        "    m_center = self._center_weighted(matrix, column_weights)\n",
        "    return self.get_vector_similarity(vector, m_center)\n",
        "\n",
        "  def for_every_pattern(self, pattern_info, _text_embedding, weights, text_range):\n",
        "    pattern_slice = pattern_info[1]\n",
        "    _pattern_slice = slice(pattern_slice[0], pattern_slice[1])\n",
        "    pattern_emb_sliced = pattern_info[2][_pattern_slice]\n",
        "\n",
        "    return self._convolve(text_range, _text_embedding, weights, pattern_emb_sliced, name='p_match')\n",
        "\n",
        "  def _convolve(self, text_range, _text_embedding, weights, pattern_emb_sliced, name=''):\n",
        "    tf = self.tf\n",
        "\n",
        "    window_size = tf.shape(pattern_emb_sliced)[0]\n",
        "\n",
        "    p_center = self._center(pattern_emb_sliced)\n",
        "\n",
        "    _blurry = tf.map_fn(\n",
        "      lambda i: self.get_matrix_vector_similarity(matrix=_text_embedding[i:i + window_size],\n",
        "                                                  column_weights=weights[i:i + window_size], vector=p_center),\n",
        "      text_range, dtype=tf.float32, name=name + '_sim_wnd')\n",
        "\n",
        "    _sharp = tf.map_fn(\n",
        "      lambda i: self.get_matrix_vector_similarity(matrix=_text_embedding[i:i + 1], column_weights=weights[i:i + 1],\n",
        "                                                  vector=p_center),\n",
        "      text_range, dtype=tf.float32, name=name + '_sim_w1')\n",
        "\n",
        "    return tf.math.maximum(_blurry, _sharp, name=name + '_merge')\n",
        "\n",
        "  def make_embedding_session_and_graph(self):\n",
        "    tf = self.tf  # hack for PyCharm because I don't want to download TF, it is provided by CoLab from UI\n",
        "    embedding_graph = tf.Graph()\n",
        "\n",
        "    with embedding_graph.as_default():\n",
        "      self._build_graph()\n",
        "\n",
        "      init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "\n",
        "      self.embedding_session = tf.Session(graph=embedding_graph)\n",
        "      self.embedding_session.run(init_op)\n",
        "\n",
        "    embedding_graph.finalize()\n",
        "\n",
        "  # ------\n",
        "  def get_distances_to_center(self, text_tokens: Tokens):\n",
        "    runz = [self.distances_to_center, self.distances_to_local_center]\n",
        "    d, dl = self.embedding_session.run(runz, feed_dict={\n",
        "      self.text_input: [text_tokens],  # text_input\n",
        "      self.text_lengths: [len(text_tokens)],  # text_lengths\n",
        "      self.mask: make_punkt_mask(text_tokens),\n",
        "\n",
        "      self.pattern_input: [['a']],\n",
        "      self.pattern_lengths: [1],\n",
        "      self.pattern_slices: [[0, 1]]\n",
        "\n",
        "    })\n",
        "\n",
        "    return d, dl\n",
        "\n",
        "  # ------\n",
        "  def find_patterns(self, text_tokens: Tokens, patterns: List[FuzzyPattern]) -> AttentionVectors:\n",
        "    patterns_tokens, patterns_lengths, pattern_slices, _ = prepare_patters_for_embedding(patterns)\n",
        "\n",
        "    runz = [self.cosine_similarities]\n",
        "\n",
        "    attentions = self.embedding_session.run(runz, feed_dict={\n",
        "      self.text_input: [text_tokens],  # text_input\n",
        "      self.text_lengths: [len(text_tokens)],  # text_lengths\n",
        "      self.mask: make_punkt_mask(text_tokens),\n",
        "\n",
        "      self.pattern_input: patterns_tokens,\n",
        "      self.pattern_lengths: patterns_lengths,\n",
        "      self.pattern_slices: pattern_slices\n",
        "\n",
        "    })[0]\n",
        "\n",
        "    av = AttentionVectors()\n",
        "\n",
        "    for i in range(len(patterns)):\n",
        "      pattern = patterns[i]\n",
        "      av.add(pattern.name, attentions[i])\n",
        "\n",
        "    return av\n",
        "  \n",
        "  def find_patterns_and_improved(self, text_tokens: Tokens, patterns: List[FuzzyPattern]) -> AttentionVectors:\n",
        "    patterns_tokens, patterns_lengths, pattern_slices, _ = prepare_patters_for_embedding(patterns)\n",
        "\n",
        "    runz = [self.cosine_similarities, self.cosine_similarities_improved]\n",
        "\n",
        "    attentions, improved_attentions = self.embedding_session.run(runz, feed_dict={\n",
        "      self.text_input: [text_tokens],  # text_input\n",
        "      self.text_lengths: [len(text_tokens)],  # text_lengths\n",
        "      self.mask: make_punkt_mask(text_tokens),\n",
        "\n",
        "      self.pattern_input: patterns_tokens,\n",
        "      self.pattern_lengths: patterns_lengths,\n",
        "      self.pattern_slices: pattern_slices\n",
        "\n",
        "    })\n",
        "\n",
        "    av = AttentionVectors()\n",
        "\n",
        "    for i in range(len(patterns)):\n",
        "      pattern = patterns[i]\n",
        "      av.add(pattern.name, attentions[i], improved_attentions[i])\n",
        "\n",
        "    return av\n",
        "\n",
        "  \n",
        "#----------------------\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "PM = PatternSearchModel(tf, hub)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeXJiOdC5YHB",
        "colab_type": "text"
      },
      "source": [
        "## Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDSb0Wrp5P8f",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        " \n",
        "want_upload = False #@param {type:\"boolean\"}\n",
        "search_for = \"\\u0437\\u0430\\u043B\\u0435 \\u0437\\u0430\" #@param {type:\"string\"}\n",
        "from patterns import AbstractPatternFactory, FuzzyPattern\n",
        "\n",
        "from text_tools import tokenize_text\n",
        "import re\n",
        "from text_normalize import normalize_text, replacements_regex\n",
        "uploaded = \"\"\"\n",
        "–ë–µ—Ä–ª–∏–Ω—Å–∫–∞—è –¥–∞–¥–∞-—è—Ä–º–∞—Ä–∫–∞ –∏ –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –≤—ã—Å—Ç–∞–≤–∫–∞ —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞ –≤ –ø–∞—Ä–∏–∂¬≠—Å–∫–æ–π –≥–∞–ª–µ—Ä–µ–µ ¬´–ò–∑—è—â–Ω—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞¬ª –≤ 1938 –≥–æ–¥—É —Å—Ç–∞–ª–∏ –≤—ã—Å—à–∏–º–∏ —Ç–æ—á–∫–∞–º–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –¥–≤—É—Ö –¥–≤–∏–∂–µ–Ω–∏–π –∏ –ø–æ–¥–≤–µ–ª–∏ –∏–º –∏—Ç–æ–≥. –ù–∞ ¬´–°—é—Ä—Ä–µ–∞–ª–∏—Å¬≠—Ç–∏—á–µ—Å–∫–æ–π —É–ª–∏—Ü–µ¬ª, –∑–∞ –º–∞–Ω–µ–∫–µ–Ω–∞–º–∏, –≤—ã—Å—Ç—Ä–æ–∏–≤—à–∏–º–∏—Å—è –≤ –ø—Ä–æ—Ö–æ–¥–µ –≤ –≥–ª–∞–≤–Ω—ã–π –∑–∞–ª, —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª–∏—Å—å –ø–ª–∞–∫–∞—Ç—ã, –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è, –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, –æ—Ç—Å—ã–ª–∞—é—â–∏–µ –∫ —Ä–∞–Ω–Ω–∏–º —ç—Ç–∞–ø–∞–º —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º–∞. –í –≥–ª–∞–≤–Ω–æ–º –∑–∞–ª–µ, –∑–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ—Ç–≤–µ—á–∞–ª –ú–∞—Ä—Å–µ–ª—å –î—é—à–∞–Ω‚Äâ\n",
        " \n",
        ", –∞ –∑–∞ –æ—Å–≤–µ—â–µ¬≠–Ω–∏–µ ‚Äî –ú–∞–Ω –†—ç–π‚Äâ\n",
        " \n",
        ", –∫–∞—Ä—Ç–∏–Ω—ã 1920-—Ö –≥–æ–¥–æ–≤ –≤–∏—Å–µ–ª–∏ —Ä—è–¥–æ–º —Å –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–ª–æ —Ä–∞–∑–≤–∏—Ç–∏\n",
        "–µ —Å—é—Ä—Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ ¬´–∏–Ω—Ç–µ—Ä–Ω–∞—Ü–∏¬≠–æ–Ω–∞¬≠–ª–∞¬ª. –ó–∞—Ä–æ–¥–∏–≤—à–∏—Å—å –∫–∞–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ —Ç–µ—á–µ–Ω–∏–µ, –∫ –∫–æ–Ω—Ü—É 1930-—Ö –≥–æ–¥–æ–≤ —Å—é—Ä—Ä–µ–∞–ª–∏–∑–º —É–∂–µ –æ–∫–æ–ª–æ 15 –ª–µ—Ç –≥–æ—Å–ø–æ–¥—Å—Ç–≤–æ–≤–∞–ª –≤ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∞–≤–∞–Ω–≥–∞—Ä–¥–µ –ü–∞—Ä–∏–∂–∞. –ü—Ä–µ–∂–¥–µ —á–µ–º –ø–æ–π—Ç–∏ –Ω–∞ —Å–ø–∞–¥ —Å –Ω–∞—á–∞–ª–æ–º\n",
        "–í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –æ–Ω —Å—Ç–∞–ª —á–∞—Å—Ç—å—é —Å–≤–µ—Ç—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã –ü–∞—Ä–∏–∂–∞ –∏ –¥–∞–∂–µ –¥–æ –Ω–µ–∫–æ—Ç–æ–†–æ–π\n",
        "—Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–∏—Å—è–≥–Ω—É–ª –≤—ã—Å–æ–∫–æ–π –º–æ–¥–µ, –ø–æ–¥–æ–±–Ω–æ —Ç–æ–º—É –∫–∞–∫ —Ä—É—Å—Å–∫–∏–π –∞–≤–∞–Ω–≥–∞—Ä–¥ ‚Äî –ø—É—Å—Ç—å —Å–æ–≤—Å–µ–º –∏–Ω–∞—á–µ ‚Äî –ø—Ä–∏—Å—è–≥–Ω—É \n",
        "–≤ —Å–≤–æ–µ –≤—Ä–µ–º—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏. –ò–∑—è—â–µ—Å—Ç–≤–æ —Å—Ç–∏–ª—è, —Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—é—Ä—Ä–µ–ª–∏–∑–º—É, —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–∞–ª–æ —ç—Ç–æ–º—É \n",
        "\n",
        "¬≠ —Å–±–ª–∏–∂–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä–æ–µ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, —É–ø—Ä–æ—á–∏–ª–æ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –æ–±—â–µ—Å—Ç–≤–µ. \n",
        "\n",
        "–û–¥–Ω–∞–∫–æ –ø–æ–Ω–∞—á–∞–ª—É –¥–ª—è –ª–∏—Ç–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ —Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤-–±—É–Ω—Ç–∞—Ä–µ–π, –Ω–∏—á—É—Ç—å –Ω–µ —Å—Ç—Ä–µ–º–∏–≤—à–∏—Ö—Å—è –∫ —Å–æ—Ü–∏–∞–ª—å¬≠–Ω–æ–º—É —É—Å–ø–µ—Ö—É, \n",
        "–±—ã–ª–∞ –∫—É–¥–∞ –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å–≤—è–∑—å —Å –¥–∞–¥–∞–∏–∑–º–æ–º\"\"\"\n",
        "\n",
        "#UNCOMMENT TO UPLOAD============================================================\n",
        "\n",
        " \n",
        "if want_upload:\n",
        "  uploaded = interactive_upload('Protocol')[0]\n",
        "# \n",
        "_regex_addon = [\n",
        "    (re.compile(r'[¬≠]'), '-'),\n",
        "]\n",
        "TOKENS=tokenize_text( normalize_text(uploaded, replacements_regex+_regex_addon))\n",
        "\n",
        "\n",
        "\n",
        "class PF(AbstractPatternFactory):\n",
        "  def __init__(self):\n",
        "    AbstractPatternFactory.__init__(self, None)\n",
        "    self._build_ner_patterns()\n",
        "\n",
        "  def _build_ner_patterns(self):\n",
        "    def cp(name, tuples):\n",
        "      return self.create_pattern(name, tuples)\n",
        "   \n",
        "    cp('_custom', ( '', search_for, ''))\n",
        "    \n",
        "\n",
        "# ---\n",
        "pf = PF()\n",
        "\n",
        "av = PM.find_patterns(text_tokens=TOKENS, patterns=pf.patterns)\n",
        "\n",
        "\n",
        " \n",
        "fig = plt.figure(figsize=(20, 6))\n",
        "ax = plt.axes()\n",
        "for name in av.vectors:\n",
        "  ax.plot(av.get_best(name, relu_th=0.8), label='v:'+name, alpha=0.5)\n",
        "ax.plot([0.5]*av.size, label='0.5', alpha=0.2)\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text(TOKENS, av.get_by_name('_custom'), _range=(0,1) )\n",
        "# TOKENS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMx9RtS0vJK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "av = PM.find_patterns(text_tokens=TOKENS, patterns=pf.patterns)\n",
        "GLOBALS__['renderer'].render_color_text(TOKENS, av.get_by_name('_custom'), _range=(0,1) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Z6PWQ9vQcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "av = PM.find_patterns_and_improved(text_tokens=TOKENS, patterns=pf.patterns)\n",
        "GLOBALS__['renderer'].render_color_text(TOKENS, av.get_by_name('_custom'), _range=(0,1) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbWXFyHGcBSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distances_to_center, distances_to_local_center = PM.get_distances_to_center(TOKENS)\n",
        "\n",
        "GLOBALS__['renderer'].render_color_text(TOKENS, distances_to_center, _range=(0,1) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLvnLtACx3A2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ml_tools import *\n",
        "v = distances_to_center\n",
        "v=relu(v, 0.5)\n",
        "GLOBALS__['renderer'].render_color_text(TOKENS, v, _range=(0,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fkF_LWlxxY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v2=relu(distances_to_local_center, 0.4)\n",
        "GLOBALS__['renderer'].render_color_text(TOKENS, v2  )\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}