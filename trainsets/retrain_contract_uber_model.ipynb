{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jT7451NXspdw",
    "outputId": "a6202399-15a0-4ebf-b4a7-18f4f954488e"
   },
   "outputs": [],
   "source": [
    "\n",
    "import analyser.hyperparams\n",
    "from analyser.hyperparams import work_dir \n",
    "from analyser.hyperparams import HyperParameters\n",
    " \n",
    "\n",
    "work_dir_default = os.path.realpath(os.path.join(  analyser.hyperparams.__file__, '..', '..', '..', 'work'))\n",
    "work_dir = os.environ.get('GPN_WORK_DIR', work_dir_default)\n",
    "print('work_dir=', work_dir)\n",
    "if not os.path.isdir(work_dir):\n",
    "  os.mkdir(work_dir)\n",
    "\n",
    "analyser.hyperparams.work_dir = work_dir\n",
    "\n",
    "assert os.path.isdir(analyser.hyperparams.work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbsxFAqC6pjQ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pexvDJbAtZM0",
    "outputId": "6e2078b9-15b8-424e-9549-92400cedbcaa"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from colab_support.renderer import *\n",
    "# from tensorflow_docs import plots\n",
    "\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from analyser.legal_docs import LegalDocument, make_headline_attention_vector\n",
    "from analyser.headers_detector import make_predicted_headline_attention_vector\n",
    "import math\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from trainsets.trainset_tools import TrainsetBalancer, SubjectTrainsetManager\n",
    " \n",
    "\n",
    "from trainsets.retrain_contract_uber_model import DbJsonDoc, UberModelTrainsetManager\n",
    "from tf_support.super_contract_model import get_base_model\n",
    "\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import SVG\n",
    "from keras.models import load_model\n",
    "import warnings\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv1D, LSTM, GRU, BatchNormalization, TimeDistributed, Dense, Bidirectional, Input, Dropout, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate, SpatialDropout1D, ActivityRegularization\n",
    "from keras.layers import MaxPooling1D, Activation, ThresholdedReLU, GaussianNoise\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tf_support.tools import KerasTrainingContext\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRQOy7o0uyTv"
   },
   "source": [
    "# Prepare trainset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "gIWUsIvPv2ia",
    "outputId": "96e72de0-68d9-42db-cadc-9614b983047a"
   },
   "outputs": [],
   "source": [
    "umtm = UberModelTrainsetManager ( analyser.hyperparams.work_dir)\n",
    "\n",
    "umtm.import_recent_contracts()\n",
    "umtm.calculate_samples_weights()\n",
    "umtm.validate_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "8pe_gZIK3JFh",
    "outputId": "1cad12f4-bdb4-4521-a7f0-9702051ddf39"
   },
   "outputs": [],
   "source": [
    "umtm.stats = umtm.stats[  pd.isna(umtm.stats.value_span) + (umtm.stats.value_span < 10000) ] #remove big docs from TS\n",
    "umtm.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "ouUjO_T7xf8A",
    "outputId": "967c3f7a-8b73-413d-f978-14eb54c575f9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "subj_count = umtm.stats['subject'].value_counts()\n",
    "\n",
    "#plot distribution---------------------\n",
    "sns.barplot(subj_count.values, subj_count.index)\n",
    "plt.title('Frequency Distribution of subjects')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print ('\\nmin', min (subj_count.values))\n",
    "print ('max', max (subj_count.values))\n",
    "print ('total', sum (subj_count.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "Ld_KZaHwqf_G",
    "outputId": "3b6189d9-b0b8-4801-c989-61b434a2ab33"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from trainsets.trainset_tools import get_feature_log_weights\n",
    "\n",
    "_classes = umtm.stats['subject'].unique().tolist()\n",
    "\n",
    "print(f'classes: {_classes}')\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                 _classes,\n",
    "#                                                 umtm.stats['subject'])\n",
    "# class_weights = dict(zip(_classes, class_weights))\n",
    "# class_weights\n",
    "\n",
    "class_weights = get_feature_log_weights(umtm.stats, 'subject')\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "Cs6cZtPy6Je1",
    "outputId": "cfb25da4-6a0c-49bf-ca00-7f645d8c568f"
   },
   "outputs": [],
   "source": [
    "from trainsets.trainset_tools import get_feature_log_weights\n",
    "\n",
    "def calculate_samples_weights(self):\n",
    "\n",
    "  self.stats: DataFrame = self.load_contract_trainset_meta()\n",
    "  subject_weights = get_feature_log_weights(self.stats, 'subject')\n",
    "  \n",
    "  value_median = self.stats.value_log1p.median()\n",
    "\n",
    "  for i, row in self.stats.iterrows():\n",
    "    subj_name = row['subject']\n",
    "\n",
    "    tagging_weight = 1.0\n",
    "    if not pd.isna(row['user_correction_date']):  # MORE weight for user-corrected datapoints\n",
    "      tagging_weight = 10.0  # TODO: must be estimated anyhow smartly\n",
    "\n",
    "    value_weight = value_median\n",
    "    if not pd.isna(row['value_log1p']):\n",
    "      # Ð²ÐµÑ Ð¿Ñ€Ð¾Ð¿Ð¾Ñ€Ñ†Ð¸Ð¾Ð½Ð°Ð»ÐµÐ½ Ð»Ð¾Ð³Ð¾Ñ€Ð¸Ñ„Ð¼Ñƒ Ñ†ÐµÐ½Ñ‹ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÐºÑ‚Ð°,\n",
    "      # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð±Ñ‹Ð»Ð¾ Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð² ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÐºÑ‚Ð°Ñ… Ð½Ð° Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÑƒÐ¼Ð¼Ñ‹)\n",
    "      value_weight = row['value_log1p']\n",
    "\n",
    "    tagging_weight *= value_weight\n",
    "    subject_weight = tagging_weight * class_weights[subj_name]\n",
    "    self.stats.at[i, 'subject_weight'] = subject_weight\n",
    "    self.stats.at[i, 'sample_weight']  = tagging_weight\n",
    "\n",
    "  # normalize weights, so the sum == Number of samples\n",
    "  self.stats.sample_weight /= self.stats.sample_weight.mean()\n",
    "  self.stats.subject_weight /= self.stats.subject_weight.mean()\n",
    "\n",
    "  self._save_stats()\n",
    "\n",
    "calculate_samples_weights(umtm)\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "umtm.stats['subject_weight'].hist(bins=20)\n",
    "umtm.stats['sample_weight'].hist(bins=20)\n",
    "\n",
    "plt.xscale('linear') # log?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "_DjNv8UQ956S",
    "outputId": "643d6a42-97eb-49c3-bd9b-b7c8b011764a"
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"subject_weight\", y=\"sample_weight\", data=umtm.stats )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxEdSGOuq62R"
   },
   "source": [
    "### look into trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CRb_AJUliUft",
    "outputId": "4b476279-b454-4824-a535-ec31d93b098a"
   },
   "outputs": [],
   "source": [
    "# umtm.calculate_samples_weights()\n",
    "SAMPLE_DOC_ID = umtm.stats.index[0]\n",
    "print('SAMPLE_DOC_ID', SAMPLE_DOC_ID)\n",
    "dp = umtm.make_xyw(SAMPLE_DOC_ID)\n",
    "(emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_kJ0L2RnrHl3",
    "outputId": "13173384-0253-41be-cf3a-1416e29a5a26"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_embedding(tok_f[:500], title=f'Tokens features {SAMPLE_DOC_ID}') \n",
    "plot_embedding(emb[:500], title=f'Embedding {SAMPLE_DOC_ID}') \n",
    "plot_embedding(sm[:500], title=f'Semantic map {SAMPLE_DOC_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niaa4g6g2Q7g"
   },
   "source": [
    "## Batch generator & TODOs ðŸ™\n",
    "\n",
    "\n",
    "- [X] TODO: add outliers to the trainset ?\n",
    "- [ ] TODO: try sparse_categorical_entropy instead of one-hot encodings\n",
    "- [ ] TODO: model 5.2, 5.1: bipolar concat layer is wrong because we concatenate thongs of different magnitudes. Add a Sigmoid activation layer\n",
    "- [ ] TODO: chechk what is better: to pad with zeros or to pad with means\n",
    "- [X] TODO: add weights to samples\n",
    "- [ ] TODO: sum semantic map alongside vertical axis, and mutiply it (as a mask) by the subject detection seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CFzuOP4w9mB"
   },
   "outputs": [],
   "source": [
    "def make_generator(self, indices: [int], batch_size: int, augment_samples=False):\n",
    "\n",
    "  np.random.seed(42)\n",
    "\n",
    "  while True:\n",
    "    # next batch\n",
    "    batch_indices = np.random.choice(a=indices, size=batch_size)\n",
    "\n",
    "    max_len = 128 * 12\n",
    "    start_from = 0\n",
    "\n",
    "    if augment_samples:\n",
    "      max_len =  random.randint(300, 1400)\n",
    "\n",
    "    batch_input_emb = []\n",
    "    batch_input_token_f = []\n",
    "    batch_output_sm = []\n",
    "    batch_output_subj = []\n",
    "\n",
    "    weights = []\n",
    "    weights_subj = []\n",
    "\n",
    "    # Read in each input, perform preprocessing and get labels\n",
    "    for doc_id in batch_indices:\n",
    "\n",
    "      dp = self.make_xyw(doc_id)\n",
    "      (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "\n",
    "      subject_weight_K=1.0\n",
    "      if augment_samples:\n",
    "        start_from = 0\n",
    "        \n",
    "        row = self.stats.loc[doc_id]\n",
    "        if random.randint(1, 2) == 1:  # 50% of samples\n",
    "          segment_center = random.randint(0, len(emb)-1) ##select random token as a center\n",
    "          if not pd.isna(row['value_span']) and random.random()<0.7:        \n",
    "            segment_center = int(row['value_span'])\n",
    "\n",
    "          _off = random.randint(max_len // 4, max_len // 2)\n",
    "          start_from = segment_center - _off\n",
    "          if start_from < 0:\n",
    "            start_from = 0\n",
    "          subject_weight_K = 0.1 #lower subject weight because there mighÐµ be no information about subject around doc. value\n",
    "\n",
    "      dp = self.trim_maxlen(dp, start_from, max_len)\n",
    "      # TODO: find samples maxlen\n",
    "\n",
    "      (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "      subject_weight *= subject_weight_K\n",
    "\n",
    "      batch_input_emb.append(emb)\n",
    "      batch_input_token_f.append(tok_f)\n",
    "\n",
    "      batch_output_sm.append(sm)\n",
    "      batch_output_subj.append(subj)\n",
    "\n",
    "      weights.append(sample_weight)\n",
    "      weights_subj.append(subject_weight)\n",
    "      # end if emb\n",
    "    # end for loop\n",
    "\n",
    "    # Return a tuple of (input, output, weights) to feed the network\n",
    "    yield ([np.array(batch_input_emb), np.array(batch_input_token_f)],\n",
    "            [np.array(batch_output_sm), np.array(batch_output_subj)],\n",
    "            [np.array(weights), np.array(weights_subj)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zrKMJ2bsn6yx",
    "outputId": "806267f9-eda5-4b6f-ceac-a805f4966b61"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 24\n",
    "EMB =  1024\n",
    " \n",
    "_SELFTEST = True\n",
    "\n",
    "\n",
    "\n",
    "_train, _test = train_test_split(umtm.stats, test_size=0.2, stratify=umtm.stats[['subject']])\n",
    "train_indices = _train.index\n",
    "test_indices = _test.index\n",
    "\n",
    "# train_indices, test_indices = split_trainset_evenly(umtm.stats, 'subject', seed=5)\n",
    "print('train_indices[0]:', train_indices[0])\n",
    "print('test_indices[0]:', test_indices[0])\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "def plot_subject_distr(df, title):  \n",
    "  target='subject'\n",
    "  plt.figure(figsize=(16,4))   \n",
    "  sns.set(style=\"whitegrid\")\n",
    "  chart = sns.countplot(data=df, y=target)\n",
    "  plt.title(f'Frequency Distribution of subjects :{title}')\n",
    "\n",
    " \n",
    "plot_subject_distr(umtm.stats, 'ALL')\n",
    "plot_subject_distr(umtm.stats[umtm.stats.index.isin(train_indices)], 'train')\n",
    "plot_subject_distr(umtm.stats[umtm.stats.index.isin(test_indices)], 'test')\n",
    "\n",
    "\n",
    "if _SELFTEST:\n",
    "  # test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True)\n",
    "  \n",
    "  x, y, w = next(train_gen)\n",
    "  \n",
    "  print('X:', len(x), 'X[0]=', x[0].shape, 'X[1]=', x[1].shape)\n",
    "  print('Y:', len(y), 'Y[0]=', y[0].shape, 'Y[1]=', y[1].shape)\n",
    "  \n",
    "\n",
    "  plot_embedding(x[0][0], 'X2: Token Embeddings')\n",
    "  plot_embedding(x[1][0], 'X1: Token Features')\n",
    "  plot_embedding(y[0][0], 'Y: Semantic Map')\n",
    "  \n",
    "  print(y[0][1])\n",
    "\n",
    "  # del x 5edbc665da3678279fbcaf1c\n",
    "  del y\n",
    "  del train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "RIRaSgxCP3Jt",
    "outputId": "53cdf54b-ae85-44ae-8aa6-52daaf913280"
   },
   "outputs": [],
   "source": [
    "\n",
    "ctx = KerasTrainingContext(umtm.work_dir, session_index=21)\n",
    "\n",
    "ctx.set_batch_size_and_trainset_size(BATCH_SIZE, \n",
    "                                     len(test_indices), \n",
    "                                     4 * len(train_indices))\n",
    "\n",
    "DEFAULT_TRAIN_CTX = ctx\n",
    "CLASSES = 43\n",
    "FEATURES = 14\n",
    "\n",
    "metrics = ['kullback_leibler_divergence', 'mse', 'binary_crossentropy']\n",
    "\n",
    "\n",
    "def make_all_generators(): \n",
    "  all_val_generator = make_generator(umtm, test_indices + train_indices, BATCH_SIZE)\n",
    "  test_generator = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "  train_generator = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True)\n",
    "\n",
    "  return train_generator, test_generator, all_val_generator\n",
    "\n",
    "def train(umodel):\n",
    "  test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True) \n",
    "  ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen)\n",
    "\n",
    "def overtrain(umodel):\n",
    "  test_gen = make_generator(umtm, train_indices+test_indices, BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, train_indices+test_indices, BATCH_SIZE, augment_samples=True) \n",
    "  ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAFmo0sG4H9k"
   },
   "source": [
    "# Models ðŸ¦–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eF7Ktdfo9aa7"
   },
   "source": [
    "### 5.1.1 ðŸ’•ðŸ’• uber_detection_model_005_1_1\n",
    "```\n",
    "0.0590: val_O1_tagging_kullback_leibler_divergence\n",
    "0.0765: val_O1_tagging_kullback_leibler_divergence: \n",
    "0.0019: val_O1_tagging_mse\n",
    "0.0315: val_O2_subject_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "OpgUW39m9abE",
    "outputId": "a08fb487-de9d-4973-bc4d-79257cfed0a0"
   },
   "outputs": [],
   "source": [
    "from tf_support.super_contract_model import uber_detection_model_005_1_1\n",
    "umodel = ctx.init_model( uber_detection_model_005_1_1, verbose=2, trained=True  )\n",
    "# umodel = ctx.init_model( uber_detection_model_005_1_1, verbose=2, trained=True, weights_file_override='/content/uber_detection_model_005_1_1' )\n",
    "\n",
    "# plot_model(umodel, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "LJAH6Fjn9abL",
    "outputId": "4133f67d-b00d-4028-fdfa-a34940e520d5"
   },
   "outputs": [],
   "source": [
    "ctx.EPOCHS = 21\n",
    "ctx.EVALUATE_ONLY = False\n",
    "#TODO: freeze bottom layers\n",
    "train(umodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owsx_kTcExnV"
   },
   "outputs": [],
   "source": [
    "# ctx.EPOCHS = 300  \n",
    "# ctx.EVALUATE_ONLY=False\n",
    "# overtrain(umodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUum89Tdhg-9"
   },
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLRR4qmKImgQ"
   },
   "source": [
    "### training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "me1LdIP5Ik9z",
    "outputId": "9962fe7b-7cc0-4c51-c322-25f1a4b03775"
   },
   "outputs": [],
   "source": [
    " \n",
    "def plot_compare_models(models, metrics, title=\"metric/epoch\"):\n",
    "  colorstep = float(1.0 / len(models))\n",
    "  power = 3\n",
    "  fig = plt.figure(figsize=(16, 6))\n",
    "  ax = fig.gca()\n",
    "  # ax.set_facecolor((0, 0, 0.1))\n",
    "\n",
    "  for i, m in enumerate(models):\n",
    "    data = ctx.get_log(m)\n",
    "    if data is not None:\n",
    "      data.set_index('epoch')\n",
    "      for metric in metrics:\n",
    "\n",
    "        key = \"val_\" + metric\n",
    "        if key in data:\n",
    "          x = data['epoch'][2:]\n",
    "          y = data[key][2:]\n",
    "          c = plt.cm.Dark2(i * colorstep)\n",
    "          plt.plot(y, label=f'{m} {key}', alpha=0.2, color=c)\n",
    "          y = y.rolling(4, win_type='gaussian').mean(std=4)\n",
    "\n",
    "          plt.plot(y, label=f'{m} {key}', color=c)\n",
    "    else:\n",
    "      print('cannot plot')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "  plt.title(title)\n",
    "  plt.grid()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "models = list(ctx.trained_models.keys())\n",
    "# print(models)\n",
    "\n",
    "plot_compare_models(models, ['loss'], 'Loss')\n",
    "\n",
    "plot_compare_models(models, ['O1_tagging_kullback_leibler_divergence'], 'TAGS: Kullback Leibler divergence')\n",
    "plot_compare_models(models, ['O1_tagging_mse'], 'TAGS: MSE')\n",
    "plot_compare_models(models, ['O2_subject_kullback_leibler_divergence'], 'Subj: Kullback Leibler divergence')\n",
    "plot_compare_models(models, ['O2_subject_mse'],  'Subjects: MSE')\n",
    "\n",
    "plot_compare_models(models, ['O1_tagging_loss', 'O2_subject_loss'], 'Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZMV42nrJYw3"
   },
   "source": [
    "### Evaluate recent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CYIADs5IBBd"
   },
   "outputs": [],
   "source": [
    "train_generator, test_generator, all_val_generator = make_all_generators()\n",
    "print(umodel.name)\n",
    "subsets=['all', 'test', 'train']\n",
    "ev = pd.DataFrame()#(columns=umodel.metrics_names)\n",
    "for i, _gen in enumerate([all_val_generator, test_generator, train_generator]):\n",
    "  evaluation = umodel.evaluate_generator(_gen, verbose=2, steps=16)\n",
    "  for a, b in zip(umodel.metrics_names, evaluation):\n",
    "    ev.at[a, umodel.name+f\"--{subsets[i]}\"] = b\n",
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1p8Mqpi32Bf"
   },
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YJLgtkpo-NUY",
    "outputId": "e6b632b8-9446-4a7c-ab31-e1b32f90d4ec"
   },
   "outputs": [],
   "source": [
    "from tf_support.tf_subject_model import decode_subj_prediction\n",
    "\n",
    "\n",
    "def plot_cm(y_true, y_pred, figsize=(12, 12), title=None):\n",
    "  cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "  cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "  cm_perc = cm / cm_sum.astype(float) * 100\n",
    "  annot = np.empty_like(cm).astype(str)\n",
    "  nrows, ncols = cm.shape\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      c = cm[i, j]\n",
    "      p = cm_perc[i, j]\n",
    "      if i == j:\n",
    "        s = cm_sum[i]\n",
    "        annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "      elif c == 0:\n",
    "        annot[i, j] = ''\n",
    "      else:\n",
    "        annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "\n",
    "      # cm[i, j] = cm_perc[i, j]\n",
    "\n",
    "  cm = pd.DataFrame(cm_perc, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "  cm.index.name = 'Actual'\n",
    "  cm.columns.name = 'Predicted'\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  sns.heatmap(cm, cmap=\"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    "  plt.title(title)\n",
    "\n",
    "\n",
    "def report_confusion_matrix(umodel, indices):\n",
    "  errors_report = pd.DataFrame()\n",
    "  errors_report['expected'] = ''\n",
    "  errors_report['predicted'] = ''\n",
    "\n",
    "  all_expected = []\n",
    "  all_predicted = []\n",
    "\n",
    "  for _id in indices:\n",
    "\n",
    "    x, y, _ = umtm.make_xyw(_id)\n",
    "    embeddings = x[0]\n",
    "    token_features = x[1]\n",
    "    prediction = umodel.predict(x=[np.expand_dims(embeddings, axis=0), np.expand_dims(token_features, axis=0)],\n",
    "                                batch_size=1)\n",
    "\n",
    "    subj_1hot = prediction[1][0]\n",
    "\n",
    "    expected = decode_subj_prediction(y[1])[0]\n",
    "    predicted = decode_subj_prediction(subj_1hot)[0]\n",
    "    all_expected.append(expected.name)\n",
    "    all_predicted.append(predicted.name)\n",
    "\n",
    "    if expected != predicted:\n",
    "      errors_report.at[_id, 'expected'] = expected\n",
    "      errors_report.at[_id, 'predicted'] = predicted\n",
    "\n",
    "      \n",
    "\n",
    "  plot_cm(all_expected, all_predicted, title=umodel.name)\n",
    "\n",
    "  report = classification_report(all_expected, all_predicted, digits=3)\n",
    "  print(umodel.name)\n",
    "  print(report)\n",
    "\n",
    "  return errors_report\n",
    " \n",
    "\n",
    "# subset = umtm.stats[~pd.isna(umtm.stats['user_correction_date'])].sort_values('analyze_date')[:50] \n",
    "subset = umtm.stats[~pd.isna(umtm.stats['user_correction_date'])].sort_values('analyze_date')\n",
    "errors_report = report_confusion_matrix(umodel, subset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "tZhJlOE8CZwn",
    "outputId": "d6c169b0-cc7f-4887-a3e1-7a8f50a965dd"
   },
   "outputs": [],
   "source": [
    "print(len(errors_report), 'wrong subjects of', len(subset))\n",
    "errors_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CR9W8NDH8B-"
   },
   "source": [
    "## Single doc eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from analyser.finalizer import get_doc_by_id\n",
    "\n",
    "# a = get_doc_by_id('ObjectId(5dee80604ddc27bcf92dd88e)')\n",
    "# print(a)\n",
    "\n",
    "from integration.db import get_mongodb_connection\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "SAMPLE_DOC_ID = umtm.stats.index[0]\n",
    "print('SAMPLE_DOC_ID', SAMPLE_DOC_ID)\n",
    "dp = umtm.make_xyw(SAMPLE_DOC_ID)\n",
    "(emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "\n",
    "\n",
    "print(f'fetching {SAMPLE_DOC_ID}')\n",
    "db = get_mongodb_connection()\n",
    "documents_collection = db['documents']\n",
    "# print(documents_collection)\n",
    "jdata =  documents_collection.find_one({'_id': ObjectId(SAMPLE_DOC_ID)})\n",
    "jdoc = DbJsonDoc(jdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "TfHQOFhw3yuO",
    "outputId": "47ea61ea-c770-47dd-92d9-c5cb077b8c28"
   },
   "outputs": [],
   "source": [
    "from integration.word_document_parser import join_paragraphs\n",
    "\n",
    "\n",
    "\n",
    "def asLegalDoc(self):\n",
    "  doc: LegalDocument = join_paragraphs(self.parse, self._id)\n",
    "  return doc\n",
    "\n",
    "doc = asLegalDoc(jdoc) #???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "-yF9kgac_hZr",
    "outputId": "a5fe8e17-6c64-4dfe-eb23-c46bd798c62c"
   },
   "outputs": [],
   "source": [
    "from tf_support.super_contract_model import seq_labels_contract \n",
    "from tf_support.tf_subject_model import decode_subj_prediction\n",
    "\n",
    "###############\n",
    "prediction = umodel.predict(   x=[  np.expand_dims(emb, axis=0), np.expand_dims(tok_f, axis=0)] , batch_size=1)\n",
    "##############\n",
    "print(len(prediction), umodel.name)\n",
    "tagging = prediction[0][0]\n",
    "\n",
    "subj_1hot = prediction[1][0]\n",
    "print('Subject:', decode_subj_prediction(subj_1hot))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(tagging, columns=seq_labels_contract)\n",
    "\n",
    "plot_embedding(df, title = f'Predictions of {umodel.name}')\n",
    "# display(HTML(render_doc (doc, df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qBRtwOj5QvNf",
    "outputId": "a3fe8ab2-49d5-45d2-fd99-1fbc2f2f0175"
   },
   "outputs": [],
   "source": [
    "from analyser.text_tools import find_top_spans\n",
    "\n",
    "for t in seq_labels_contract:\n",
    "  spans = list( find_top_spans( df[t].values, threshold=0.3))  \n",
    "  print(t.upper(), spans)\n",
    "  display(HTML(render_slices(spans, doc, df[t].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BtKQtmZ48U1f",
    "outputId": "c3955868-4bd7-468a-d385-80242d9cef81"
   },
   "outputs": [],
   "source": [
    "mean_ = df.values.max(-1)*0.5\n",
    "print (mean_.shape)\n",
    "display(HTML( to_color_text (doc.tokens,  mean_)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF-UGHyh-C9",
    "7X_zYCYEdlPM",
    "lyI4hbTRFjyM"
   ],
   "name": "Local: structure keras uber model - clean train, evaluate, test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
