import numpy as np

from legal_docs import CharterDocument, HeadlineMeta, LegalDocument, \
  make_constraints_attention_vectors, extract_all_contraints_from_sentence, \
  deprecated, make_soft_attention_vector, org_types, embedd_generic_tokenized_sentences_2
from ml_tools import split_by_token
from parsing import ParsingContext, ParsingConfig
from patterns import AbstractPatternFactoryLowCase
from renderer import *
from text_tools import find_ner_end
from text_tools import untokenize
from transaction_values import extract_sum, ValueConstraint

default_charter_parsing_config:ParsingConfig=ParsingConfig()
default_charter_parsing_config.headline_attention_threshold = 1.4


class CharterAnlysingContext(ParsingContext):
  def __init__(self, embedder, renderer: AbstractRenderer):
    ParsingContext.__init__(self, embedder, renderer)

    self.factory: CharterPatternFactory = None

    self.renderer = renderer

    self.org = None
    self.constraints = None
    self.doc = None

    self.config = default_charter_parsing_config

  def analyze_charter(self, txt, verbose=False):
    """
    üöÄ
    :param txt:
    :param verbose:
    :return:
    """

    if self.factory is None:
      self.factory = CharterPatternFactory(self.embedder)

    self._reset_context()
    # 0. parse
    _charter_doc = CharterDocument(txt)
    _charter_doc.right_padding = 0

    # 1. find top level structure
    _charter_doc.parse()

    self.doc = _charter_doc

    # 2. embedd headlines
    embedded_headlines = _charter_doc.embedd_headlines(self.factory)
    self._logstep("embedding headlines into semantic space")

    _charter_doc.sections = _charter_doc.find_sections_by_headlines_2(
      self, self.factory.headlines, embedded_headlines, 'headline.', self.config.headline_attention_threshold)

    self._logstep("extracting doc structure")

    if 'name' in _charter_doc.sections:
      section: HeadlineMeta = _charter_doc.sections['name']
      org = self.detect_ners(section.body)
      self._logstep("extracting NERs (named entities)")
    else:
      self.warning('–°–µ–∫—Ü–∏—è –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∫–æ–º–ø–Ω–∞–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞')
      self.warning('–ü–æ–ø—ã—Ç–∞–µ–º—Å—è –∏—Å–∫–∞—Ç—å –ø—Ä–æ—Å—Ç–æ –≤ –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞')
      org = self.detect_ners(_charter_doc.subdoc(0, 3000))
      # org = {
      #   'type': 'org_unknown',
      #   'name': "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ",
      #   'type_name': "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ",
      #   'tokens': [],
      #   'attention_vector': []
      # }

    rz = self.find_contraints(_charter_doc.sections)
    self._logstep("Finding margin transaction values")

    #   html = render_constraint_values(rz)
    #   display(HTML(html))
    self.org = org
    self.constraints = rz

    self.verbosity_level = 1

    self.log_warnings()

    return org, rz

  # ------------------------------------------------------------------------------
  @deprecated
  def detect_ners(self, section):
    """
    XXX: TODO: üö∑üî• moved to charter_parser.py
    :param section:
    :return:
    """
    assert section is not None

    section.embedd(self.factory)

    section.calculate_distances_per_pattern(self.factory, pattern_prefix='org_', merge=True)
    section.calculate_distances_per_pattern(self.factory, pattern_prefix='ner_org', merge=True)
    section.calculate_distances_per_pattern(self.factory, pattern_prefix='nerneg_', merge=True)

    org_by_type_dict, org_type = self._detect_org_type_and_name(section)

    if self.verbosity_level > 1:
      self.renderer.render_color_text(section.tokens_cc, section.distances_per_pattern_dict[org_type],
                                      _range=[0, 1])

    start = org_by_type_dict[org_type][0]
    start = start + len(self.factory.patterns_dict[org_type].embeddings)
    end = 1 + find_ner_end(section.tokens, start)

    orgname_sub_section: LegalDocument = section.subdoc(start, end)
    org_name = orgname_sub_section.untokenize_cc()

    if self.verbosity_level > 1:
      self.renderer.render_color_text(orgname_sub_section.tokens_cc,
                                      orgname_sub_section.distances_per_pattern_dict[org_type],
                                      _range=[0, 1])
      print('Org type:', org_types[org_type], org_by_type_dict[org_type])

    rez = {
      'type': org_type,
      'name': org_name,
      'type_name': org_types[org_type],
      'tokens': section.tokens_cc,
      'attention_vector': section.distances_per_pattern_dict[org_type]
    }

    return rez

  # ------------------------------------------------------------------------------
  @deprecated
  def _detect_org_type_and_name(self, section):
    """
        XXX: TODO: üö∑üî• moved to charter_parser.py

    """
    s_attention_vector_neg = self.factory._build_org_type_attention_vector(section)

    org_by_type = {}
    best_org_type = None
    _max = 0
    for org_type in org_types.keys():

      vector = section.distances_per_pattern_dict[org_type] * s_attention_vector_neg
      if self.verbosity_level > 2:
        print('_detect_org_type_and_name, org_type=', org_type, section.distances_per_pattern_dict[org_type][0:10])

      idx = np.argmax(vector)
      val = section.distances_per_pattern_dict[org_type][idx]
      if val > _max:
        _max = val
        best_org_type = org_type

      org_by_type[org_type] = [idx, val]

    if self.verbosity_level > 2:
      print('_detect_org_type_and_name', org_by_type)

    return org_by_type, best_org_type

  # ---------------------------------------
  def find_contraints(self, sections):
    # 5. extract constraint values
    sections_filtered = {}
    prefix = 'head.'
    for k in sections:
      if k[:len(prefix)] == prefix:
        sections_filtered[k] = sections[k]

    rz = self.extract_constraint_values_from_sections(sections_filtered)
    return rz

  ##---------------------------------------
  def extract_constraint_values_from_sections(self, sections):
    rez = {}

    for head_type in sections:
      section = sections[head_type]
      rez[head_type] = self.extract_constraint_values_from_section(section)

    return rez

  ##---------------------------------------
  def extract_constraint_values_from_section(self, section: HeadlineMeta):

    if self.verbosity_level > 1:
      print('extract_constraint_values_from_section', section.type)

    body = section.body

    if self.verbosity_level > 1:
      print('extract_constraint_values_from_section', 'embedding....')

    sentenses_i = []
    senetences = split_by_token(body.tokens, '\n')
    for s in senetences:
      line = untokenize(s) + '\n'
      sum = extract_sum(line)
      if sum is not None:
        sentenses_i.append(line)
      if self.verbosity_level > 2:
        print('-', sum, line)

    hl_subdoc = section.subdoc

    r_by_head_type = {
      'section': head_types_dict[section.type],
      'caption': untokenize(hl_subdoc.tokens_cc),
      'sentences': self._extract_constraint_values_from_region(sentenses_i, self.factory)
    }
    self._logstep(f"Finding margin transaction values in section {untokenize(hl_subdoc.tokens_cc)}")
    return r_by_head_type

  ##---------------------------------------
  def _extract_constraint_values_from_region(self, sentenses_i, _embedd_factory):
    if sentenses_i is None or len(sentenses_i) == 0:
      return []

    ssubdocs = embedd_generic_tokenized_sentences_2(sentenses_i, _embedd_factory.embedder)

    for ssubdoc in ssubdocs:
      ssubdoc.calculate_distances_per_pattern(_embedd_factory, pattern_prefix='sum_max.', merge=True)
      ssubdoc.calculate_distances_per_pattern(_embedd_factory, pattern_prefix='sum__', merge=True)
      ssubdoc.calculate_distances_per_pattern(_embedd_factory, pattern_prefix='d_order.', merge=True)

      vectors = make_constraints_attention_vectors(ssubdoc)
      ssubdoc.distances_per_pattern_dict = {**ssubdoc.distances_per_pattern_dict, **vectors}

      if self.verbosity_level > 1:
        self.renderer.render_color_text(
          ssubdoc.tokens,
          ssubdoc.distances_per_pattern_dict['deal_value_attention_vector'], _range=(0, 1))

    sentences = []
    for sentence_subdoc in ssubdocs:
      constraints: List[ValueConstraint] = extract_all_contraints_from_sentence(sentence_subdoc,
                                                                                sentence_subdoc.distances_per_pattern_dict[
                                                                                  'deal_value_attention_vector'])

      sentence = {
        'quote': untokenize(sentence_subdoc.tokens_cc),
        'subdoc': sentence_subdoc,
        'constraints': constraints
      }

      sentences.append(sentence)
    return sentences

  # ==============
  # VIOLATIONS

  def find_ranges_by_group(self, charter_constraints, m_convert, verbose=False):
    ranges_by_group = {}
    for head_group in charter_constraints:
      #     print('-' * 20)
      group_c = charter_constraints[head_group]
      data = self._combine_constraints_in_group(group_c, m_convert, verbose)
      ranges_by_group[head_group] = data
    return ranges_by_group

  def _combine_constraints_in_group(self, group_c, m_convert, verbose=False):
    # print(group_c)
    # print(group_c['section'])

    data = {
      'name': group_c['section'],
      'ranges': {}
    }

    sentences = group_c['sentences']
    #   print (charter_constraints[head_group]['sentences'])
    sentence_id = 0
    for sentence in sentences:
      constraint_low = None
      constraint_up = None

      sentence_id += 1
      #     print (sentence['constraints'])

      s_constraints = sentence['constraints']
      # –±–æ–ª—å—à–∏–µ –∏—â–µ–º
      maximals = [x for x in s_constraints if x.value.sign > 0]

      if len(maximals) > 0:
        constraint_low = min(maximals, key=lambda item: m_convert(item.value).value)
        if verbose:
          print("all maximals:")
          self.renderer.render_values(maximals)
          print('\t\t\t constraint_low', constraint_low.value.value)
          self.renderer.render_values([constraint_low])

      minimals = [x for x in s_constraints if x.value.sign <= 0]
      if len(minimals) > 0:
        constraint_up = min(minimals, key=lambda item: m_convert(item.value).value)
        if verbose:
          print("all: minimals")
          self.renderer.render_values(minimals)
          print('\t\t\t constraint_upper', constraint_up.value.value)
          self.renderer.render_values([constraint_up])
          print("----X")

      if constraint_low is not None or constraint_up is not None:
        data['ranges'][sentence_id] = VConstraint(constraint_low, constraint_up, group_c)

    return data
  # ==================================================================VIOLATIONS


class CharterPatternFactory(AbstractPatternFactoryLowCase):
  """
  üè≠
  """

  def __init__(self, embedder):
    AbstractPatternFactoryLowCase.__init__(self, embedder)

    self._build_head_patterns()
    self._build_order_patterns()
    self._build_sum_margin_extraction_patterns()
    self._build_sum_patterns()

    self._build_ner_patterns()

    self.embedd()

    self.headlines = ['head.directors', 'head.all', 'head.gen', 'head.pravlenie', 'name']

  def _build_head_patterns(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    head_prfx = "—Å—Ç–∞—Ç—å—è 0"

    cp('headline.name.1', ('–ü–æ–ª–Ω–æ–µ', '—Ñ–∏—Ä–º–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–æ–±—â–µ—Å—Ç–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:'))
    cp('headline.name.2', ('', '–û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø', ''))
    cp('headline.name.3', ('', '—Ñ–∏—Ä–º–µ–Ω–Ω–æ–µ', ''))
    cp('headline.name.4', ('', '—Ä—É—Å—Å–∫–æ–º', ''))
    cp('headline.name.5', ('', '—è–∑—ã–∫–µ', ''))
    cp('headline.name.6', ('', '–ø–æ–ª–Ω–æ–µ', ''))

    cp('headline.head.all.1', (head_prfx, '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –æ–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤', ''))
    cp('headline.head.all.2', (head_prfx, '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –æ–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤', '–æ–±—â–µ—Å—Ç–≤–∞'))
    cp('headline.head.all.3', (head_prfx, '—Å–æ–±—Ä–∞–Ω–∏–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤\n', ''))

    cp('headline.head.all.4', ('', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏', ''))
    cp('headline.head.all.5', ('', '—Å–æ–±—Ä–∞–Ω–∏—è', ''))
    cp('headline.head.all.6', ('', '—É—á–∞—Å—Ç–Ω–∏–∫–æ–≤', ''))
    cp('headline.head.all.7', ('', '–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤', ''))

    cp('headline.head.directors.1', (head_prfx, '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è —Å–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤', '–æ–±—â–µ—Å—Ç–≤–∞'))
    cp('headline.head.directors.2', ('', '—Å–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –æ–±—â–µ—Å—Ç–≤–∞', ''))
    cp('headline.head.directors.3', ('', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏', ''))
    cp('headline.head.directors.4', ('', '—Å–æ–≤–µ—Ç–∞', ''))
    cp('headline.head.directors.5', ('', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤', ''))

    cp('headline.head.pravlenie.1', (head_prfx, '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø—Ä–∞–≤–ª–µ–Ω–∏—è', ''))
    cp('headline.head.pravlenie.2', ('', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏', ''))
    cp('headline.head.pravlenie.3', ('', '–ø—Ä–∞–≤–ª–µ–Ω–∏—è', ''))
    #     cp('d_head_pravlenie.2', ('', '–æ–±—â–µ—Å—Ç–≤–∞', ''))

    cp('headline.head.gen.1', (head_prfx, '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', ''))
    cp('headline.head.gen.2', ('', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏', ''))
    cp('headline.head.gen.3', ('', '–≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ', ''))
    cp('headline.head.gen.4', ('', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', ''))

  def _build_sum_patterns(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    suffix = '–º–ª–Ω. —Ç—ã—Å. –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π –¥–æ–ª–ª–∞—Ä–æ–≤ –∫–æ–ø–µ–µ–∫ –µ–≤—Ä–æ'
    prefix = '—Ä–µ—à–µ–Ω–∏–π –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫ '

    cp('sum_max1', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ –±–æ–ª–µ–µ 0', suffix))
    cp('sum_max2', (prefix + '—Ü–µ–Ω–∞', '–Ω–µ –±–æ–ª—å—à–µ 0', suffix))
    cp('sum_max3', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å <', '0', suffix))
    cp('sum_max4', (prefix + '—Ü–µ–Ω–∞ –º–µ–Ω–µ–µ', '0', suffix))
    cp('sum_max5', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å', '0', suffix))
    cp('sum_max6', (prefix + '–æ–±—â–∞—è —Å—É–º–º–∞ –º–æ–∂–µ—Ç —Å–æ—Å—Ç–∞–≤–∏—Ç—å', '0', suffix))
    cp('sum_max7', (prefix + '–ª–∏–º–∏—Ç —Å–æ–≥–ª–∞—à–µ–Ω–∏—è', '0', suffix))
    cp('sum_max8', (prefix + '–≤–µ—Ä—Ö–Ω–∏–π –ª–∏–º–∏—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏', '0', suffix))
    cp('sum_max9', (prefix + '–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞', '0', suffix))

  def _build_sum_margin_extraction_patterns(self):
    suffix = '–º–ª–Ω. —Ç—ã—Å. –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π –¥–æ–ª–ª–∞—Ä–æ–≤ –∫–æ–ø–µ–µ–∫ –µ–≤—Ä–æ'
    prefix = '—Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–æ–∫ '

    # less than
    self.create_pattern('sum__lt_1', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ –±–æ–ª–µ–µ 0', suffix))
    self.create_pattern('sum__lt_2', (prefix + '—Ü–µ–Ω–∞', '–Ω–µ –±–æ–ª—å—à–µ 0', suffix))
    self.create_pattern('sum__lt_3', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '< 0', suffix))
    self.create_pattern('sum__lt_4', (prefix + '—Ü–µ–Ω–∞', '–º–µ–Ω–µ–µ 0', suffix))
    self.create_pattern('sum__lt_4.1', (prefix + '—Ü–µ–Ω–∞', '–Ω–∏–∂–µ 0', suffix))
    self.create_pattern('sum__lt_5', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å 0', suffix))
    self.create_pattern('sum__lt_6', (prefix + '–ª–∏–º–∏—Ç —Å–æ–≥–ª–∞—à–µ–Ω–∏—è', '0', suffix))
    self.create_pattern('sum__lt_7', (prefix + '–≤–µ—Ä—Ö–Ω–∏–π –ª–∏–º–∏—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏', '0', suffix))
    self.create_pattern('sum__lt_8', (prefix, '–º–∞–∫—Å–∏–º—É–º 0', suffix))
    self.create_pattern('sum__lt_9', (prefix, '–¥–æ 0', suffix))
    self.create_pattern('sum__lt_10', (prefix, '–Ω–æ –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—â—É—é 0', suffix))
    self.create_pattern('sum__lt_11', (prefix, '—Å–æ–≤–æ–∫—É–ø–Ω–æ–µ –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 0', suffix))

    # greather than
    self.create_pattern('sum__gt_1', (prefix + '—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç', '–±–æ–ª–µ–µ 0', suffix))
    self.create_pattern('sum__gt_2', (prefix + '', '–ø—Ä–µ–≤—ã—à–∞–µ—Ç 0', suffix))
    self.create_pattern('sum__gt_3', (prefix + '', '—Å–≤—ã—à–µ 0', suffix))
    self.create_pattern('sum__gt_4', (prefix + '', '—Å–¥–µ–ª–∫–∞ –∏–º–µ–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Ä–∞–≤–Ω—É—é –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞—é—â—É—é 0', suffix))

  @deprecated
  def _build_order_patterns____OLD(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    cp('d_order_1', ('–ü–æ—Ä—è–¥–æ–∫', '–æ–¥–æ–±—Ä–µ–Ω–∏—è —Å–¥–µ–ª–æ–∫', '–≤ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –∏–º–µ–µ—Ç—Å—è –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å'))
    cp('d_order_2', ('', '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π', '–æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–æ–∫'))
    cp('d_order_3',
       ('', '–æ–¥–æ–±—Ä–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è', '–∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –∫–∞–∫–æ–π-–ª–∏–±–æ —Å–¥–µ–ª–∫–∏ –û–±—â–µ—Å—Ç–≤–∞'))
    cp('d_order_4', ('', '–°–¥–µ–ª–∫–∏', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Ä–∞–≤–Ω–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç'))
    cp('d_order_5', ('', '–°–¥–µ–ª–∫–∏', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–Ω–µ–µ'))

  def _build_order_patterns(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    prefix = '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ –∏–ª–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏–∏'

    cp('d_order_4', (prefix, 'c–¥–µ–ª–∫–∏', ', —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Ä–∞–≤–Ω–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç'))
    cp('d_order_5', (prefix, 'c–¥–µ–ª–∫–∏', ', —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–Ω–µ–µ'))

  def _build_ner_patterns(self):
    def cp(name, tuples):
      return self.create_pattern(name, tuples)

    for o_type in org_types.keys():
      cp(o_type, ('', org_types[o_type], '"'))

    cp('ner_org.1', ('–ü–æ–ª–Ω–æ–µ', '—Ñ–∏—Ä–º–µ–Ω–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–æ–±—â–µ—Å—Ç–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:'))

    cp('ner_org.6', ('', '–û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø', ''))

    cp('ner_org.2', ('', '—Ñ–∏—Ä–º–µ–Ω–Ω–æ–µ', ''))
    cp('ner_org.3', ('', '—Ä—É—Å—Å–∫–æ–º', ''))
    cp('ner_org.4', ('', '—è–∑—ã–∫–µ', ''))
    cp('ner_org.5', ('', '–ø–æ–ª–Ω–æ–µ', ''))

    cp('nerneg_1', ('–æ–±—â–µ—Å—Ç–≤–æ –∏–º–µ–µ—Ç', '–ø–µ—á–∞—Ç—å', ''))
    cp('nerneg_2', ('', '—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ', ''))
    cp('nerneg_3', ('–Ω–∞', '–∞–Ω–≥–ª–∏–π—Å–∫–æ–º', '—è–∑—ã–∫–µ'))

  def _build_org_type_attention_vector(self, subdoc: CharterDocument):
    attention_vector_neg = make_soft_attention_vector(subdoc, 'nerneg_1', blur=80)
    attention_vector_neg = 1 + (1 - attention_vector_neg)  # normalize(attention_vector_neg * -1)
    return attention_vector_neg


head_types_dict = {'head.directors': '–°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤',
                   'head.all': '–û–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤/–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤',
                   'head.gen': '–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä',
                   #                      'shareholders':'–û–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤',
                   'head.pravlenie': '–ü—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—â–µ—Å—Ç–≤–∞',
                   'head.unknown': '*–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ—Ä–≥–∞–Ω —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è*'}
head_types = ['head.directors', 'head.all', 'head.gen', 'head.pravlenie']

# =======================
# =======================
# =======================
from ml_tools import ProbableValue


class VConstraint:
  def __init__(self, lower, upper, head_group):
    self.lower = ProbableValue(ValueConstraint(0, 'RUB', +1), 0)
    self.upper = ProbableValue(ValueConstraint(np.inf, 'RUB', -1), 0)

    if lower is not None:
      self.lower = lower

    if upper is not None:
      self.upper = upper

    self.head_group = head_group

  def maybe_convert(self, v: ValueConstraint, convet_m):
    html = ""
    v_converted = v
    if v.currency != 'RUB':
      v_converted = convet_m(v)
      html += as_warning(f"–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–∞–ª—é—Ç {as_currency(v)} --> RUB ")
      html += as_offset(as_warning(f"–ø—Ä–∏–º–µ—Ä–Ω–æ: {as_currency(v)} ~~  {as_currency(v_converted)}  "))
    return v, v_converted, html

  def check_contract_value(self, _v: ProbableValue, convet_m, renderer):
    greather_lower = False
    greather_upper = False

    if _v is None:
      return as_error_html("—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞")
    v: ValueConstraint = _v.value

    if v is None:
      return as_error_html("—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ –≤–µ—Ä–Ω–∞")

    if v.value is None:
      return as_error_html(f"—Å—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ –Ω–µ –≤–µ—Ä–Ω–∞ {v.currency}")
    ###----

    lower_v = None
    upper_v = None
    if self.lower is not None:
      lower_v: ValueConstraint = self.lower.value
    if self.upper is not None:
      upper_v: ValueConstraint = self.upper.value

    html = as_msg(f"–¥–∏–∞–ø–∞–∑–æ–Ω: {as_currency(lower_v)} < ..... < {as_currency(upper_v)}")

    v, v_converted, h = self.maybe_convert(v, convet_m)
    html += h

    if self.lower is not None:
      lower_v: ValueConstraint = self.lower.value
      lower_v, lower_converted, h = self.maybe_convert(lower_v, convet_m)
      html += h

      if v_converted.value >= lower_converted.value:
        greather_lower = True
        html += as_warning("—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ...".upper())
        html += as_warning(
          f"—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞  {as_currency(v_converted)}  –ë–û–õ–¨–®–ï –Ω–∏–∂–Ω–µ–π –ø–æ—Ä–æ–≥–æ–≤–æ–π {as_currency(lower_converted)} ")
        html += as_quote(untokenize(lower_v.context[0]))

    if self.upper is not None:

      upper_v: ValueConstraint = self.upper.value
      upper_v, upper_converted, h = self.maybe_convert(upper_v, convet_m)
      html += h

      if v_converted.value >= upper_converted.value:

        html += as_error_html(
          f"—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞  {as_currency(v_converted)} –ë–û–õ–¨–®–ï –≤–µ—Ä—Ö–Ω–µ–π –ø–æ—Ä–æ–≥–æ–≤–æ–π {as_currency(upper_converted)} ")

      elif greather_lower:
        head_name = self.head_group['section']
        html += as_error_html(f'—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã "{head_name.upper()}"')

        if lower_v.context is not None:
          html += as_quote(renderer.to_color_text(lower_v.context[0], lower_v.context[1], _range=[0, 1]))
        if upper_v.context is not None:
          html += '<br>'
          html += as_quote(renderer.to_color_text(upper_v.context[0], upper_v.context[1], _range=[0, 1]))

    return html

# -----------


# rendering:----------------------------
