import re
from typing import Iterator

from contract_agents import find_org_names, ORG_LEVELS_re, find_org_names_in_tag
from contract_parser import find_value_sign_currency_attention
from hyperparams import HyperParameters
from legal_docs import LegalDocument, tokenize_doc_into_sentences_map, ContractValue
from ml_tools import *
from parsing import ParsingContext
from patterns import *
from structures import ORG_LEVELS_names
from text_normalize import r_group, ru_cap, r_quoted
from text_tools import is_long_enough, span_len
from tf_support.embedder_elmo import ElmoEmbedder

something = r'(\s*.{1,100}\s*)'
itog1 = r_group(r_group(ru_cap('–∏—Ç–æ–≥–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è') + '|' + ru_cap('—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è')) + r"[:\n]?")

za = r_group(r_quoted('–∑–∞'))
pr = r_group(r_quoted('–ø—Ä–æ—Ç–∏–≤') + something)
vo = r_group(r_quoted('–≤–æ–∑–¥–µ—Ä–∂–∞–ª—Å—è') + something)

protocol_votes_ = r_group(itog1 + something) + r_group(za + something + pr + something + vo)
protocol_votes_re = re.compile(protocol_votes_, re.IGNORECASE | re.UNICODE)


class ProtocolDocument4(LegalDocument):

  def __init__(self, doc: LegalDocument):
    super().__init__('')
    if doc is not None:
      self.__dict__ = doc.__dict__

    self.sentence_map: TextMap = None
    self.sentences_embeddings = None

    self.distances_per_sentence_pattern_dict = {}

    self.agents_tags: [SemanticTag] = []
    self.org_level: [SemanticTag] = []
    self.agenda_questions: [SemanticTag] = []
    self.margin_values: [ContractValue] = []

  def get_tags(self) -> [SemanticTag]:
    tags = []
    tags += self.agents_tags
    tags += self.org_level
    tags += self.agenda_questions
    for mv in self.margin_values:
      tags += mv.as_list()

    return tags


ProtocolDocument = ProtocolDocument4  # aliasing


class ProtocolParser(ParsingContext):
  patterns_dict = [
    ['sum_max1', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ –±–æ–ª–µ–µ 0 –º–ª–Ω. —Ç—ã—Å. –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π –¥–æ–ª–ª–∞—Ä–æ–≤ –∫–æ–ø–µ–µ–∫ –µ–≤—Ä–æ'],

    # ['solution_1','—Ä–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],
    # ['solution_2','–ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –ø—Ä–∏–Ω—è—Ç—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è:'],

    ['not_value_1', '—Ä–∞–∑–º–µ—Ä —É—Å—Ç–∞–≤–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ 0 —Ä—É–±–ª–µ–π'],
    ['not_value_2', '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–∏ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è'],

    ['agenda_end_1', '–∫–≤–æ—Ä—É–º –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∑–∞—Å–µ–¥–∞–Ω–∏—è –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏–º–µ–µ—Ç—Å—è'],
    ['agenda_end_2', '–í–æ–ø—Ä–æ—Å –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],
    ['agenda_end_3', '–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ —Ä–µ—à–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è:'],

    ['agenda_start_1', '–ø–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],
    ['agenda_start_2', '–ü–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è'],

    ['deal_approval_1', '–æ–¥–æ–±—Ä–∏—Ç—å —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏'],
    ['deal_approval_1.1', '–æ–¥–æ–±—Ä–∏—Ç—å —Å–¥–µ–ª–∫—É'],
    ['deal_approval_2', '–¥–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞'],
    ['deal_approval_3', '–ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏'],
    ['deal_approval_3.1', '–ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω–æ–π —Å–¥–µ–ª–∫–∏'],
    ['deal_approval_4', '–∑–∞–∫–ª—é—á–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä –∞—Ä–µ–Ω–¥—ã'],

    ['question_1', '–ü–æ –≤–æ–ø—Ä–æ—Å—É ‚Ññ 0'],
    ['question_2', '–ü–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],
    ['question_3', '–†–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],
    ['question_4', '–†–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ 1 –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],

    ['footers_1', '–í—Ä–µ–º—è –ø–æ–¥–≤–µ–¥–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è'],
    ['footers_2', '–°–ø–∏—Å–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:'],
    ['footers_3', '–ü–æ–¥—Å—á–µ—Ç –≥–æ–ª–æ—Å–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏–ª –°–µ–∫—Ä–µ—Ç–∞—Ä—å –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤'],
    ['footers_4', '–ü—Ä–æ—Ç–æ–∫–æ–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ 2-—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö']

  ]

  def __init__(self, embedder, elmo_embedder_default: ElmoEmbedder):
    ParsingContext.__init__(self, embedder)
    self.elmo_embedder_default = elmo_embedder_default
    self.protocols_factory: ProtocolPatternFactory = ProtocolPatternFactory(embedder)

    patterns_te = [p[1] for p in ProtocolParser.patterns_dict]
    self.patterns_embeddings = elmo_embedder_default.embedd_strings(patterns_te)

  def ebmedd(self, doc: ProtocolDocument):
    doc.sentence_map = tokenize_doc_into_sentences_map(doc, 250)

    ### ‚öôÔ∏èüîÆ SENTENCES embedding
    doc.sentences_embeddings = self.elmo_embedder_default.embedd_strings(doc.sentence_map.tokens)

    ### ‚öôÔ∏èüîÆ WORDS Ebmedding
    doc.embedd_tokens(self.embedder)

    doc.calculate_distances_per_pattern(self.protocols_factory)
    doc.distances_per_sentence_pattern_dict = calc_distances_per_pattern_dict(doc.sentences_embeddings,
                                                                              self.patterns_dict,
                                                                              self.patterns_embeddings)

  def analyse(self, doc: ProtocolDocument):
    self.ebmedd(doc)
    self._analyse_embedded(doc)

  def _analyse_embedded(self, doc: ProtocolDocument):
    doc.org_level = list(find_org_structural_level(doc))
    doc.agents_tags = list(find_protocol_org(doc))
    doc.agenda_questions = self.find_question_decision_sections(doc)
    doc.margin_values = self.find_values(doc)

    doc.agents_tags += list(self.find_agents_in_all_sections(doc, doc.agenda_questions))

  def find_agents_in_all_sections(self, doc: LegalDocument, agenda_questions: List[SemanticTag]) -> List[SemanticTag]:
    ret = []
    for parent in agenda_questions:
      x: List[SemanticTag] = self._find_agents_in_section(doc, parent, tag_kind_prefix='contract_agent_')
      if x:
        ret += x
    return ret

  def _find_agents_in_section(self, protocol: LegalDocument, parent: SemanticTag, tag_kind_prefix: str) -> List[
    SemanticTag]:
    x: List[SemanticTag] = find_org_names_in_tag(protocol, parent, max_names=10, tag_kind_prefix=tag_kind_prefix)
    return x

  def find_values(self, doc) -> [ContractValue]:
    values: [ContractValue] = find_value_sign_currency_attention(doc, doc.distances_per_pattern_dict[
      'relu_value_attention_vector'])

    # set parents for values
    for tag in doc.agenda_questions:

      for v in values:
        if tag.is_nested(v.span()):
          v.parent.parent = tag.kind

    return values

  def collect_spans_having_votes(self, segments, textmap):
    """
    search for votes in each document segment
    collect only
    :param segments:
    :param textmap:
    :return:  segments with votes
    """
    for span in segments:

      subdoc = textmap.slice(span)
      protocol_votes = list(subdoc.finditer(protocol_votes_re))
      if protocol_votes:
        yield span

  def find_protocol_sections_edges(self, distances_per_pattern_dict):

    patterns = ['deal_approval_', 'footers_', 'question_']
    vv_ = []
    for p in patterns:
      v_ = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, p)
      v_ = relu(v_, 0.5)
      vv_.append(v_)

    v_sections_attention = sum_probabilities(vv_)

    v_sections_attention = relu(v_sections_attention, 0.7)
    return v_sections_attention

  def _get_value_attention_vector(self, doc: LegalDocument):
    s_value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_p_')

    doc.distances_per_pattern_dict['__max_value_av'] = s_value_attention_vector  # just for debugging
    not_value_av = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'not_sum_')

    not_value_av = smooth_safe(not_value_av, window_len=5)

    not_value_av = relu(not_value_av, 0.5)
    doc.distances_per_pattern_dict['__not_value_av'] = not_value_av  # just for debugging

    s_value_attention_vector -= not_value_av * 0.8
    s_value_attention_vector = relu(s_value_attention_vector, 0.3)
    return s_value_attention_vector

  def find_question_decision_sections(self, doc: ProtocolDocument):
    wa = doc.distances_per_pattern_dict  # words attention
    v_sections_attention = self.find_protocol_sections_edges(doc.distances_per_sentence_pattern_dict)

    # --------------
    question_spans_sent = spans_between_non_zero_attention(v_sections_attention)
    question_spans_words = doc.sentence_map.remap_slices(question_spans_sent, doc.tokens_map)
    # --------------

    # *More* attention to spans having votes
    spans_having_votes = list(self.collect_spans_having_votes(question_spans_sent, doc.sentence_map))

    spans_having_votes_words = doc.sentence_map.remap_slices(spans_having_votes, doc.tokens_map)
    # questions_attention =  spans_to_attention(question_spans_words, len(doc))
    wa['bin_votes_attention'] = spans_to_attention(spans_having_votes_words, len(doc))

    # v_deal_approval_words = sentence_map.remap_spans(v_deal_approval,  doc.tokens_map )
    v_deal_approval = max_exclusive_pattern_by_prefix(doc.distances_per_sentence_pattern_dict, 'deal_approval_')
    _spans, v_deal_approval_words_attention = sentences_attention_to_words(v_deal_approval, doc.sentence_map,
                                                                           doc.tokens_map)

    ## value attention

    wa['relu_value_attention_vector'] = self._get_value_attention_vector(doc)
    wa['relu_deal_approval'] = relu(v_deal_approval_words_attention, 0.5)

    _value_attention_vector = sum_probabilities(
      [wa['relu_value_attention_vector'],
       wa['relu_deal_approval'],
       wa['bin_votes_attention'] / 3.0])

    wa['relu_value_attention_vector'] = relu(_value_attention_vector, 0.5)
    # // words_spans_having_votes = doc.sentence_map.remap_slices(spans_having_votes, doc.tokens_map)

    values: List[ContractValue] = find_value_sign_currency_attention(doc, wa['relu_value_attention_vector'])

    numbers_attention = np.zeros(len(doc.tokens_map))
    numbers_confidence = np.zeros(len(doc.tokens_map))
    for v in values:
      numbers_confidence[v.value.as_slice()] += v.value.confidence
      numbers_attention[v.value.as_slice()] = 1
      numbers_attention[v.currency.as_slice()] = 1
      numbers_attention[v.sign.as_slice()] = 1

    block_confidence = sum_probabilities([numbers_attention, wa['relu_deal_approval'], wa['bin_votes_attention'] / 5])

    return list(find_confident_spans(question_spans_words, block_confidence, 'agenda_item', 0.6))


class ProtocolPatternFactory(AbstractPatternFactory):
  def create_pattern(self, pattern_name, ppp):
    _ppp = (ppp[0].lower(), ppp[1].lower(), ppp[2].lower())
    fp = FuzzyPattern(_ppp, pattern_name)
    self.patterns.append(fp)
    return fp

  def __init__(self, embedder):
    AbstractPatternFactory.__init__(self, embedder)

    self._build_subject_pattern()
    self._build_sum_margin_extraction_patterns()
    self.embedd()

  def _build_sum_margin_extraction_patterns(self):
    suffix = '–º–ª–Ω. —Ç—ã—Å. –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π –¥–æ–ª–ª–∞—Ä–æ–≤ –∫–æ–ø–µ–µ–∫ –µ–≤—Ä–æ'
    prefix = ''

    sum_comp_pat = CoumpoundFuzzyPattern()

    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_1', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ –±–æ–ª–µ–µ 0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_2', (prefix + '—Ü–µ–Ω–∞', '–Ω–µ –±–æ–ª—å—à–µ 0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_3', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å <', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_4', (prefix + '—Ü–µ–Ω–∞ –º–µ–Ω–µ–µ', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_5', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_6', (prefix + '–æ–±—â–∞—è —Å—É–º–º–∞ –º–æ–∂–µ—Ç —Å–æ—Å—Ç–∞–≤–∏—Ç—å', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_7', (prefix + '–ª–∏–º–∏—Ç —Å–æ–≥–ª–∞—à–µ–Ω–∏—è', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_8', (prefix + '–≤–µ—Ä—Ö–Ω–∏–π –ª–∏–º–∏—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_9', (prefix + '–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞', '0', suffix)))

    # self.create_pattern('sum_max_neg1', ('–µ–∂–µ–º–µ—Å—è—á–Ω–æ –Ω–µ –ø–æ–∑–¥–Ω–µ–µ', '0', '—á–∏—Å–ª–∞ –∫–∞–∂–¥–æ–≥–æ –º–µ—Å—è—Ü–∞'))
    # self.create_pattern('sum_max_neg2', ('–ø—Ä–∏–Ω—è–ª–∏ —É—á–∞—Å—Ç–∏–µ –≤ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–∏', '0', '—á–µ–ª–æ–≤–µ–∫') )
    # self.create_pattern('sum_max_neg3', ('—Å—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å', '0', '–º–µ—Å—è—Ü–µ–≤ —Å –¥–∞—Ç—ã –≤—ã–¥–∞—á–∏'))
    # self.create_pattern('sum_max_neg4', ('–ø–æ–∑–¥–Ω–µ–µ —á–µ–º –∑–∞', '0', '–∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –¥–Ω–µ–π –¥–æ –¥–∞—Ç—ã –µ–≥–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è '))
    # self.create_pattern('sum_max_neg5', ('–æ–±—â–∞—è –ø–ª–æ—â–∞–¥—å', '0', '–∫–≤ . –º.'))

    f = self
    f.create_pattern('not_sum_0', ('', '–ø—É–Ω–∫—Ç 0.', ''))
    f.create_pattern('not_sum_1', ('', '0 –¥–Ω–µ–π', ''))
    f.create_pattern('not_sum_1.1', ('', '–≤ —Ç–µ—á–µ–Ω–∏–µ 0 ( –Ω–æ–ª—è ) –¥–Ω–µ–π', ''))
    f.create_pattern('not_sum_1.2', ('', '0 —è–Ω–≤–∞—Ä—è', ''))
    f.create_pattern('not_sum_2', ('', '0 –º–∏–Ω—É—Ç', ''))
    f.create_pattern('not_sum_3', ('', '0 —á–∞—Å–æ–≤', ''))
    f.create_pattern('not_sum_4', ('', '0 –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤', ''))
    f.create_pattern('not_sum_5', ('', '0 %', ''))
    f.create_pattern('not_sum_5.1', ('', '0 % –≥–æ–ª–æ—Å–æ–≤', ''))
    f.create_pattern('not_sum_6', ('', '2000 –≥–æ–¥', ''))
    f.create_pattern('not_sum_7', ('', '0 —á–µ–ª–æ–≤–µ–∫', ''))
    f.create_pattern('not_sum_8', ('', '0 –º–µ—Ç—Ä–æ–≤', ''))

  def _build_subject_pattern(self):
    ep = ExclusivePattern()

    PRFX = "–ü–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è: \n"

    if True:
      ep.add_pattern(self.create_pattern('t_deal_1', (PRFX, '–û–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏', '—Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –ø—Ä–æ–¥–∞–∂–µ–π')))
      ep.add_pattern(self.create_pattern('t_deal_2', (
        PRFX + '–û —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞', '—Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏', '—Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –∑–∞–∫–ª—é—á–µ–Ω–∏–µ–º –¥–æ–≥–æ–≤–æ—Ä–∞')))
      ep.add_pattern(self.create_pattern('t_deal_3', (
        PRFX + '–æ–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏', '–∫—Ä—É–ø–Ω–æ–π —Å–¥–µ–ª–∫–∏', '—Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –ø—Ä–æ–¥–∞–∂–µ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞')))

      for p in ep.patterns:
        p.soft_sliding_window_borders = True

    if True:
      ep.add_pattern(self.create_pattern('t_org_1', (PRFX, '–û —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∏–ª–∏–∞–ª–∞', '–û–±—â–µ—Å—Ç–≤–∞')))
      ep.add_pattern(self.create_pattern('t_org_2', (PRFX, '–û–± —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ –ü–æ–ª–æ–∂–µ–Ω–∏—è', '–æ —Ñ–∏–ª–∏–∞–ª–µ –û–±—â–µ—Å—Ç–≤–∞')))
      ep.add_pattern(self.create_pattern('t_org_3', (PRFX, '–û –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è', '—Ñ–∏–ª–∏–∞–ª–∞')))
      ep.add_pattern(self.create_pattern('t_org_4', (PRFX, '–û –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–∏ –ø–æ–ª–Ω–æ–º–æ—á–∏–π —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è', '—Ñ–∏–ª–∏–∞–ª–∞')))
      ep.add_pattern(self.create_pattern('t_org_5', (PRFX, '–û –≤–Ω–µ—Å–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π', '')))

    if True:
      ep.add_pattern(
        self.create_pattern('t_charity_1', (PRFX + '–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–º–æ—â–∏')))
      ep.add_pattern(
        self.create_pattern('t_charity_2', (PRFX + '–û —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏', '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))
      ep.add_pattern(self.create_pattern('t_charity_3', (PRFX + '–û–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏', '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))

      t_char_mix = CoumpoundFuzzyPattern()
      t_char_mix.name = "t_charity_mixed"

      t_char_mix.add_pattern(
        self.create_pattern('tm_charity_1', (PRFX + '–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–º–æ—â–∏', '')))
      t_char_mix.add_pattern(
        self.create_pattern('tm_charity_2', (PRFX + '–û —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ', '—Å–¥–µ–ª–∫–∏ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))
      t_char_mix.add_pattern(self.create_pattern('tm_charity_3', (PRFX + '–û–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏', '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))

      ep.add_pattern(t_char_mix)

    self.subject_pattern = ep


def find_protocol_org(protocol: ProtocolDocument) -> List[SemanticTag]:
  ret = []
  x: List[SemanticTag] = find_org_names(protocol[0:HyperParameters.protocol_caption_max_size_words])
  nm = SemanticTag.find_by_kind(x, 'org.1.name')
  if nm is not None:
    ret.append(nm)

  tp = SemanticTag.find_by_kind(x, 'org.1.type')
  if tp is not None:
    ret.append(tp)

  protocol.agents_tags = ret
  return ret


import re

from pyjarowinkler import distance


def closest_name(pattern: str, knowns: [str]) -> (str, int):
  #
  min_distance = 0
  found = None
  for b in knowns:
    d = distance.get_jaro_distance(pattern, b, winkler=True, scaling=0.1)
    if d > min_distance:
      found = b
      min_distance = d

  return found, min_distance


def find_org_structural_level(doc: LegalDocument) -> Iterator[SemanticTag]:
  compiled_re = re.compile(ORG_LEVELS_re, re.MULTILINE | re.IGNORECASE | re.UNICODE)

  entity_type = 'org_structural_level'
  for m in re.finditer(compiled_re, doc.text):

    char_span = m.span(entity_type)
    span = doc.tokens_map.token_indices_by_char_range_2(char_span)
    val = doc.tokens_map.text_range(span)

    val, conf = closest_name(val, ORG_LEVELS_names)

    confidence = conf * (1.0 - (span[0] / len(doc)))  # relative distance from the beginning of the document
    if span_len(char_span) > 1 and is_long_enough(val):

      if confidence > HyperParameters.org_level_min_confidence:
        tag = SemanticTag(entity_type, val, span)
        tag.confidence = confidence

        yield tag


def find_confident_spans(slices: [int], block_confidence: FixedVector, tag_name: str, threshold: float) -> Iterator[
  SemanticTag]:
  k = 0
  for _slice in slices:
    k += 1
    pv = block_confidence[_slice[0]:_slice[1]]
    confidence = estimate_confidence_by_mean_top_non_zeros(pv, 5)

    if confidence > threshold:
      st = SemanticTag(f"{tag_name}_{k}", None, _slice)
      st.confidence = confidence
      yield (st)
