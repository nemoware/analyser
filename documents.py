import os
import pickle
import sys
import traceback
import warnings

import nltk

from hyperparams import models_path
from text_tools import Tokens, my_punctuation, untokenize, replace_tokens, tokenize_text

TEXT_PADDING_SYMBOL = ' '
nltk.download('punkt')


class TextMap:

  def __init__(self, text: str, map=None):
    self._full_text = str(text)
    self._offset_chars = 0
    if map is None:
      self.map = TOKENIZER_DEFAULT.tokens_map(text)
    else:
      self.map = list(map)

    self.untokenize = self.text_range  # alias

  def __add__(self, other):
    off = len(self._full_text)
    self._full_text += other._full_text
    # if len(self.map)>0:
    #   off = self.map[-1][-1]
    # else:
    #   off=len(self._full_text)
    for span in other.map:
      self.map.append((span[0] + off, span[1] + off))

    return self

  def set_token(self, index, new_token):
    assert len(new_token) == self.map[index][1] - self.map[index][0]
    self._full_text = self._full_text[: self.map[index][0]] + new_token + self._full_text[self.map[index][1]:]

  def finditer(self, regexp):
    for m in regexp.finditer(self.text):
      yield self.token_indices_by_char_range_2(m.span(0))

  def token_index_by_char(self, _char_index: int) -> int:
    """
    [span 0] out of span [span 1] [span 2]

    :param _char_index:
    :return:
    """

    char_index = _char_index + self._offset_chars
    for span_index in range(len(self.map)):
      span = self.map[span_index]
      if char_index < span[1]:  # span end
        return span_index

    if char_index >= len(self.text):
      return len(self.map)
    return -1

  def token_indices_by_char_range_2(self, span: [int]) -> (int, int):
    a = self.token_index_by_char(span[0])
    b = self.token_index_by_char(span[1])
    if a == b and b >= 0:
      b = a + 1
    return a, b

  token_indices_by_char_range = token_indices_by_char_range_2

  def slice(self, span: slice) -> 'TextMap':
    sliced = TextMap(self._full_text, self.map[span])
    if sliced.map:
      sliced._offset_chars = sliced.map[0][0]
      sliced._full_text = sliced._full_text[0:  sliced.map[-1][-1]]
    else:
      sliced._offset_chars = 0

    return sliced

  def split(self, delimiter: str) -> [Tokens]:
    last = 0

    for i in range(self.get_len()):
      if self[i] == delimiter:
        yield self[last: i]
        last = i + 1
    yield self[last: self.get_len()]

  def split_spans(self, delimiter: str, add_delimiter=False):
    addon = 0
    if add_delimiter:
      addon = 1

    last = 0
    for i in range(self.get_len()):
      if self[i] == delimiter:
        yield [last, i + addon]
        last = i + 1
    yield [last, self.get_len()]

  def sentence_at_index(self, i: int, return_delimiters=True) -> (int, int):
    sent_spans = self.split_spans('\n', add_delimiter=return_delimiters)
    d_add = 1
    if return_delimiters:
      d_add = 0

    for s in sent_spans:
      if i >= s[0] and i < s[1] + d_add:
        return s
    return [0, self.get_len()]

  def char_range(self, span: [int]) -> (int, int):
    a = span[0]
    b = span[1]

    if a is None:
      a = 0

    if b is None:
      b = len(self.map)

    if a >= len(self.map):
      return 0, 0

    start = self.map[a][0]

    _last = min(len(self.map), b)
    stop = self.map[_last - 1][1]

    return start, stop

  def remap_spans(self, spans, target_map: 'TextMap'):
    assert self._full_text == target_map._full_text
    ret = []
    for span in spans:
      char_range = self.char_range(span)
      words_range = target_map.token_indices_by_char_range(char_range)
      ret.append(words_range)
    return ret

  def remap_slices(self, spans, target_map: 'TextMap'):
    assert self._full_text == target_map._full_text
    ret = []
    for span in spans:
      char_range = self.char_range([span.start, span.stop])
      words_range = target_map.token_indices_by_char_range(char_range)
      ret.append(words_range)
    return ret

  def text_range(self, span) -> str:
    try:
      start, stop = self.char_range(span)
      return self._full_text[start: stop]
    except:
      err = f'cannot deal with {span}'
      traceback.print_exc(file=sys.stdout)
      raise RuntimeError(err)

  def get_text(self):
    if len(self.map) == 0:
      return ''
    return self.text_range([0, len(self.map)])

  def tokens_by_range(self, span) -> Tokens:
    tokens_i = self.map[span[0]:span[1]]
    return [
      self._full_text[tr[0]:tr[1]] for tr in tokens_i
    ]

  def get_len(self):
    return len(self.map)

  def __len__(self):
    return self.get_len()

  def __getitem__(self, key):
    if isinstance(key, slice):
      # Get the start, stop, and step from the slice
      return [self[ii] for ii in range(*key.indices(len(self)))]
    elif isinstance(key, int):

      r = self.map[key]
      # print('__getitem__', key)
      return self._full_text[r[0]:r[1]]
    else:
      raise TypeError("Invalid argument type.")

  def get_tokens(self):
    return [
      self._full_text[tr[0]:tr[1]] for tr in self.map
    ]

  tokens = property(get_tokens)
  text = property(get_text)


class CaseNormalizer:
  __shared_state = {}  ## see http://code.activestate.com/recipes/66531/

  def __init__(self):
    self.__dict__ = self.__shared_state
    if 'replacements_map' not in self.__dict__:
      p = os.path.join(models_path, 'word_cases_stats.pickle')
      print('loading word cases stats model', p)

      with open(p, 'rb') as handle:
        self.replacements_map = pickle.load(handle)

  def normalize_tokens_map_case(self, map: TextMap) -> TextMap:
    norm_tokens = replace_tokens(map.tokens, self.replacements_map)
    norm_map = TextMap(map._full_text, map.map)
    for k in range(len(map)):
      norm_map.set_token(k, norm_tokens[k])
    # chars = list(map.text)
    # for i in range(0, len(map)):
    #   r = map.map[i]
    #   chars[r[0]:r[1]] = norm_tokens[i]
    # norm_map = TextMap(''.join(chars), list(map.map))
    # # XXXX
    # dfdfdfdf
    return norm_map

  def normalize_tokens(self, tokens: Tokens) -> Tokens:
    return replace_tokens(tokens, self.replacements_map)

  def normalize_text(self, text: str) -> str:
    warnings.warn(
      "Deprecated, because this class must not perform tokenization. Use normalize_tokens or  normalize_tokens_map_case",
      DeprecationWarning)
    tokens = tokenize_text(text)
    tokens = self.normalize_tokens(tokens)
    return untokenize(tokens)

  def normalize_word(self, token: str) -> str:
    if token.lower() in self.replacements_map:
      return self.replacements_map[token.lower()]
    else:
      return token


class EmbeddableText:
  warnings.warn("deprecated", DeprecationWarning)

  def __init__(self):
    warnings.warn("deprecated", DeprecationWarning)

    self.embeddings = None


# ---------------------------------------------------


class GTokenizer:
  def tokenize(self, s) -> Tokens:
    raise NotImplementedError()

  def untokenize(self, t: Tokens) -> str:
    raise NotImplementedError()


def span_tokenize(text):
  start_from = 0
  text = text.replace('`', '!')
  text = text.replace('"', '!')
  tokens = list(nltk.word_tokenize(text))
  __debug = []

  for search_token in tokens:

    ix_new = text.find(search_token, start_from)
    if ix_new < 0:
      msg = f'ACHTUNG! [{search_token}] not found with text.find, next text is: {text[start_from:start_from + 30]}'
      warnings.warn(msg)
    else:
      start_from = ix_new
      end = start_from + len(search_token)
      __debug.append((search_token, start_from, end))
      yield [start_from, end]
      start_from = end


class DefaultGTokenizer(GTokenizer):

  def __init__(self):

    # pth = os.path.join(os.path.dirname(__file__), 'nltk_data_download')
    # nltk.download('punkt', download_dir=pth)
    pass

  def tokenize_line(self, line):
    return [line[t[0]:t[1]] for t in span_tokenize(line)]

  def tokenize(self, text) -> Tokens:
    return [text[t[0]:t[1]] for t in self.tokens_map(text)]

  def untokenize(self, tokens: Tokens) -> str:
    warnings.warn("deprecated", DeprecationWarning)
    # TODO: remove it!!
    return "".join([" " + i if not i.startswith("'") and i not in my_punctuation else i for i in tokens]).strip()

  # build tokens map to char pos
  def tokens_map(self, text):

    result = []
    for i in range(len(text)):
      if text[i] == '\n':
        result.append([i, i + 1])

    result += [s for s in span_tokenize(text)]

    result.sort(key=lambda x: x[0])
    return result


# TODO: use it!
TOKENIZER_DEFAULT = DefaultGTokenizer()

import numpy as np


def sentences_attention_to_words(attention_v, sentence_map: TextMap, words_map: TextMap):
  q_sent_indices = np.nonzero(attention_v)[0]
  w_spans_attention = np.zeros(len(words_map))
  char_ranges = [(sentence_map.map[i], attention_v[i]) for i in q_sent_indices]

  w_spans = []
  for char_range, a in char_ranges:
    words_range = words_map.token_indices_by_char_range(char_range)
    w_spans.append(words_range)
    w_spans_attention[words_range[0]:words_range[1]] += a

  return w_spans, w_spans_attention
